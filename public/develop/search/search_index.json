{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-mlearning","title":"gemseo-mlearning","text":"<p><code>gemseo-mlearning</code> is a plugin of the library GEMSEO, dedicated to machine learning. This package is open-source, under the LGPL v3 license.</p>"},{"location":"#overview","title":"Overview","text":"<p>This package adds new regression models and optimization algorithms based on SMT.</p> <p>A package for active learning is also available, deeply based on the core GEMSEO objects for optimization, as well as a SurrogateBasedOptimization library built on its top. An effort is being made to improve both content and performance in future versions.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version with <code>pip install gemseo-mlearning</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Antoine Dechaume</li> <li>Beno\u00eet Pauwels</li> <li>Cl\u00e9ment Laboulfie</li> <li>Matthias De Lozzo</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#develop","title":"Develop","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>ActiveLearningAlgo   can acquire points by batch using the <code>batch_size</code>   and <code>mc_size</code> argument,   when the regressor is based on a random process   such as <code>GaussianProcessRegressor</code>   and <code>OTGaussianProcessRegressor</code>.   This option is only available for criteria dedicated   to level set LevelSet   (alternatively quantile estimation Quantile)   and the expected improvement for maximum/minimum estimation Maximum.</li> <li>The Branin and   Rosenbrock problems   can be used to benchmark   the efficiency of the active learning algorithms   and estimate several quantities of interest,   optimas or quantiles for instance.</li> <li>AcquisitionView   can be used to plot   both the output of the original model,   the prediction of the surrogate model,   the standard deviation of the surrogate model   and the acquisition criterion,   when the input dimension is 1 or 2.</li> <li>SMTRegressor   can be any surrogate model available in the Python package SMT.</li> <li><code>\"SMT_EGO\"</code> is the name of the expected global optimization (EGO) algorithm   wrapping the surrogate-based optimizers available in the Python package SMT;   this is the unique algorithm of the SMTEGO optimization library.</li> <li>SurrogateBasedOptimization   can use the acquisition criteria <code>CB</code> and <code>Output</code> in addition to <code>EI</code>.</li> <li>SurrogateBasedOptimization   can use an existing BaseRegressor   and save the BaseRegressor that it enriches   using the <code>regression_file_path</code> option.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>BREAKING CHANGE: The acquisition algorithm settings have to be passed to   SurrogateBasedOptimizer   as keyword arguments, i.e. <code>SurrogateBasedOptimizer(..., key_1=value_1, key_2=value_2, ...)</code>.</li> <li>BREAKING CHANGE: The term <code>option</code> has been replaced by <code>setting</code> when it was linked to a DOE or an optimization algorithm.</li> <li>BREAKING CHANGE: The argument <code>distribution</code> of   ActiveLearningAlgo.   renamed to <code>regressor</code>;   it can be either a</li> <li>BaseRegressor   or a   BaseRegressorDistribution.</li> <li>BREAKING CHANGE: The method <code>compute_next_input_data</code> of   ActiveLearningAlgo.   renamed to   find_next_point.</li> <li>BREAKING CHANGE: The method <code>update_algo</code> of   ActiveLearningAlgo.   renamed to   acquire_new_points.</li> <li>BREAKING CHANGE: <code>MaxExpectedImprovement</code> renamed to <code>`Maximum</code>.</li> <li>BREAKING CHANGE: <code>MinExpectedImprovement</code> renamed to <code>`Minimum</code>.</li> <li>BREAKING CHANGE: <code>ExpectedImprovement</code> removed.</li> <li>BREAKING CHANGE: the acquisition criterion <code>LimitState</code> renamed to   LevelSet</li> <li>BREAKING CHANGE: each acquisition criterion class has a specific module   in gemseo_mlearning.active_learning.acquisition_criteria   whose name is the snake-case version of it class name, i.e. <code>nice_criterion.py</code> contains <code>NiceCriterion</code>.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.adaptive.distribution.MLRegressorDistribution</code> renamed to   gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.lib_surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.surrogate_based_optimization.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.core.surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.</li> <li>BREAKING CHANGE: <code>MLDataAcquisition</code> renamed to   ActiveLearningAlgo.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterion</code> renamed to   BaseAcquisitionCriterion   and moved to</li> <li>gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterionFactory</code> renamed to   AcquisitionCriterionFactory,   moved to   gemseo_mlearning.active_learning.acquisition_criteria   and without the property <code>available_criteria</code> (use <code>AcquisitionCriterionFactory.class_names</code>).</li> <li>BREAKING CHANGE: <code>gemseo.adaptive</code> renamed to gemseo_mlearning.active_learning.</li> <li>BREAKING CHANGE: <code>gemseo.adaptive.criteria</code> renamed to   gemseo_mlearning.active_learning.acquisition_criteria.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The data transformer can be set with the <code>\"transformer\"</code> key of the <code>regression_options</code> dictionary   passed to SurrogateBasedOptimization.</li> <li>The Quantile   estimates the quantile by Monte Carlo sampling   by means of the probability distributions of the input variables;   these distributions are defined with its new argument <code>uncertain_space</code>.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>AcquisitionCriterionFactory</code>; replaced by different factories for the developers.</li> <li><code>sample_discipline</code>; use sample_disciplines from <code>gemseo</code> instead.</li> <li><code>sample_disciplines</code>; move to <code>gemseo</code>: sample_disciplines.</li> <li><code>MAEMeasure</code>; moved to <code>gemseo</code>: MAEMeasure.</li> <li><code>MEMeasure</code>; moved to <code>gemseo</code>: MEMeasure.</li> <li><code>GradientBoostingRegressor</code>; moved to <code>gemseo</code>: GradientBoostingRegressor.</li> <li><code>MLPRegressor</code>; moved to <code>gemseo</code>: MLPRegressor.</li> <li><code>OTGaussianProcessRegressor</code>; moved to <code>gemseo</code>: OTGaussianProcessRegressor.</li> <li><code>RegressorChain</code>; moved to <code>gemseo</code>: RegressorChain.</li> <li><code>SVMRegressor</code>; moved to <code>gemseo</code>: SVMRegressor.</li> <li><code>TPSRegressor</code>; moved to <code>gemseo</code>: TPSRegressor.</li> </ul>"},{"location":"changelog/#version-112-december-2023","title":"Version 1.1.2 (December 2023)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> <li><code>OTGaussianProcessRegressor</code> has a new optional argument <code>optimizer</code>   to select the OpenTURNS optimizer for the covariance model parameters.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-111-september-2023","title":"Version 1.1.1 (September 2023)","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li><code>OTGaussianProcessRegressor.predict_std</code>   no longer returns the variance of the output but its standard deviation.</li> </ul>"},{"location":"changelog/#version-110-june-2023","title":"Version 1.1.0 (June 2023)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>An argument <code>trend_type</code> of type <code>OTGaussianProcessRegressor.TrendType</code> to <code>OTGaussianProcessRegressor</code>;   the trend type of the Gaussian process regressor can be either constant,   linear or quadratic.</li> <li>A new optimization library   SurrogateBasedOptimization   to perform EGO-like surrogate-based optimization on unconstrained problems.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>The output of an <code>MLDataAcquisitionCriterion</code>   based on a regressor built from constant output values is no longer <code>nan</code>.</li> </ul>"},{"location":"changelog/#version-101-february-2022","title":"Version 1.0.1 (February 2022)","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>BaseRegressorDistribution   can now use a regression algorithm instantiated with transformers.</li> </ul>"},{"location":"changelog/#version-100-july-2022","title":"Version 1.0.0 (July 2022)","text":"<p>First release.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>The developers thank all the open source libraries making <code>gemseo-mlearning</code> possible.</p>"},{"location":"credits/#exec-1--external-dependencies","title":"External Dependencies","text":"<p><code>gemseo-mlearning</code> depends on software with compatible licenses that are listed below.</p> Project License <code>Python</code> Python Software License <code>gemseo</code> GNU Lesser General Public License v3 <code>numpy</code> BSD License <code>openturns</code> LGPL <code>scikit-learn</code> BSD License <code>scipy</code> BSD License <code>smt</code> BSD-3"},{"location":"credits/#exec-1--external-applications","title":"External applications","text":"<p>Some external applications are used by <code>gemseo-mlearning</code>, but not linked with the application, for testing, documentation generation, training or example purposes.</p> Project License <code>black</code> MIT <code>commitizen</code> MIT License <code>covdefaults</code> MIT License <code>docformatter</code> MIT License <code>griffe-fieldz</code> BSD-3-Clause <code>griffe-inherited-docstrings</code> ISC <code>insert-license</code> MIT <code>markdown-exec</code> ISC <code>mike</code> BSD-3-Clause <code>mkdocs-bibtex</code> BSD-3-Clause-LBNL <code>mkdocs-gallery</code> BSD 3-Clause <code>mkdocs-gen-files</code> MIT License <code>mkdocs-include-markdown-plugin</code> Apache Software License <code>mkdocs-literate-nav</code> MIT License <code>mkdocs-material</code> MIT License <code>mkdocs-section-index</code> MIT License <code>mkdocstrings</code> ISC <code>pre-commit</code> MIT License <code>pygrep-hooks</code> MIT <code>pytest</code> MIT License <code>pytest-cov</code> MIT License <code>pytest-xdist</code> MIT License <code>ruff</code> MIT License <code>setuptools</code> MIT License <code>setuptools-scm</code> MIT License"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-mlearning</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-mlearning</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-mlearning</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"developer_guide/active_learning/","title":"Active learning","text":""},{"location":"developer_guide/active_learning/#active-learning","title":"Active learning","text":"<p>This section describes the design of the active_learning subpackage.</p> <p>Info</p> <p>Open the user guide for general information, e.g. concepts, API, examples, etc.</p>"},{"location":"developer_guide/active_learning/#tree-structure","title":"Tree structure","text":"<pre><code>\ud83d\udcc1 gemseo_mlearning\n\u2514\u2500\u2500 \ud83d\udcc1 active_learning # Subpackage for active learning (AL)\n    \u251c\u2500\u2500 \ud83d\udcc4 active_learning_algo.py # Class to set and solve an AL problem\n    \u251c\u2500\u2500 \ud83d\udcc1 acquisition_criteria # Acquisition criteria (ACs)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 exploration # ACs to improve the regressor\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_exploration.py # Base class for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 distance.py # AC (distance to the learning set)\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 exploration.py # Specific AC family\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 factory.py # Factory for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 standard_deviation.py # An AC (regressor's standard deviation)\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 variance.py # AC (regressor's variance)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 level_set/ # ACs to approximate a level set\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 maximum/ # ACs to approximate the global maximum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 minimum/ # ACs to approximate the global minimum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 quantile/ # ACs to approximate a quantile\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion.py # Base class for ACs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion_family.py # Base class for AC families\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 base_factory.py # Base class for AC\n    \u251c\u2500\u2500 \ud83d\udcc1 distributions # Regressor distributions (RD)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_regressor_distribution.py # The base class for RDs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 kriging_distribution.py # The RD for Kriging regressors\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 regressor_distribution.py # The RD for any regressor\n    \u2514\u2500\u2500 \ud83d\udcc1 visualization # Visualization tools\n        \u251c\u2500\u2500 \ud83d\udcc4 acquisition_view.py # Plot the acquisition process (in 2D only)\n        \u2514\u2500\u2500 \ud83d\udcc4 qoi_history_view.py # Plot the history of the quantity of interest\n</code></pre>"},{"location":"developer_guide/active_learning/#class-diagram","title":"Class diagram","text":"<pre><code>classDiagram\n\n    ActiveLearningAlgo *-- OptimizationProblem\n    ActiveLearningAlgo o-- DesignSpace\n    ActiveLearningAlgo --&gt; AcquisitionCriterionFamilyFactory: criterion_family_name\n    ActiveLearningAlgo --&gt; BaseAcquisitionCriterionFactory: criterion_name \\n criterion_options\n    ActiveLearningAlgo --&gt; BaseRegressorDistribution: regressor\n    BaseDriverLibrary --* ActiveLearningAlgo\n    BaseDOELibrary --* BaseDriverLibrary\n    BaseOptimizationLibrary --* BaseDriverLibrary\n    Database --* ActiveLearningAlgo\n    AcquisitionView --* ActiveLearningAlgo\n    QOIHistoryView --* ActiveLearningAlgo\n\n    OptimizationProblem o-- BaseAcquisitionCriterion\n\n    AcquisitionCriterionFamilyFactory --&gt; BaseAcquisitionCriterionFamily\n    BaseAcquisitionCriterionFamily --&gt; BaseAcquisitionCriterionFactory\n    BaseAcquisitionCriterionFactory --&gt; BaseAcquisitionCriterion\n\n    BaseAcquisitionCriterion --|&gt; MDOFunction\n    BaseAcquisitionCriterion o-- BaseRegressorDistribution\n\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterion\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFactory\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFamily\n    &lt;&lt;abstract&gt;&gt; BaseRegressorDistribution\n\n    class ActiveLearningAlgo {\n        +default_algo_name\n        +default_doe_settings\n        +default_opt_settings\n        +acquisition_criterion\n        +input_space\n        +n_initial_samples\n        +qoi\n        +qoi_history\n        +regressor\n        +regressor_distribution\n        +set_acquisition_algorithm()\n        +find_nex_point()\n        +acquire_new_points()\n        +plot_acquisition_view()\n        +plot_qoi_history()\n        +update_problem()\n    }\n\n    class AcquisitionCriterionFamilyFactory {\n        +create()\n    }\n\n    class BaseAcquisitionCriterionFamily {\n        +ACQUISITION_CRITERION_FACTORY\n    }\n\n    class BaseAcquisitionCriterionFactory {\n        #DEFAULT_CLASS_NAME\n    }</code></pre>"},{"location":"developer_guide/active_learning/#how-to","title":"How to...","text":"<code>... create a new family of acquisition criteria?</code> <ol> <li>Derive an abstract class <code>BaseNewAcquisitionCriterion</code> from    BaseAcquisitionCriterion.</li> <li>Derive <code>FirstAcquisitionCriterion</code>, <code>SecondAcquisitionCriterion</code>, ... from <code>BaseNewAcquisitionCriterion</code>    in modules located in <code>root_package_name.subpackage_name.package_name</code>.</li> <li>Derive <code>NewAcquisitionCriterionFactory</code> from    BaseAcquisitionCriterionFactory    and set the class attributes:    <pre><code>_CLASS = BaseNewAcquisitionCriterion\n_DEFAULT_CLASS_NAME = \"FirstAcquisitionCriterion\"\n_MODULE_NAMES = (\"root_package.subpackage_name.package_name\",)\n</code></pre></li> <li>Derive <code>NewAcquisitionCriterionFamily</code> from    BaseAcquisitionCriterionFamily    and set the class attribute <code>ACQUISITION_CRITERION_FACTORY = NewAcquisitionCriterionFactory</code>.</li> </ol> <p>Now, the user can instantiate the ActiveLearningAlgo</p> <ul> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   to use this family of acquisition criteria,</li> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   and <code>criterion_name=\"SecondAcquisitionCriterion\"</code>   to use a specific acquisition criterion from this family.</li> </ul>"},{"location":"generated/examples/active_learning/","title":"Active learning","text":""},{"location":"generated/examples/active_learning/#active-learning","title":"Active learning","text":"<p> Download all examples in Python source code: active_learning_python.zip</p> <p> Download all examples in Jupyter notebooks: active_learning_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/","title":"Optimization","text":""},{"location":"generated/examples/optimization/#surrogate-based-optimization","title":"Surrogate-based optimization","text":"<p> Surrogate-based optimization using in-house features. </p> <p> Surrogate-based optimization using SMT. </p> <p> Download all examples in Python source code: optimization_python.zip</p> <p> Download all examples in Jupyter notebooks: optimization_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/mg_execution_times/","title":"Computation times","text":"<p>00:37.300 total execution time for generated_examples_optimization files:</p> <p>+--------------------------------------------------------------------------------+-----------+--------+ | plot_smt_ego (docs/examples/optimization/plot_smt_ego.py) | 00:25.349 | 0.0 MB | +--------------------------------------------------------------------------------+-----------+--------+ | plot_sbo_ego (docs/examples/optimization/plot_sbo_ego.py) | 00:11.951 | 0.0 MB | +--------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/optimization/plot_sbo_ego/","title":"Surrogate-based optimization using in-house features.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/plot_sbo_ego/#surrogate-based-optimization-using-in-house-features","title":"Surrogate-based optimization using in-house features.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configure_logger\nfrom gemseo import execute_algo\nfrom gemseo.post.dataset.zvsxy import ZvsXY\nfrom gemseo.problems.dataset.rosenbrock import create_rosenbrock_dataset\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nconfigure_logger()\n</code></pre> <p>Out:</p> <pre><code>&lt;RootLogger root (INFO)&gt;\n</code></pre> <p>In this example, we seek to minimize the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\) over the design space \\([-2,2]^2\\). First, we instantiate the problem with \\((0, 0)\\) as initial guess:</p> <pre><code>problem = Rosenbrock()\n</code></pre> <p>Then, we minimize the Rosenbrock function using:</p> <ul> <li>the <code>\"SBO\"</code> algorithm,</li> <li>a maximum number of 40 evaluations,   including the initial one at the center of the design space   (this first point is common to all optimization algorithms)   and the initial training dataset,</li> <li> <p>its default settings,   namely:</p> </li> <li> <p>the expected improvement as acquisition criterion,</p> </li> <li>1 point acquired at a time,</li> <li>the <code>GaussianProcessRegressor</code> based on scikit-learn,</li> <li>10 initial training points     based on an optimized latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion     from 50 start points with a limit of 20 iterations per local optimization.</li> </ul> <pre><code>execute_algo(problem, \"SBO\", max_iter=40)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 18:50:51: Optimization problem:\n    INFO - 18:50:51:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 18:50:51:    with respect to x\n    INFO - 18:50:51:    over the design space:\n    INFO - 18:50:51:       +------+-------------+-------+-------------+-------+\n    INFO - 18:50:51:       | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 18:50:51:       +------+-------------+-------+-------------+-------+\n    INFO - 18:50:51:       | x[0] |      -2     |   0   |      2      | float |\n    INFO - 18:50:51:       | x[1] |      -2     |   0   |      2      | float |\n    INFO - 18:50:51:       +------+-------------+-------+-------------+-------+\n    INFO - 18:50:51: Solving optimization problem with algorithm SBO:\n    INFO - 18:50:51:      2%|\u258e         | 1/40 [00:00&lt;00:00, 4152.78 it/sec, obj=1]\n    INFO - 18:50:51:      5%|\u258c         | 2/40 [00:00&lt;00:15,  2.46 it/sec, obj=1.06e+3]\n    INFO - 18:50:52:      8%|\u258a         | 3/40 [00:01&lt;00:14,  2.51 it/sec, obj=5.31]\n    INFO - 18:50:52:     10%|\u2588         | 4/40 [00:01&lt;00:15,  2.26 it/sec, obj=690]\n    INFO - 18:50:53:     12%|\u2588\u258e        | 5/40 [00:02&lt;00:14,  2.34 it/sec, obj=7.58]\n    INFO - 18:50:53:     15%|\u2588\u258c        | 6/40 [00:02&lt;00:14,  2.31 it/sec, obj=4.35]\n    INFO - 18:50:54:     18%|\u2588\u258a        | 7/40 [00:02&lt;00:14,  2.35 it/sec, obj=401]\n    INFO - 18:50:54:     20%|\u2588\u2588        | 8/40 [00:03&lt;00:13,  2.29 it/sec, obj=227]\n    INFO - 18:50:54:     22%|\u2588\u2588\u258e       | 9/40 [00:03&lt;00:13,  2.35 it/sec, obj=9.9]\n    INFO - 18:50:55:     25%|\u2588\u2588\u258c       | 10/40 [00:04&lt;00:12,  2.33 it/sec, obj=6.7]\n    INFO - 18:50:55:     28%|\u2588\u2588\u258a       | 11/40 [00:04&lt;00:12,  2.38 it/sec, obj=5.16]\n    INFO - 18:50:56:     30%|\u2588\u2588\u2588       | 12/40 [00:05&lt;00:11,  2.35 it/sec, obj=23]\n    INFO - 18:50:56:     32%|\u2588\u2588\u2588\u258e      | 13/40 [00:05&lt;00:11,  2.40 it/sec, obj=0.159]\n    INFO - 18:50:56:     35%|\u2588\u2588\u2588\u258c      | 14/40 [00:05&lt;00:10,  2.42 it/sec, obj=7.45]\n    INFO - 18:50:57:     38%|\u2588\u2588\u2588\u258a      | 15/40 [00:06&lt;00:10,  2.34 it/sec, obj=0.667]\n    INFO - 18:50:57:     40%|\u2588\u2588\u2588\u2588      | 16/40 [00:06&lt;00:10,  2.36 it/sec, obj=2.37]\n    INFO - 18:50:58:     42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:07&lt;00:09,  2.41 it/sec, obj=1.85]\n    INFO - 18:50:58:     45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:07&lt;00:09,  2.44 it/sec, obj=0.0667]\n    INFO - 18:50:58:     48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:07&lt;00:08,  2.47 it/sec, obj=2.46]\n    INFO - 18:50:59:     50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:07&lt;00:07,  2.50 it/sec, obj=14.5]\n    INFO - 18:50:59:     52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:08&lt;00:07,  2.51 it/sec, obj=0.983]\n    INFO - 18:50:59:     55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:08&lt;00:07,  2.55 it/sec, obj=0.157]\n    INFO - 18:51:00:     57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:08&lt;00:06,  2.58 it/sec, obj=0.157]\n    INFO - 18:51:00:     60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:09&lt;00:06,  2.61 it/sec, obj=0.157]\n    INFO - 18:51:00:     62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:09&lt;00:05,  2.63 it/sec, obj=0.157]\n    INFO - 18:51:00:     65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:09&lt;00:05,  2.66 it/sec, obj=0.157]\n    INFO - 18:51:01:     68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:10&lt;00:04,  2.67 it/sec, obj=0.157]\n    INFO - 18:51:01:     70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:10&lt;00:04,  2.67 it/sec, obj=2.45]\n    INFO - 18:51:01:     72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:10&lt;00:04,  2.68 it/sec, obj=0.157]\n    INFO - 18:51:02:     75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:11&lt;00:03,  2.70 it/sec, obj=0.157]\n    INFO - 18:51:02: Optimization result:\n    INFO - 18:51:02:    Optimizer info:\n    INFO - 18:51:02:       Status: None\n    INFO - 18:51:02:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 18:51:02:       Number of calls to the objective function by the optimizer: 42\n    INFO - 18:51:02:    Solution:\n    INFO - 18:51:02:       Objective: 0.06668504773012356\n    INFO - 18:51:02:       Design space:\n    INFO - 18:51:02:          +------+-------------+--------------------+-------------+-------+\n    INFO - 18:51:02:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 18:51:02:          +------+-------------+--------------------+-------------+-------+\n    INFO - 18:51:02:          | x[0] |      -2     | 0.7893323855827736 |      2      | float |\n    INFO - 18:51:02:          | x[1] |      -2     | 0.6379802069784151 |      2      | float |\n    INFO - 18:51:02:          +------+-------------+--------------------+-------------+-------+\n</code></pre> Optimization result:<ul><li>Design variables: [0.78933239 0.63798021]</li><li>Objective function: 0.06668504773012356</li><li>Feasible solution: True</li></ul> <p>We can see that the solution is close to the theoretical one \\((x^*,f^*)=((1,1),0)\\).</p> <p>We can also visualize all the evaluations and note that most of the points have been added in the valley as expected:</p> <pre><code>optimization_history = problem.to_dataset()\n\ninitial_point = optimization_history[0:1]\ninitial_point.name = \"Initial point\"\n\ninitial_training_points = optimization_history[1:12]\ninitial_training_points.name = \"Initial training points\"\n\nacquired_points = optimization_history[12:]\nacquired_points.name = \"Acquired points\"\n\nvisualization = ZvsXY(\n    create_rosenbrock_dataset(900),\n    (\"x\", 0),\n    (\"x\", 1),\n    \"rosen\",\n    fill=False,\n    other_datasets=(initial_point, initial_training_points, acquired_points),\n)\nvisualization.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Lastly, we can compare the solution to the one obtained with COBYLA, which is another popular gradient-free optimization algorithm:</p> <pre><code>execute_algo(Rosenbrock(), \"NLOPT_COBYLA\", max_iter=40)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 18:51:02: Optimization problem:\n    INFO - 18:51:02:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 18:51:02:    with respect to x\n    INFO - 18:51:02:    over the design space:\n    INFO - 18:51:02:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:02:       | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 18:51:02:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:02:       | x[0] |      -2     |   0   |      2      | float |\n    INFO - 18:51:02:       | x[1] |      -2     |   0   |      2      | float |\n    INFO - 18:51:02:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:02: Solving optimization problem with algorithm NLOPT_COBYLA:\n    INFO - 18:51:02:      2%|\u258e         | 1/40 [00:00&lt;00:00, 2720.04 it/sec, obj=1]\n    INFO - 18:51:02:      5%|\u258c         | 2/40 [00:00&lt;00:00, 2143.23 it/sec, obj=100]\n    INFO - 18:51:02:      8%|\u258a         | 3/40 [00:00&lt;00:00, 2520.11 it/sec, obj=101]\n    INFO - 18:51:02:     10%|\u2588         | 4/40 [00:00&lt;00:00, 2082.06 it/sec, obj=148]\n    INFO - 18:51:02:     12%|\u2588\u258e        | 5/40 [00:00&lt;00:00, 2022.91 it/sec, obj=24.8]\n    INFO - 18:51:02:     15%|\u2588\u258c        | 6/40 [00:00&lt;00:00, 1975.96 it/sec, obj=3.64]\n    INFO - 18:51:02:     18%|\u2588\u258a        | 7/40 [00:00&lt;00:00, 1957.34 it/sec, obj=6.11]\n    INFO - 18:51:02:     20%|\u2588\u2588        | 8/40 [00:00&lt;00:00, 1947.89 it/sec, obj=1.88]\n    INFO - 18:51:02:     22%|\u2588\u2588\u258e       | 9/40 [00:00&lt;00:00, 1932.36 it/sec, obj=1.4]\n    INFO - 18:51:02:     25%|\u2588\u2588\u258c       | 10/40 [00:00&lt;00:00, 1932.41 it/sec, obj=0.954]\n    INFO - 18:51:02:     28%|\u2588\u2588\u258a       | 11/40 [00:00&lt;00:00, 1900.06 it/sec, obj=1.21]\n    INFO - 18:51:02:     30%|\u2588\u2588\u2588       | 12/40 [00:00&lt;00:00, 1899.38 it/sec, obj=0.861]\n    INFO - 18:51:02:     32%|\u2588\u2588\u2588\u258e      | 13/40 [00:00&lt;00:00, 1894.25 it/sec, obj=0.88]\n    INFO - 18:51:02:     35%|\u2588\u2588\u2588\u258c      | 14/40 [00:00&lt;00:00, 1901.25 it/sec, obj=0.848]\n    INFO - 18:51:02:     38%|\u2588\u2588\u2588\u258a      | 15/40 [00:00&lt;00:00, 1893.48 it/sec, obj=0.82]\n    INFO - 18:51:02:     40%|\u2588\u2588\u2588\u2588      | 16/40 [00:00&lt;00:00, 1896.27 it/sec, obj=0.751]\n    INFO - 18:51:02:     42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:00&lt;00:00, 1891.83 it/sec, obj=0.759]\n    INFO - 18:51:02:     45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:00&lt;00:00, 1888.85 it/sec, obj=0.753]\n    INFO - 18:51:02:     48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:00&lt;00:00, 1887.04 it/sec, obj=0.778]\n    INFO - 18:51:02:     50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:00&lt;00:00, 1883.81 it/sec, obj=0.744]\n    INFO - 18:51:02:     52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:00&lt;00:00, 1882.54 it/sec, obj=0.744]\n    INFO - 18:51:02:     55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:00&lt;00:00, 1881.93 it/sec, obj=0.748]\n    INFO - 18:51:02:     57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:00&lt;00:00, 1885.74 it/sec, obj=0.73]\n    INFO - 18:51:02:     60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:00&lt;00:00, 1885.47 it/sec, obj=0.724]\n    INFO - 18:51:02:     62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:00&lt;00:00, 1888.54 it/sec, obj=0.711]\n    INFO - 18:51:02:     65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:00&lt;00:00, 1887.17 it/sec, obj=0.698]\n    INFO - 18:51:02:     68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:00&lt;00:00, 1887.44 it/sec, obj=0.674]\n    INFO - 18:51:02:     70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:00&lt;00:00, 1885.26 it/sec, obj=0.651]\n    INFO - 18:51:02:     72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:00&lt;00:00, 1867.89 it/sec, obj=0.627]\n    INFO - 18:51:02:     75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:00&lt;00:00, 1869.01 it/sec, obj=0.606]\n    INFO - 18:51:02:     78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [00:00&lt;00:00, 1869.63 it/sec, obj=0.59]\n    INFO - 18:51:02:     80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [00:00&lt;00:00, 1869.95 it/sec, obj=0.565]\n    INFO - 18:51:02:     82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [00:00&lt;00:00, 1867.78 it/sec, obj=0.543]\n    INFO - 18:51:02:     85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [00:00&lt;00:00, 1870.74 it/sec, obj=0.512]\n    INFO - 18:51:02:     88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [00:00&lt;00:00, 1870.74 it/sec, obj=0.524]\n    INFO - 18:51:02:     90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [00:00&lt;00:00, 1873.55 it/sec, obj=0.583]\n    INFO - 18:51:02:     92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [00:00&lt;00:00, 1872.91 it/sec, obj=0.495]\n    INFO - 18:51:02:     95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [00:00&lt;00:00, 1875.74 it/sec, obj=0.495]\n    INFO - 18:51:02:     98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [00:00&lt;00:00, 1873.72 it/sec, obj=0.483]\n    INFO - 18:51:02:    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:00&lt;00:00, 1875.20 it/sec, obj=0.474]\n    INFO - 18:51:02: Optimization result:\n    INFO - 18:51:02:    Optimizer info:\n    INFO - 18:51:02:       Status: None\n    INFO - 18:51:02:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 18:51:02:       Number of calls to the objective function by the optimizer: 42\n    INFO - 18:51:02:    Solution:\n    INFO - 18:51:02:       Objective: 0.4736299669847872\n    INFO - 18:51:02:       Design space:\n    INFO - 18:51:02:          +------+-------------+---------------------+-------------+-------+\n    INFO - 18:51:02:          | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 18:51:02:          +------+-------------+---------------------+-------------+-------+\n    INFO - 18:51:02:          | x[0] |      -2     |  0.3123774247774849 |      2      | float |\n    INFO - 18:51:02:          | x[1] |      -2     | 0.09474211955640266 |      2      | float |\n    INFO - 18:51:02:          +------+-------------+---------------------+-------------+-------+\n</code></pre> Optimization result:<ul><li>Design variables: [0.31237742 0.09474212]</li><li>Objective function: 0.4736299669847872</li><li>Feasible solution: True</li></ul> <p>and conclude that for this problem and this initial guess, the surrogate-based algorithm is better than COBYLA.</p> <p>Total running time of the script: ( 0 minutes  11.951 seconds)</p> <p> Download Python source code: plot_sbo_ego.py</p> <p> Download Jupyter notebook: plot_sbo_ego.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/plot_smt_ego/","title":"Surrogate-based optimization using SMT.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/plot_smt_ego/#surrogate-based-optimization-using-smt","title":"Surrogate-based optimization using SMT.","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling, with a focus on derivatives. Bayesian optimization features are also available through its <code>EGO</code> class, with various acquisition criteria and strategies to acquire points in parallel.</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import configure_logger\nfrom gemseo import execute_algo\nfrom gemseo.post.dataset.zvsxy import ZvsXY\nfrom gemseo.problems.dataset.rosenbrock import create_rosenbrock_dataset\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nconfigure_logger()\n</code></pre> <p>Out:</p> <pre><code>&lt;RootLogger root (INFO)&gt;\n</code></pre> <p>In this example, we seek to minimize the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\) over the design space \\([-2,2]^2\\). First, we instantiate the problem with \\((0, 0)\\) as initial guess:</p> <pre><code>problem = Rosenbrock()\n</code></pre> <p>Then, we minimize the Rosenbrock function using:</p> <ul> <li>the <code>\"SMT_EGO\"</code> algorithm,</li> <li>a maximum number of evaluations equal to 40,   including the initial one at the center of the design space   (this first point is common to all optimization algorithms)   and the initial training dataset,</li> <li> <p>its default settings,   namely</p> </li> <li> <p>the expected improvement as acquisition criterion,</p> </li> <li>1 point acquired at a time,</li> <li>the Kriging-based surrogate model <code>\"KRG\"</code>,</li> <li>10 initial training points based on a latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion     from 50 start points with a limit of 20 iterations per local optimization.</li> </ul> <pre><code>execute_algo(problem, \"SMT_EGO\", max_iter=40)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 18:51:03: Optimization problem:\n    INFO - 18:51:03:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 18:51:03:    with respect to x\n    INFO - 18:51:03:    over the design space:\n    INFO - 18:51:03:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:03:       | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 18:51:03:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:03:       | x[0] |      -2     |   0   |      2      | float |\n    INFO - 18:51:03:       | x[1] |      -2     |   0   |      2      | float |\n    INFO - 18:51:03:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:03: Solving optimization problem with algorithm SMT_EGO:\n    INFO - 18:51:03:      2%|\u258e         | 1/40 [00:00&lt;00:00, 2809.31 it/sec, obj=1]\n    INFO - 18:51:03:      5%|\u258c         | 2/40 [00:00&lt;00:02, 14.26 it/sec, obj=6.2]\n    INFO - 18:51:03:      8%|\u258a         | 3/40 [00:00&lt;00:01, 21.32 it/sec, obj=613]\n    INFO - 18:51:03:     10%|\u2588         | 4/40 [00:00&lt;00:01, 28.29 it/sec, obj=174]\n    INFO - 18:51:03:     12%|\u2588\u258e        | 5/40 [00:00&lt;00:00, 35.22 it/sec, obj=760]\n    INFO - 18:51:03:     15%|\u2588\u258c        | 6/40 [00:00&lt;00:00, 42.10 it/sec, obj=1.78e+3]\n    INFO - 18:51:03:     18%|\u2588\u258a        | 7/40 [00:00&lt;00:00, 48.93 it/sec, obj=534]\n    INFO - 18:51:03:     20%|\u2588\u2588        | 8/40 [00:00&lt;00:00, 55.72 it/sec, obj=210]\n    INFO - 18:51:03:     22%|\u2588\u2588\u258e       | 9/40 [00:00&lt;00:00, 62.45 it/sec, obj=303]\n    INFO - 18:51:03:     25%|\u2588\u2588\u258c       | 10/40 [00:00&lt;00:00, 69.15 it/sec, obj=83.5]\n    INFO - 18:51:03:     28%|\u2588\u2588\u258a       | 11/40 [00:00&lt;00:00, 75.78 it/sec, obj=125]\n    INFO - 18:51:04:     30%|\u2588\u2588\u2588       | 12/40 [00:01&lt;00:02,  9.59 it/sec, obj=0.682]\n    INFO - 18:51:05:     32%|\u2588\u2588\u2588\u258e      | 13/40 [00:02&lt;00:05,  5.15 it/sec, obj=9.17]\n    INFO - 18:51:06:     35%|\u2588\u2588\u2588\u258c      | 14/40 [00:03&lt;00:06,  3.92 it/sec, obj=0.0477]\n    INFO - 18:51:07:     38%|\u2588\u2588\u2588\u258a      | 15/40 [00:04&lt;00:07,  3.34 it/sec, obj=401]\n    INFO - 18:51:08:     40%|\u2588\u2588\u2588\u2588      | 16/40 [00:05&lt;00:08,  2.87 it/sec, obj=9.16]\n    INFO - 18:51:09:     42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:06&lt;00:08,  2.64 it/sec, obj=0.101]\n    INFO - 18:51:10:     45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:07&lt;00:08,  2.51 it/sec, obj=4.81]\n    INFO - 18:51:10:     48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:07&lt;00:08,  2.44 it/sec, obj=120]\n    INFO - 18:51:11:     50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:08&lt;00:08,  2.34 it/sec, obj=4.25]\n    INFO - 18:51:12:     52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:09&lt;00:08,  2.25 it/sec, obj=399]\n    INFO - 18:51:13:     55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:10&lt;00:08,  2.17 it/sec, obj=347]\n    INFO - 18:51:14:     57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:10&lt;00:08,  2.10 it/sec, obj=26.6]\n    INFO - 18:51:14:     60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:11&lt;00:07,  2.05 it/sec, obj=1.76]\n    INFO - 18:51:15:     62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:12&lt;00:07,  2.01 it/sec, obj=748]\n    INFO - 18:51:16:     65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:13&lt;00:07,  1.95 it/sec, obj=10.7]\n    INFO - 18:51:17:     68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:14&lt;00:06,  1.91 it/sec, obj=145]\n    INFO - 18:51:17:     70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:14&lt;00:06,  1.88 it/sec, obj=1.82e+3]\n    INFO - 18:51:18:     72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:15&lt;00:05,  1.85 it/sec, obj=76.9]\n    INFO - 18:51:19:     75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:16&lt;00:05,  1.81 it/sec, obj=4.14]\n    INFO - 18:51:20:     78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [00:17&lt;00:05,  1.78 it/sec, obj=35.1]\n    INFO - 18:51:21:     80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [00:18&lt;00:04,  1.75 it/sec, obj=291]\n    INFO - 18:51:22:     82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [00:19&lt;00:04,  1.73 it/sec, obj=55.6]\n    INFO - 18:51:22:     85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [00:19&lt;00:03,  1.71 it/sec, obj=7.48]\n    INFO - 18:51:23:     88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [00:20&lt;00:02,  1.69 it/sec, obj=29.8]\n    INFO - 18:51:24:     90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [00:21&lt;00:02,  1.67 it/sec, obj=371]\n    INFO - 18:51:25:     92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [00:22&lt;00:01,  1.65 it/sec, obj=24.2]\n    INFO - 18:51:26:     95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [00:23&lt;00:01,  1.63 it/sec, obj=28.2]\n    INFO - 18:51:27:     98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [00:24&lt;00:00,  1.61 it/sec, obj=18.4]\n    INFO - 18:51:28:    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:25&lt;00:00,  1.60 it/sec, obj=36.1]\n    INFO - 18:51:28: Optimization result:\n    INFO - 18:51:28:    Optimizer info:\n    INFO - 18:51:28:       Status: None\n    INFO - 18:51:28:       Message: None\n    INFO - 18:51:28:       Number of calls to the objective function by the optimizer: 40\n    INFO - 18:51:28:    Solution:\n    INFO - 18:51:28:       Objective: [0.04770946]\n    INFO - 18:51:28:       Design space:\n    INFO - 18:51:28:          +------+-------------+--------------------+-------------+-------+\n    INFO - 18:51:28:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 18:51:28:          +------+-------------+--------------------+-------------+-------+\n    INFO - 18:51:28:          | x[0] |      -2     | 0.7466740126279879 |      2      | float |\n    INFO - 18:51:28:          | x[1] |      -2     | 0.7488427592686571 |      2      | float |\n    INFO - 18:51:28:          +------+-------------+--------------------+-------------+-------+\n</code></pre> Optimization result:<ul><li>Design variables: [0.74667401 0.74884276]</li><li>Objective function: [0.04770946]</li><li>Feasible solution: True</li></ul> <p>We can see that the solution is close to the theoretical one \\((x^*,f^*)=((1,1),0)\\).</p> <p>We can also visualize all the evaluations and note that most of the points have been added in the valley as expected:</p> <pre><code>optimization_history = problem.to_dataset()\n\ninitial_point = optimization_history[0:1]\ninitial_point.name = \"Initial point\"\n\ninitial_training_points = optimization_history[1:12]\ninitial_training_points.name = \"Initial training points\"\n\nacquired_points = optimization_history[12:]\nacquired_points.name = \"Acquired points\"\n\nvisualization = ZvsXY(\n    create_rosenbrock_dataset(900),\n    (\"x\", 0),\n    (\"x\", 1),\n    \"rosen\",\n    fill=False,\n    other_datasets=(initial_point, initial_training_points, acquired_points),\n)\nvisualization.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Lastly, we can compare the solution to the one obtained with COBYLA, which is another popular gradient-free optimization algorithm:</p> <pre><code>execute_algo(Rosenbrock(), \"NLOPT_COBYLA\", max_iter=40)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 18:51:28: Optimization problem:\n    INFO - 18:51:28:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 18:51:28:    with respect to x\n    INFO - 18:51:28:    over the design space:\n    INFO - 18:51:28:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:28:       | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 18:51:28:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:28:       | x[0] |      -2     |   0   |      2      | float |\n    INFO - 18:51:28:       | x[1] |      -2     |   0   |      2      | float |\n    INFO - 18:51:28:       +------+-------------+-------+-------------+-------+\n    INFO - 18:51:28: Solving optimization problem with algorithm NLOPT_COBYLA:\n    INFO - 18:51:28:      2%|\u258e         | 1/40 [00:00&lt;00:00, 2339.27 it/sec, obj=1]\n    INFO - 18:51:28:      5%|\u258c         | 2/40 [00:00&lt;00:00, 1905.64 it/sec, obj=100]\n    INFO - 18:51:28:      8%|\u258a         | 3/40 [00:00&lt;00:00, 2223.52 it/sec, obj=101]\n    INFO - 18:51:28:     10%|\u2588         | 4/40 [00:00&lt;00:00, 1970.31 it/sec, obj=148]\n    INFO - 18:51:28:     12%|\u2588\u258e        | 5/40 [00:00&lt;00:00, 1923.64 it/sec, obj=24.8]\n    INFO - 18:51:28:     15%|\u2588\u258c        | 6/40 [00:00&lt;00:00, 1887.20 it/sec, obj=3.64]\n    INFO - 18:51:28:     18%|\u2588\u258a        | 7/40 [00:00&lt;00:00, 1885.08 it/sec, obj=6.11]\n    INFO - 18:51:28:     20%|\u2588\u2588        | 8/40 [00:00&lt;00:00, 1867.14 it/sec, obj=1.88]\n    INFO - 18:51:28:     22%|\u2588\u2588\u258e       | 9/40 [00:00&lt;00:00, 1873.11 it/sec, obj=1.4]\n    INFO - 18:51:28:     25%|\u2588\u2588\u258c       | 10/40 [00:00&lt;00:00, 1848.69 it/sec, obj=0.954]\n    INFO - 18:51:28:     28%|\u2588\u2588\u258a       | 11/40 [00:00&lt;00:00, 1849.49 it/sec, obj=1.21]\n    INFO - 18:51:28:     30%|\u2588\u2588\u2588       | 12/40 [00:00&lt;00:00, 1839.47 it/sec, obj=0.861]\n    INFO - 18:51:28:     32%|\u2588\u2588\u2588\u258e      | 13/40 [00:00&lt;00:00, 1829.98 it/sec, obj=0.88]\n    INFO - 18:51:28:     35%|\u2588\u2588\u2588\u258c      | 14/40 [00:00&lt;00:00, 1836.16 it/sec, obj=0.848]\n    INFO - 18:51:28:     38%|\u2588\u2588\u2588\u258a      | 15/40 [00:00&lt;00:00, 1831.73 it/sec, obj=0.82]\n    INFO - 18:51:28:     40%|\u2588\u2588\u2588\u2588      | 16/40 [00:00&lt;00:00, 1839.51 it/sec, obj=0.751]\n    INFO - 18:51:28:     42%|\u2588\u2588\u2588\u2588\u258e     | 17/40 [00:00&lt;00:00, 1837.76 it/sec, obj=0.759]\n    INFO - 18:51:28:     45%|\u2588\u2588\u2588\u2588\u258c     | 18/40 [00:00&lt;00:00, 1845.09 it/sec, obj=0.753]\n    INFO - 18:51:28:     48%|\u2588\u2588\u2588\u2588\u258a     | 19/40 [00:00&lt;00:00, 1844.20 it/sec, obj=0.778]\n    INFO - 18:51:28:     50%|\u2588\u2588\u2588\u2588\u2588     | 20/40 [00:00&lt;00:00, 1845.80 it/sec, obj=0.744]\n    INFO - 18:51:28:     52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 21/40 [00:00&lt;00:00, 1844.96 it/sec, obj=0.744]\n    INFO - 18:51:28:     55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 22/40 [00:00&lt;00:00, 1848.90 it/sec, obj=0.748]\n    INFO - 18:51:28:     57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 23/40 [00:00&lt;00:00, 1844.92 it/sec, obj=0.73]\n    INFO - 18:51:28:     60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 24/40 [00:00&lt;00:00, 1846.05 it/sec, obj=0.724]\n    INFO - 18:51:28:     62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 25/40 [00:00&lt;00:00, 1848.33 it/sec, obj=0.711]\n    INFO - 18:51:28:     65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 26/40 [00:00&lt;00:00, 1840.38 it/sec, obj=0.698]\n    INFO - 18:51:28:     68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 27/40 [00:00&lt;00:00, 1845.72 it/sec, obj=0.674]\n    INFO - 18:51:28:     70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 28/40 [00:00&lt;00:00, 1844.84 it/sec, obj=0.651]\n    INFO - 18:51:28:     72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 29/40 [00:00&lt;00:00, 1849.45 it/sec, obj=0.627]\n    INFO - 18:51:28:     75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 30/40 [00:00&lt;00:00, 1849.69 it/sec, obj=0.606]\n    INFO - 18:51:28:     78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 31/40 [00:00&lt;00:00, 1853.35 it/sec, obj=0.59]\n    INFO - 18:51:28:     80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 32/40 [00:00&lt;00:00, 1852.63 it/sec, obj=0.565]\n    INFO - 18:51:28:     82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 33/40 [00:00&lt;00:00, 1856.98 it/sec, obj=0.543]\n    INFO - 18:51:28:     85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 34/40 [00:00&lt;00:00, 1857.05 it/sec, obj=0.512]\n    INFO - 18:51:28:     88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 35/40 [00:00&lt;00:00, 1859.93 it/sec, obj=0.524]\n    INFO - 18:51:28:     90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 36/40 [00:00&lt;00:00, 1860.05 it/sec, obj=0.583]\n    INFO - 18:51:28:     92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 37/40 [00:00&lt;00:00, 1864.54 it/sec, obj=0.495]\n    INFO - 18:51:28:     95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 38/40 [00:00&lt;00:00, 1865.31 it/sec, obj=0.495]\n    INFO - 18:51:28:     98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 39/40 [00:00&lt;00:00, 1868.48 it/sec, obj=0.483]\n    INFO - 18:51:28:    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:00&lt;00:00, 1868.72 it/sec, obj=0.474]\n    INFO - 18:51:28: Optimization result:\n    INFO - 18:51:28:    Optimizer info:\n    INFO - 18:51:28:       Status: None\n    INFO - 18:51:28:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 18:51:28:       Number of calls to the objective function by the optimizer: 42\n    INFO - 18:51:28:    Solution:\n    INFO - 18:51:28:       Objective: 0.4736299669847872\n    INFO - 18:51:28:       Design space:\n    INFO - 18:51:28:          +------+-------------+---------------------+-------------+-------+\n    INFO - 18:51:28:          | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 18:51:28:          +------+-------------+---------------------+-------------+-------+\n    INFO - 18:51:28:          | x[0] |      -2     |  0.3123774247774849 |      2      | float |\n    INFO - 18:51:28:          | x[1] |      -2     | 0.09474211955640266 |      2      | float |\n    INFO - 18:51:28:          +------+-------------+---------------------+-------------+-------+\n</code></pre> Optimization result:<ul><li>Design variables: [0.31237742 0.09474212]</li><li>Objective function: 0.4736299669847872</li><li>Feasible solution: True</li></ul> <p>and conclude that for this problem and this initial guess, the surrogate-based algorithm is better than COBYLA.</p> <p>Total running time of the script: ( 0 minutes  25.349 seconds)</p> <p> Download Python source code: plot_smt_ego.py</p> <p> Download Jupyter notebook: plot_smt_ego.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/","title":"Regression models","text":""},{"location":"generated/examples/regression/#regression-models","title":"Regression models","text":"<p> SMT's surrogate model. </p> <p> Download all examples in Python source code: regression_python.zip</p> <p> Download all examples in Jupyter notebooks: regression_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/mg_execution_times/","title":"Computation times","text":"<p>00:06.258 total execution time for generated_examples_regression files:</p> <p>+------------------------------------------------------------------------------------------------+-----------+--------+ | plot_smt_regressor (docs/examples/regression/plot_smt_regressor.py) | 00:06.258 | 0.0 MB | +------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/regression/plot_smt_regressor/","title":"SMT's surrogate model.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/regression/plot_smt_regressor/#smts-surrogate-model","title":"SMT's surrogate model.","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling, with a focus on derivatives. The SMTRegressor class allows you to use any SMT's surrogate model in your GEMSEO processes, including the gradient-enhanced surrogate models as long as your training dataset includes both output and gradient samples as explained at the end of this page.</p> <p>In this example, we will approximate the [Rosenbrock function]<sup>1</sup></p> \\[f(x,y) = (1-x)^2 + 100(y-x^2)^2\\] <p>over the domain \\([-2,2]^2\\).</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import compute_doe\nfrom gemseo import sample_disciplines\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom gemseo.post.dataset.zvsxy import ZvsXY\n\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\nfrom gemseo_mlearning.regression.smt_regressor import SMTRegressor\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.9/site-packages/gemseo/third_party/prettytable/prettytable.py:73: DeprecationWarning: invalid escape sequence \\[\n  _re = re.compile(\"\\033\\[[0-9;]*m\")\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.9/site-packages/gemseo/third_party/prettytable/prettytable.py:1291: DeprecationWarning: invalid escape sequence \\{\n  self.vertical_char = random.choice(\"~!@#$%^&amp;*()_+|-=\\{}[];':\\\",./;&lt;&gt;?\")\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.9/site-packages/gemseo/third_party/prettytable/prettytable.py:1292: DeprecationWarning: invalid escape sequence \\{\n  self.horizontal_char = random.choice(\"~!@#$%^&amp;*()_+|-=\\{}[];':\\\",./;&lt;&gt;?\")\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.9/site-packages/gemseo/third_party/prettytable/prettytable.py:1293: DeprecationWarning: invalid escape sequence \\{\n  self.junction_char = random.choice(\"~!@#$%^&amp;*()_+|-=\\{}[];':\\\",./;&lt;&gt;?\")\n</code></pre> <p>First, we create the Rosenbrock discipline:</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>and the input space:</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>Then, we use an optimized Latin hypercube sampling (LHS) technique to generate 20 samples:</p> <pre><code>training_data = sample_disciplines(\n    [discipline], input_space, \"y\", \"OT_OPT_LHS\", n_samples=20\n)\n</code></pre> <p>From this learning dataset, we train an SMTRegressor based on the SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>surrogate_model = SMTRegressor(\n    training_data, model_class_name=\"RBF\", parameters={\"d0\": 2}\n)\nsurrogate_model.learn()\n</code></pre> <p>Finally, we assess its quality:</p> <pre><code>r2 = R2Measure(surrogate_model)\nr2_l = r2.compute_learning_measure()[0]\nr2_cv = r2.compute_cross_validation_measure()[0]\ntest_data = sample_disciplines(\n    [discipline], input_space, \"y\", \"OT_MONTE_CARLO\", n_samples=1000\n)\nr2_t = r2.compute_test_measure(test_data)[0]\nf\"Learning R2: {r2_l}; cross-validation R2: {r2_cv}; test R2: {r2_t}\"\n</code></pre> <p>Out:</p> <pre><code>'Learning R2: 1.0; cross-validation R2: 0.8257126302519093; test R2: 0.9617160347610478'\n</code></pre> <p>see how good it is with its R2 close to 1 on the test dataset, and plot its output over a 20x20 grid:</p> <pre><code>input_data = compute_doe(input_space, \"fullfact\", n_samples=400)\noutput_data = surrogate_model.predict(input_data)\npredictions = IODataset()\npredictions.add_input_group(input_data, variable_names=[\"x1\", \"x2\"])\npredictions.add_output_group(output_data, variable_names=[\"y\"])\n\nplot = ZvsXY(predictions, \"x1\", \"x2\", \"y\", other_datasets=(training_data,))\nplot.color = \"white\"\nplot.colormap = \"viridis\"\nplot.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Total running time of the script: ( 0 minutes  6.258 seconds)</p> <p> Download Python source code: plot_smt_regressor.py</p> <p> Download Jupyter notebook: plot_smt_regressor.ipynb</p> <p>Gallery generated by mkdocs-gallery</p> <ol> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_mlearning<ul> <li>active_learning<ul> <li>acquisition_criteria<ul> <li>base_acquisition_criterion</li> <li>base_acquisition_criterion_family</li> <li>base_factory</li> <li>exploration<ul> <li>base_exploration</li> <li>distance</li> <li>exploration</li> <li>factory</li> <li>standard_deviation</li> <li>variance</li> </ul> </li> <li>level_set<ul> <li>base_ei_ef</li> <li>base_level_set</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>level_set</li> <li>u</li> </ul> </li> <li>maximum<ul> <li>base_maximum</li> <li>ei</li> <li>factory</li> <li>maximum</li> <li>output</li> <li>ucb</li> </ul> </li> <li>minimum<ul> <li>base_minimum</li> <li>ei</li> <li>factory</li> <li>lcb</li> <li>minimum</li> <li>output</li> </ul> </li> <li>quantile<ul> <li>base_quantile</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>quantile</li> <li>u</li> </ul> </li> </ul> </li> <li>active_learning_algo</li> <li>distributions<ul> <li>base_regressor_distribution</li> <li>kriging_distribution</li> <li>regressor_distribution</li> </ul> </li> <li>visualization<ul> <li>acquisition_view</li> <li>qoi_history_view</li> </ul> </li> </ul> </li> <li>algos<ul> <li>opt<ul> <li>core<ul> <li>surrogate_based_optimizer</li> </ul> </li> <li>sbo_settings</li> <li>smt<ul> <li>ego_settings</li> <li>smt_ego</li> </ul> </li> <li>surrogate_based_optimization</li> </ul> </li> </ul> </li> <li>problems<ul> <li>branin<ul> <li>branin_discipline</li> <li>branin_function</li> <li>branin_problem</li> <li>branin_space</li> <li>functions</li> </ul> </li> <li>rosenbrock<ul> <li>functions</li> <li>rosenbrock_discipline</li> <li>rosenbrock_function</li> <li>rosenbrock_problem</li> <li>rosenbrock_space</li> </ul> </li> </ul> </li> <li>regression<ul> <li>smt_regressor</li> <li>smt_regressor_settings</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/gemseo_mlearning/","title":"API documentation","text":""},{"location":"reference/gemseo_mlearning/#gemseo_mlearning","title":"gemseo_mlearning","text":"<p>A GEMSEO extension for advanced machine learning.</p> <p>GEMSEO includes the main machine learning capabilities.</p>"},{"location":"reference/gemseo_mlearning/active_learning/","title":"Active learning","text":""},{"location":"reference/gemseo_mlearning/active_learning/#gemseo_mlearning.active_learning","title":"active_learning","text":"<p>Active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/","title":"Active learning algo","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo","title":"active_learning_algo","text":"<p>Active learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo","title":"ActiveLearningAlgo","text":"<pre><code>ActiveLearningAlgo(\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10000,\n    **criterion_arguments: Any\n)\n</code></pre> <p>An active learning algorithm using a regressor and acquisition criteria.</p> <p>Parameters:</p> <ul> <li> <code>criterion_family_name</code>               (<code>str</code>)           \u2013            <p>The name of a family of acquisition criteria, e.g. <code>\"Minimum\"</code>, <code>\"Maximum\"</code>, <code>\"LevelSet\"</code>, <code>\"Quantile\"</code> or <code>\"Exploration\"</code>.</p> </li> <li> <code>input_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The input space on which to look for the new learning point.</p> </li> <li> <code>regressor</code>               (<code>BaseRegressor | BaseRegressorDistribution</code>)           \u2013            <p>Either a regressor or a regressor distribution.</p> </li> <li> <code>criterion_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the acquisition criterion. If empty, use the default criterion of the family <code>criterion_family_name</code>.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criteria in parallel.</p> </li> <li> <code>**criterion_arguments</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The parameters of the acquisition criterion.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>When the output dimension is greater than 1.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def __init__(\n    self,\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n    **criterion_arguments: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        criterion_family_name: The name of a family of acquisition criteria,\n            *e.g.* `\"Minimum\"`, `\"Maximum\"`, `\"LevelSet\"`, `\"Quantile\"`\n            or `\"Exploration\"`.\n        input_space: The input space on which to look for the new learning point.\n        regressor: Either a regressor or a regressor distribution.\n        criterion_name: The name of the acquisition criterion.\n            If empty,\n            use the default criterion of the family `criterion_family_name`.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criteria in parallel.\n        **criterion_arguments: The parameters of the acquisition criterion.\n\n    Raises:\n        NotImplementedError: When the output dimension is greater than 1.\n    \"\"\"  # noqa: D205 D212 D415\n    # Create the regressor distribution.\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        distribution = KrigingDistribution(regressor)\n        distribution.learn()\n    elif isinstance(regressor, BaseRegressor):\n        distribution = RegressorDistribution(regressor)\n        distribution.learn()\n    else:\n        distribution = regressor\n\n    if distribution.output_dimension &gt; 1:\n        msg = \"ActiveLearningAlgo works only with scalar output.\"\n        raise NotImplementedError(msg)\n\n    # Create the acquisition problem.\n    family_factory = AcquisitionCriterionFamilyFactory()\n    self.__criterion_family_name = criterion_family_name\n    criterion_family = family_factory.get_class(criterion_family_name)\n    criterion_factory = criterion_family.ACQUISITION_CRITERION_FACTORY()\n    self.__acquisition_criterion = criterion_factory.create(\n        criterion_name,\n        distribution,\n        batch_size=batch_size,\n        mc_size=mc_size,\n        **criterion_arguments,\n    )\n    # Create the optimization space\n    # that is different from the input space\n    # when acquiring points in parallel.\n    optimization_space = DesignSpace()\n    lower_bound = tile(input_space.get_lower_bounds(), batch_size)\n    upper_bound = tile(input_space.get_upper_bounds(), batch_size)\n    input_space.initialize_missing_current_values()\n    value = tile(input_space.get_current_value(), batch_size)\n    optimization_space.add_variable(\n        \"x\",\n        size=int(len(lower_bound)),\n        lower_bound=lower_bound,\n        upper_bound=upper_bound,\n        value=value,\n    )\n    problem = self.__acquisition_problem = OptimizationProblem(optimization_space)\n    problem.objective = self.__acquisition_criterion\n    if not problem.objective.has_jac:\n        problem.differentiation_method = (\n            OptimizationProblem.DifferentiationMethod.FINITE_DIFFERENCES\n        )\n    if problem.objective.MAXIMIZE:\n        problem.minimize_objective = False\n\n    # Initialize acquisition algorithm.\n    self.set_acquisition_algorithm(\n        self.__default_algo_name, **self.__default_algo_settings\n    )\n\n    # Miscellaneous.\n    self.__database = Database()\n    self.__n_initial_samples = len(distribution.algo.learning_set)\n    self.__distribution = distribution\n    self.__input_space = input_space\n    self.__batch_size = batch_size\n\n    # Create the acquisition view.\n    if distribution.algo.input_dimension == 2 and batch_size == 1:\n        self.__acquisition_view = AcquisitionView(self)\n    else:\n        self.__acquisition_view = None\n\n    self.__n_evaluations_history = []\n    self.__qoi_history = []\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion","title":"acquisition_criterion  <code>property</code>","text":"<pre><code>acquisition_criterion: BaseAcquisitionCriterion\n</code></pre> <p>The acquisition criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion_family_name","title":"acquisition_criterion_family_name  <code>property</code>","text":"<pre><code>acquisition_criterion_family_name: str\n</code></pre> <p>The name of the acquisition criterion family.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.batch_size","title":"batch_size  <code>property</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>The number of points to be acquired in parallel.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.input_space","title":"input_space  <code>property</code>","text":"<pre><code>input_space: DesignSpace\n</code></pre> <p>The input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.n_initial_samples","title":"n_initial_samples  <code>property</code>","text":"<pre><code>n_initial_samples: int\n</code></pre> <p>The number of initial samples.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: float | None\n</code></pre> <p>The quantity of interest (QOI).</p> <p>When there is no quantity of interest associated with the acquisition criterion, this attribute is <code>None</code>.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi_history","title":"qoi_history  <code>property</code>","text":"<pre><code>qoi_history: tuple[list[int], list[float]]\n</code></pre> <p>The history of the quantity of interest (QOI) when it exists.</p> <p>The second term represents this history while the first one represents the history of the number of model evaluations corresponding to these QOI estimations.</p> <p>When there is no quantity of interest associated with the acquisition criterion, these lists are empty.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor","title":"regressor  <code>property</code>","text":"<pre><code>regressor: BaseRegressor\n</code></pre> <p>The regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor_distribution","title":"regressor_distribution  <code>property</code>","text":"<pre><code>regressor_distribution: BaseRegressorDistribution\n</code></pre> <p>The regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquire_new_points","title":"acquire_new_points","text":"<pre><code>acquire_new_points(\n    discipline: Discipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]\n</code></pre> <p>Update the machine learning algorithm by learning new samples.</p> <p>This method acquires new learning input-output samples and trains the machine learning algorithm with the resulting enriched learning set. The effective number of points will be the largest integer multiple of batch_size and less than or equal to n_samples.</p> <p>Parameters:</p> <ul> <li> <code>discipline</code>               (<code>Discipline</code>)           \u2013            <p>The discipline computing the reference output data from the input data provided by the acquisition process.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of samples to update the machine learning algorithm. It should be a multiple of batch_size.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display intermediate results Only when the input space dimension is 2.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the plots of the intermediate results. If empty, do not save them. Only when the input space dimension is 2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Database, OptimizationProblem]</code>           \u2013            <p>The concatenation of the optimization histories related to the different points and the last acquisition problem.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the input space dimension is not 2.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def acquire_new_points(\n    self,\n    discipline: Discipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]:\n    \"\"\"Update the machine learning algorithm by learning new samples.\n\n    This method acquires new learning input-output samples\n    and trains the machine learning algorithm\n    with the resulting enriched learning set.\n    The effective number of points will be the largest integer multiple\n    of batch_size and less than or equal to n_samples.\n\n    Args:\n        discipline: The discipline computing the reference output data\n            from the input data provided by the acquisition process.\n        n_samples: The number of samples to update the machine learning algorithm.\n            It should be a multiple of batch_size.\n        show: Whether to display intermediate results\n            Only when the input space dimension is 2.\n        file_path: The file path to save the plots of the intermediate results.\n            If empty, do not save them.\n            Only when the input space dimension is 2.\n\n    Returns:\n        The concatenation of the optimization histories\n        related to the different points\n        and the last acquisition problem.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    plot = show or file_path\n    if plot:\n        self.__check_acquisition_view()\n\n    self.__n_evaluations_history.append(self.__n_initial_samples)\n    self.__qoi_history.append(self.__acquisition_criterion.qoi)\n    total_n_samples = self.__n_initial_samples\n    n_batches = int(n_samples / self.__batch_size)\n    LOGGER.info(\"Acquiring %s points in batches of %s\", n_samples, self.batch_size)\n    with OneLineLogging(TQDM_LOGGER):\n        for batch_id in CustomTqdmProgressBar(range(1, n_batches + 1)):\n            array_input_data = self.find_next_point()\n            if plot:\n                self.__acquisition_view.draw(\n                    discipline=discipline,\n                    new_point=array_input_data[0],\n                    show=show,\n                    file_path=file_path,\n                )\n\n            for inputs, outputs in self.__acquisition_problem.database.items():\n                self.__database.store(\n                    array([batch_id, *inputs.unwrap().tolist()]), outputs\n                )\n\n            for points in range(self.__batch_size):\n                input_data = self.__input_space.convert_array_to_dict(\n                    array_input_data[points, :]\n                )\n\n                discipline.execute(input_data)\n\n                extra_learning_set = IODataset()\n                distribution = self.__distribution\n                variable_names_to_n_components = distribution.algo.sizes\n                new_points = hstack(list(input_data.values()))[newaxis]\n                extra_learning_set.add_group(\n                    group_name=IODataset.INPUT_GROUP,\n                    data=new_points,\n                    variable_names=distribution.input_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                output_names = distribution.output_names\n                output_data = discipline.get_output_data()\n                extra_learning_set.add_group(\n                    group_name=IODataset.OUTPUT_GROUP,\n                    data=hstack(list(output_data.values()))[newaxis],\n                    variable_names=output_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                augmented_learning_set = concat(\n                    [distribution.algo.learning_set, extra_learning_set],\n                    ignore_index=True,\n                )\n\n                self.__distribution.change_learning_set(augmented_learning_set)\n\n            self.update_problem()\n            self.__qoi_history.append(self.__acquisition_criterion.qoi)\n            total_n_samples += self.__batch_size\n            self.__n_evaluations_history.append(total_n_samples)\n\n    return self.__database, self.__acquisition_problem\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.find_next_point","title":"find_next_point","text":"<pre><code>find_next_point(as_dict: bool = False) -&gt; DataType\n</code></pre> <p>Find the next <code>batch_size</code> learning point(s).</p> <p>Parameters:</p> <ul> <li> <code>as_dict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the input data split by input names. Otherwise, return a unique array. In both cases, the arrays will be shaped as <code>(batch_size, input_dimension)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The next <code>batch_size</code> learning point(s).</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def find_next_point(\n    self,\n    as_dict: bool = False,\n) -&gt; DataType:\n    \"\"\"Find the next `batch_size` learning point(s).\n\n    Args:\n        as_dict: Whether to return the input data split by input names.\n            Otherwise, return a unique array.\n            In both cases,\n            the arrays will be shaped as ``(batch_size, input_dimension)``.\n\n    Returns:\n        The next `batch_size` learning point(s).\n    \"\"\"\n    with LoggingContext(logging.getLogger(\"gemseo\")):\n        input_data = self.__acquisition_algo.execute(\n            self.__acquisition_problem, **self.__acquisition_algo_settings\n        ).x_opt\n        input_data = input_data.reshape(self.__batch_size, -1)\n    if as_dict:\n        return self.__acquisition_problem.design_space.convert_array_to_dict(\n            input_data\n        )\n\n    return input_data\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_acquisition_view","title":"plot_acquisition_view","text":"<pre><code>plot_acquisition_view(\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>discipline</code>               (<code>Discipline | None</code>, default:                   <code>None</code> )           \u2013            <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> </li> <li> <code>n_test</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>The number of points per dimension.</p> </li> <li> <code>filled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The acquisition view.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the input space dimension is not 2.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_acquisition_view(\n    self,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    self.__check_acquisition_view()\n    return self.__acquisition_view.draw(\n        discipline=discipline,\n        filled=filled,\n        n_test=n_test,\n        show=show,\n        file_path=file_path,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_qoi_history","title":"plot_qoi_history","text":"<pre><code>plot_qoi_history(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Plot the history of the quantity of interest.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> <li> <code>label</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The label for the QOI. If empty, the name of the criterion family name.</p> </li> <li> <code>add_markers</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add markers.</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options to create the Lines object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Lines</code>           \u2013            <p>The history of the quantity of interest.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_qoi_history(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Plot the history of the quantity of interest.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n            If empty, the name of the criterion family name.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        The history of the quantity of interest.\n    \"\"\"\n    return QOIHistoryView(self).draw(\n        show=show,\n        file_path=file_path,\n        label=label,\n        add_markers=add_markers,\n        **options,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.set_acquisition_algorithm","title":"set_acquisition_algorithm","text":"<pre><code>set_acquisition_algorithm(\n    algo_name: str, **settings: Any\n) -&gt; None\n</code></pre> <p>Set sampling or optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of a DOE or optimization algorithm to find the learning point(s).</p> </li> <li> <code>**settings</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The values of some algorithm settings.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def set_acquisition_algorithm(self, algo_name: str, **settings: Any) -&gt; None:\n    \"\"\"Set sampling or optimization algorithm.\n\n    Args:\n        algo_name: The name of a DOE or optimization algorithm\n            to find the learning point(s).\n        **settings: The values of some algorithm settings.\n    \"\"\"\n    factory = DOELibraryFactory()\n    if not factory.is_available(algo_name):\n        factory = OptimizationLibraryFactory()\n\n    self.__acquisition_algo = factory.create(algo_name)\n    self.__acquisition_algo_settings = settings\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.update_problem","title":"update_problem","text":"<pre><code>update_problem() -&gt; None\n</code></pre> <p>Update the acquisition problem.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def update_problem(self) -&gt; None:\n    \"\"\"Update the acquisition problem.\"\"\"\n    self.__acquisition_problem.reset(preprocessing=False)\n    self.__acquisition_criterion.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/","title":"Acquisition criteria","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/#gemseo_mlearning.active_learning.acquisition_criteria","title":"acquisition_criteria","text":"<p>Acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/","title":"Base acquisition criterion","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion","title":"base_acquisition_criterion","text":"<p>Base class for acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion","title":"BaseAcquisitionCriterion","text":"<pre><code>BaseAcquisitionCriterion(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>Base class for acquisition criteria.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/","title":"Base acquisition criterion family","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family","title":"base_acquisition_criterion_family","text":"<p>Base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.AcquisitionCriterionFamilyFactory","title":"AcquisitionCriterionFamilyFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.BaseAcquisitionCriterionFamily","title":"BaseAcquisitionCriterionFamily","text":"<p>The base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/","title":"Base factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory","title":"base_factory","text":"<p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory","title":"BaseAcquisitionCriterionFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFamilyFactory","title":"BaseAcquisitionCriterionFamilyFactory","text":"<p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration","title":"exploration","text":"<p>Acquisition criteria for exploration.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/","title":"Base exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration","title":"base_exploration","text":"<p>Base class for the acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration","title":"BaseExploration","text":"<pre><code>BaseExploration(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class the for acquisition criteria to explore the input space.</p> <p>Optimize it to make the error of the surrogate model tends towards zero when the number of acquired points tends to infinity.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/","title":"Distance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance","title":"distance","text":"<p>Distance to the learning set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance","title":"Distance","text":"<pre><code>Distance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Distance to the learning set.</p> <p>This acquisition criterion computes the minimum distance between a new point and the point of the learning dataset, scaled by the minimum distance between two distinct learning points.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration","title":"exploration","text":"<p>Family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration.Exploration","title":"Exploration","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory","title":"factory","text":"<p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory","title":"ExplorationFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/","title":"Standard deviation","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation","title":"standard_deviation","text":"<p>Output standard deviation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation","title":"StandardDeviation","text":"<pre><code>StandardDeviation(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output standard deviation.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma[x] = \\sqrt{\\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/","title":"Variance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance","title":"variance","text":"<p>Output variance.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance","title":"Variance","text":"<pre><code>Variance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output variance.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma^2[x] = \\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set","title":"level_set","text":"<p>Acquisition criteria for level set estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/","title":"Base ei ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef","title":"base_ei_ef","text":"<p>Base class for EI and EF criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF","title":"BaseEIEF","text":"<pre><code>BaseEIEF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The base class for EI and EF criteria to approximate a level set.</p> <p>EI and EF stands for expected improvement and expected feasibility respectively. More information in the subclasses EI and EF.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/","title":"Base level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set","title":"base_level_set","text":"<p>Base class for acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet","title":"BaseLevelSet","text":"<pre><code>BaseLevelSet(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a level set estimation.</p> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float</code>)           \u2013            <p>The model output value characterizing the level set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> \\[EF[x]=\\mathbb{E}\\left[\\max\\left(\\kappa\\mathbb{S}[Y(x)]-|y-Y(x)|,0\\right)\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EF[x] = \\mathbb{S}[Y(x)] ( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) ) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left( \\max(\\kappa\\mathbb{S}[Y(x_i)] - |y - Y(x_i)|,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] =\\mathbb{E}\\left[\\max\\left((\\kappa\\mathbb{S}[Y(x)])^2-(y - Y(x))^2,0\\right)\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = \\mathbb{V}[Y(x)]\\times ( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) ) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left( \\max((\\kappa\\mathbb{S}[Y(x_i)])^2 - (y - Y(x_i))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory","title":"LevelSetFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set","title":"level_set","text":"<p>Family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set.LevelSet","title":"LevelSet","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\mathbb{E}\\left[\\left(\\frac{y-Y(x)}{\\mathbb{S}[Y(x)]} \\right)^2\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\). It has an analytic expression:</p> \\[U[x] = \\left(\\frac{y-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\right)^2.\\] <p>For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i \\leq q}\\left( \\left(\\frac{y-Y(x_i)}{\\mathbb{S}[Y(x_i)]}\\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float</code>)           \u2013            <p>The model output value characterizing the level set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum","title":"maximum","text":"<p>Acquisition criteria for maximum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/","title":"Base maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum","title":"base_maximum","text":"<p>Base class for acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum","title":"BaseMaximum","text":"<pre><code>BaseMaximum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a maximum.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMaximum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-y_{\\text{max}},0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{max}}\\) is the maximum output value in the learning set.</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = (\\mathbb{E}[Y(x)] - y_{\\text{max}})\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{\\mathbb{E}[Y(x)] - y_{\\text{max}}}{\\mathbb{S}[Y(x)]}\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(Y(x_i)-y_{\\text{max}},0)\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory","title":"MaximumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum","title":"maximum","text":"<p>Family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum.Maximum","title":"Maximum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/","title":"Output","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output","title":"output","text":"<p>Output-based criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output","title":"Output","text":"<pre><code>Output(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Output</code>, <code>BaseMaximum</code></p> <p>Output-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/","title":"Ucb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb","title":"ucb","text":"<p>Upper confidence bound (UCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB","title":"UCB","text":"<pre><code>UCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMaximum</code></p> <p>The upper confidence bound (UCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] + \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A factor associated with the standard deviation to increase the mean value.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_confidence_bound.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        kappa: A factor associated with the standard deviation\n            to increase the mean value.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self.__kappa = kappa\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum","title":"minimum","text":"<p>Acquisition criteria for minimum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/","title":"Base minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum","title":"base_minimum","text":"<p>Base class for acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum","title":"BaseMinimum","text":"<pre><code>BaseMinimum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a minimum.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMinimum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(y_{\\text{min}}-Y(x),0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{min}}\\) is the minimum output value in the learning set.</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = (y_{\\text{min}}-\\mathbb{E}[Y(x)])\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{y_{\\text{min}}-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(y_{\\text{min}}-Y(x_i),0)\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory","title":"MinimumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/","title":"Lcb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb","title":"lcb","text":"<p>Lower confidence bound (LCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB","title":"LCB","text":"<pre><code>LCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMinimum</code></p> <p>The lower confidence bound (LCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] - \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A factor associated with the standard deviation to increase the mean value.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:  # noqa: D102\n    super().__init__(\n        regressor_distribution,\n        kappa=-abs(kappa),\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum","title":"minimum","text":"<p>Family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum.Minimum","title":"Minimum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/","title":"Output","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output","title":"output","text":"<p>The output-based criterion to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output","title":"Output","text":"<pre><code>Output(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Output</code>, <code>BaseMinimum</code></p> <p>Output-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile","title":"quantile","text":"<p>Acquisition criteria for quantile estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/","title":"Base quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile","title":"base_quantile","text":"<p>Base class for acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile","title":"BaseQuantile","text":"<pre><code>BaseQuantile(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a quantile.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> \\[ EF[x] = \\mathbb{E}\\left[\\max(\\kappa\\mathbb{S}[Y(x)] - |y_{\\alpha} - Y(x)|,0)\\right] \\] <p>where \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EF[x] = \\mathbb{S}[Y(x)]\\times \\left( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(\\kappa\\mathbb{S}[Y(x_i)] - |y_{\\alpha} - Y(x_i)|,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[ EI[x] = \\mathbb{E} \\left[\\max\\left((\\kappa\\mathbb{S}[Y(x)])^2 - (y_{\\alpha} - Y(x))^2,0\\right)\\right] \\] <p>where \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = \\mathbb{V}[Y(x)] \\left( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max((\\kappa\\mathbb{S}[Y(x_i)])^2 - (y_{\\alpha} - Y(x_i))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate quantiles.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory","title":"QuantileFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate level sets.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseAcquisitionCriterion]</code>           \u2013            <p>The class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the class is not available.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile","title":"quantile","text":"<p>Family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile.Quantile","title":"Quantile","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\frac{|y_{\\alpha}-\\mathbb{E}[Y(x)]|}{\\mathbb{S}[Y(x)])}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output. For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i\\leq q}\\left( \\left(\\frac{y_{\\alpha}-Y(x_i)}{\\mathbb{S}[Y(x_i)]}\\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/","title":"Distributions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions","title":"distributions","text":"<p>Regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions.get_regressor_distribution","title":"get_regressor_distribution","text":"<pre><code>get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution\n</code></pre> <p>Return the distribution of a regressor.</p> <p>Parameters:</p> <ul> <li> <code>regressor</code>               (<code>BaseRegressor</code>)           \u2013            <p>The regression algorithm.</p> </li> <li> <code>use_bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bootstrap for resampling. If <code>False</code>, use cross-validation.</p> </li> <li> <code>use_loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use leave-one-out resampling when <code>use_bootstrap</code> is <code>False</code>. If <code>False</code>, use parameterized cross-validation.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseRegressorDistribution</code>           \u2013            <p>The distribution of the regression algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/__init__.py</code> <pre><code>def get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution:\n    \"\"\"Return the distribution of a regressor.\n\n    Args:\n        regressor: The regression algorithm.\n        use_bootstrap: Whether to use bootstrap for resampling.\n            If `False`, use cross-validation.\n        use_loo: Whether to use leave-one-out resampling when\n            `use_bootstrap` is `False`.\n            If `False`, use parameterized cross-validation.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n\n    Returns:\n        The distribution of the regression algorithm.\n    \"\"\"\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        return KrigingDistribution(regressor)\n\n    return RegressorDistribution(regressor, use_bootstrap, use_loo, size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/","title":"Base regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution","title":"base_regressor_distribution","text":"<p>Base class for regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution","title":"BaseRegressorDistribution","text":"<pre><code>BaseRegressorDistribution(regressor: BaseRegressor)\n</code></pre> <p>The distribution of a regressor.</p> <p>Parameters:</p> <ul> <li> <code>regressor</code>               (<code>BaseRegressor</code>)           \u2013            <p>A regression model.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def __init__(self, regressor: BaseRegressor) -&gt; None:\n    \"\"\"\n    Args:\n        regressor: A regression model.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo = regressor\n    self._samples = []\n    self._transform_input_group = self.algo._transform_input_group\n    self._transform_output_group = self.algo._transform_output_group\n    self._input_variables_to_transform = self.algo._input_variables_to_transform\n    self._output_variables_to_transform = self.algo._output_variables_to_transform\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval  <code>abstractmethod</code>","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_confidence_interval(\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    \"\"\"Compute the lower bounds and upper bounds of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n        level: A quantile level.\n\n    Returns:\n        The lower and upper bound values of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_mean","title":"compute_mean  <code>abstractmethod</code>","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_mean(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output mean of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output mean of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_samples","title":"compute_samples  <code>abstractmethod</code>","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_samples(\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    \"\"\"Generate samples from the random process.\n\n    Args:\n        input_data: The $N$ input points of dimension $d$\n            at which to observe the random process;\n            shaped as `(N, d)`.\n        n_samples: The number of samples `M`.\n\n    Returns:\n        The output samples per output dimension shaped as `(N, M, p)`\n        where `p` is the output dimension.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_variance","title":"compute_variance  <code>abstractmethod</code>","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_variance(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output variance of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output variance of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/","title":"Kriging distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution","title":"kriging_distribution","text":"<p>Kriging-like regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution","title":"KrigingDistribution","text":"<pre><code>KrigingDistribution(algo: BaseRandomProcessRegressor)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Kriging-like regressor distribution.</p> <p>Parameters:</p> <ul> <li> <code>algo</code>               (<code>BaseRandomProcessRegressor</code>)           \u2013            <p>The description is missing.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def __init__(  # noqa: D107\n    self, algo: BaseRandomProcessRegressor\n) -&gt; None:\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRandomProcessRegressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray]] | tuple[NumberArray, NumberArray]</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    mean = self.compute_mean(input_data)\n    std = self.compute_standard_deviation(input_data)\n    quantile = norm.ppf(level)\n    if isinstance(mean, Mapping):\n        lower = {name: mean[name] - quantile * std[name] for name in mean}\n        upper = {name: mean[name] + quantile * std[name] for name in mean}\n    else:\n        lower = mean - quantile * std\n        upper = mean + quantile * std\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.algo.compute_samples(input_data, n_samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict_std(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.compute_standard_deviation(input_data) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/","title":"Regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution","title":"regressor_distribution","text":"<p>Universal regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution","title":"RegressorDistribution","text":"<pre><code>RegressorDistribution(\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Universal regressor distribution.</p> <p>Parameters:</p> <ul> <li> <code>algo</code>               (<code>BaseRegressor</code>)           \u2013            <p>The description is missing.</p> </li> <li> <code>bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>The resampling method. If <code>True</code>, use bootstrap resampling. Otherwise, use cross-validation resampling.</p> </li> <li> <code>loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The leave-One-Out sub-method, when bootstrap is <code>False</code>. If <code>False</code>, use parameterized cross-validation, Otherwise use leave-one-out.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def __init__(\n    self,\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        bootstrap: The resampling method.\n            If `True`, use bootstrap resampling.\n            Otherwise, use cross-validation resampling.\n        loo: The leave-One-Out sub-method, when bootstrap is `False`.\n            If `False`, use parameterized cross-validation,\n            Otherwise use leave-one-out.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n    \"\"\"  # noqa: D205 D212 D415\n    if bootstrap:\n        self.method = self.BOOTSTRAP\n        self.size = size or self.N_BOOTSTRAP\n    else:\n        if loo:\n            self.method = self.LOO\n            self.size = len(algo.learning_set)\n        else:\n            self.method = self.CROSS_VALIDATION\n            self.size = size or self.N_FOLDS\n    self.algos = [\n        algo.__class__(algo.learning_set, settings_model=algo._settings)\n        for _ in range(self.size)\n    ]\n    self.weights = []\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP","title":"N_BOOTSTRAP  <code>class-attribute</code>","text":"<pre><code>N_BOOTSTRAP: int = 100\n</code></pre> <p>The default number of replicates for the bootstrap method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS","title":"N_FOLDS  <code>class-attribute</code>","text":"<pre><code>N_FOLDS: int = 5\n</code></pre> <p>The default number of folds for the cross-validation method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>The resampling method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre> <p>The size of the resampling set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: list[Callable[[NumberArray], float]] = []\n</code></pre> <p>The weight functions related to the sub-algorithms.</p> <p>A weight function computes a weight from an input data array.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:  # noqa: D102\n    for algo in self.algos:\n        algo.learning_set = learning_set\n    super().change_learning_set(learning_set)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray]] | tuple[NumberArray, NumberArray]</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    level = (1.0 - level) / 2.0\n    predictions = self.predict_members(input_data)\n    if isinstance(predictions, Mapping):\n        lower = {\n            name: quantile(value, level, axis=0)\n            for name, value in predictions.items()\n        }\n        upper = {\n            name: quantile(value, 1 - level, axis=0)\n            for name, value in predictions.items()\n        }\n    else:\n        lower = quantile(predictions, level, axis=0)\n        upper = quantile(predictions, 1 - level, axis=0)\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    return self.__average(weights, predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.predict_members(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    term1 = self.__average(weights, predictions**2)\n    term2 = self.__average(weights, predictions) ** 2\n    return term1 - term2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.evaluate_weights","title":"evaluate_weights","text":"<pre><code>evaluate_weights(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Evaluate weights.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The input data with shape (size, n_input_data)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The weights.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def evaluate_weights(self, input_data: NumberArray) -&gt; NumberArray:\n    \"\"\"Evaluate weights.\n\n    Args:\n        input_data: The input data with shape (size, n_input_data)\n\n    Returns:\n        The weights.\n    \"\"\"\n    weights = array([func(input_data) for func in self.weights])\n    return weights / npsum(weights, 0)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def learn(  # noqa: D102\n    self,\n    samples: list[int] | None = None,\n) -&gt; None:\n    self.weights = []\n    super().learn(samples)\n    if self.method in [self.CROSS_VALIDATION, self.LOO]:\n        n_folds = self.size\n        folds = array_split(self._samples, n_folds)\n    for index, algo in enumerate(self.algos):\n        if self.method == self.BOOTSTRAP:\n            new_samples = unique(\n                default_rng(1).choice(self._samples, len(self._samples))\n            )\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n        else:\n            fold = folds[index]\n            new_samples = npdelete(self._samples, fold)\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n\n        algo.learn(new_samples.tolist())\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict_members","title":"predict_members","text":"<pre><code>predict_members(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output value with the different machine learning algorithms.</p> <p>After prediction, the method stacks the results.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data, specified as either as a NumPy array or as dictionary of NumPy arrays indexed by inputs names. The NumPy array can be either a <code>(d,)</code> array representing a sample in dimension <code>d</code>, or a <code>(M, d)</code> array representing <code>M</code> samples in dimension <code>d</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output data (dimension <code>p</code>) of <code>N</code> machine learning algorithms.     If <code>input_data.shape == (d,)</code>, then <code>output_data.shape == (N, p)</code>.     If <code>input_data.shape == (M,d)</code>, then <code>output_data.shape == (N,M,p)</code>.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def predict_members(self, input_data: DataType) -&gt; DataType:\n    \"\"\"Predict the output value with the different machine learning algorithms.\n\n    After prediction, the method stacks the results.\n\n    Args:\n        input_data: The input data,\n            specified as either as a NumPy array or as dictionary of NumPy arrays\n            indexed by inputs names.\n            The NumPy array can be either a `(d,)` array\n            representing a sample in dimension `d`,\n            or a `(M, d)` array representing `M` samples in dimension `d`.\n\n    Returns:\n        The output data (dimension `p`) of `N` machine learning algorithms.\n            If `input_data.shape == (d,)`, then `output_data.shape == (N, p)`.\n            If `input_data.shape == (M,d)`, then `output_data.shape == (N,M,p)`.\n    \"\"\"\n    predictions = [algo.predict(input_data) for algo in self.algos]\n    if isinstance(input_data, Mapping):\n        return {\n            name: stack([prediction[name] for prediction in predictions])\n            for name in predictions[0]\n        }\n    return stack(predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/","title":"Visualization","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/#gemseo_mlearning.active_learning.visualization","title":"visualization","text":"<p>Visualization tools for active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/","title":"Acquisition view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view","title":"acquisition_view","text":"<p>An acquisition plot.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView","title":"AcquisitionView","text":"<pre><code>AcquisitionView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of points acquired during active learning.</p> <p>This visualization tool only works with a 2-dimensional input space.</p> <p>Parameters:</p> <ul> <li> <code>active_learning_algo</code>               (<code>ActiveLearningAlgo</code>)           \u2013            <p>The active learning algorithm using sequential acquisition.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>When the active learning algorithm uses parallel acquisition.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm\n            using sequential acquisition.\n\n    Raises:\n        NotImplementedError: When the active learning algorithm uses parallel\n            acquisition.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.batch_size &gt; 1:\n        msg = (\n            \"AcquisitionView does not support active learning algorithm using \"\n            \"parallel acquisition.\"\n        )\n        raise NotImplementedError(msg)\n\n    self.__algo = active_learning_algo\n    self.__input_dimension = (\n        active_learning_algo.regressor_distribution.algo.input_dimension\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView.draw","title":"draw","text":"<pre><code>draw(\n    new_point: RealArray | None = None,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>new_point</code>               (<code>RealArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The new point to be acquired.</p> </li> <li> <code>discipline</code>               (<code>Discipline | None</code>, default:                   <code>None</code> )           \u2013            <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> </li> <li> <code>n_test</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>The number of points per dimension.</p> </li> <li> <code>filled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The acquisition view.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def draw(\n    self,\n    new_point: RealArray | None = None,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        new_point: The new point to be acquired.\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n    \"\"\"\n    # Create grid.\n    input_space = self.__algo.input_space\n    lower_bounds = input_space.get_lower_bounds()\n    upper_bounds = input_space.get_upper_bounds()\n    test_x1 = linspace(lower_bounds[0], upper_bounds[0], n_test)\n    test_x2 = linspace(lower_bounds[1], upper_bounds[1], n_test)\n    grid = array(meshgrid(test_x1, test_x2)).T.reshape(-1, 2)\n\n    # Generate data.\n    distribution = self.__algo.regressor_distribution\n    final_dataset = distribution.algo.learning_set\n    #    The learning input samples.\n    points = final_dataset.input_dataset.to_numpy()\n    points_x = points[:, 0]\n    points_y = points[:, 1]\n    #    The predictions, the standard deviations and the criterion values.\n    predictions = distribution.predict(grid).reshape((n_test, n_test)).T\n    std = distribution.compute_standard_deviation(grid).reshape((n_test, n_test)).T\n    acquisition_criterion = self.__algo.acquisition_criterion.original.func\n    criterion_values = acquisition_criterion(grid).reshape((n_test, n_test)).T\n    x_name, y_name = final_dataset.input_names\n    output_name = final_dataset.output_names[0]\n    #    The observations if the discipline is available.\n    observations = None\n    if discipline is not None:\n        observations = zeros((n_test, n_test))\n        for i in range(n_test):\n            for j in range(n_test):\n                xij = array([test_x1[j], test_x2[i]])\n                input_data = {x_name: array([xij[0]]), y_name: array([xij[1]])}\n                observations[i, j] = discipline.execute(input_data)[output_name][0]\n\n    # Create figure and sub-figures.\n    fig, axes = plt.subplots(2, 2)\n    titles = [\n        [\"Discipline\", self.__algo.acquisition_criterion.__class__.__name__],\n        [\"Surrogate\", \"Standard deviation\"],\n    ]\n    data = [[observations, criterion_values], [predictions, std]]\n    cf = []\n    color = \"white\" if filled else \"black\"\n    n_initial_samples = self.__algo.n_initial_samples\n    contour_method = \"contourf\" if filled else \"contour\"\n    for i in range(2):\n        for j in range(2):\n            if (i, j) == (0, 0) and discipline is None:\n                continue\n\n            ax = axes[i, j]\n            cf.append(getattr(ax, contour_method)(test_x1, test_x2, data[i][j]))\n            for x, y in zip(\n                points_x[:n_initial_samples],\n                points_y[:n_initial_samples],\n            ):\n                ax.plot(x, y, \".\", ms=2, color=color)\n\n            for index, (x, y) in enumerate(\n                zip(\n                    points_x[n_initial_samples:],\n                    points_y[n_initial_samples:],\n                )\n            ):\n                ax.plot(x, y, \"+\", color=color)\n                ax.annotate(\n                    str(n_initial_samples + 1 + index),\n                    (0.05 + x, 0.05 + y),\n                    color=color,\n                )\n\n            if new_point is not None:\n                ax.plot(*new_point, \"o\", color=color)\n\n            ax.set_title(titles[i][j])\n\n    if discipline is None:\n        fig.delaxes(axes.flatten()[0])\n        fig.colorbar(cf[0], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[1, 0])\n        fig.colorbar(cf[2], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n    else:\n        fig.colorbar(cf[0], ax=axes[:, 0])\n        axes[0, 0].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        axes[0, 1].set_yticks([])\n        fig.colorbar(cf[3], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n\n    save_show_figure(fig, show, file_path)\n    return fig\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/","title":"Qoi history view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view","title":"qoi_history_view","text":"<p>History view.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView","title":"QOIHistoryView","text":"<pre><code>QOIHistoryView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of the history of the quantity of interest (QOI).</p> <p>Parameters:</p> <ul> <li> <code>active_learning_algo</code>               (<code>ActiveLearningAlgo</code>)           \u2013            <p>The active learning algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When there is no quantity of interest associated with the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm.\n\n    Raises:\n        ValueError: When there is no quantity of interest\n            associated with the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.acquisition_criterion.qoi is None:\n        msg = (\n            \"There is no quantity of interest \"\n            \"associated with the acquisition criterion \"\n            f\"{active_learning_algo.acquisition_criterion.__class__.__name__}.\"\n        )\n        raise ValueError(msg)\n\n    self.__algo = active_learning_algo\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView.draw","title":"draw","text":"<pre><code>draw(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Draw the QOI history.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> <li> <code>label</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The label for the QOI. If empty, use the name of the acquisition criterion family.</p> </li> <li> <code>add_markers</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add markers.</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options to create the Lines object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Lines</code>           \u2013            <p>A view of the QOI history.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def draw(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Draw the QOI history.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n            If empty, use the name of the acquisition criterion family.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        A view of the QOI history.\n    \"\"\"\n    if not label:\n        label = self.__algo.acquisition_criterion_family_name\n\n    x_label = \"Number of evaluations\"\n    n_evaluations_history, qoi_history = self.__algo.qoi_history\n    qoi_history = [qoi[0] for qoi in qoi_history]\n    dataset = IODataset()\n    dataset.add_variable(x_label, array(n_evaluations_history)[:, newaxis])\n    dataset.add_variable(label, array(qoi_history)[:, newaxis])\n    lines = Lines(\n        dataset,\n        variables=[label],\n        abscissa_variable=x_label,\n        add_markers=add_markers,\n        **options,\n    )\n    lines.marker = \".\"\n    lines.execute(show=show, save=file_path != \"\", file_path=file_path)\n    return lines\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/","title":"Algos","text":""},{"location":"reference/gemseo_mlearning/algos/#gemseo_mlearning.algos","title":"algos","text":"<p>Wrappers for algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/#gemseo_mlearning.algos.opt","title":"opt","text":"<p>Wrappers for optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/","title":"Sbo settings","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings","title":"sbo_settings","text":"<p>Settings for the surrogate-based optimization algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion","title":"AcquisitionCriterion","text":"<p>               Bases: <code>StrEnum</code></p> <p>An acquisition criterion.</p> <p>In the following, the training output values already used and the random output of the surrogate model at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\) and \\(Y(x)\\). The expectation and the standard deviation of \\(Y(x)\\) are respectively denoted \\(\\mathbb{E}[Y(x)]\\) and \\(\\mathbb{S}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.CB","title":"CB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CB = 'CB'\n</code></pre> <p>The confidence bound.</p> <p>The acquisition criterion is \\(\\mathbb{E}[Y(x)]-3\\mathbb{S}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.EI","title":"EI  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EI = 'EI'\n</code></pre> <p>The expected improvement.</p> <p>The acquisition criterion is \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.Output","title":"Output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Output = 'Output'\n</code></pre> <p>The mean output.</p> <p>The acquisition criterion is \\(\\mathbb{E}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.SBOSettings","title":"SBOSettings","text":"<p>               Bases: <code>BaseOptimizerSettings</code></p> <p>The settings for the surrogate-based optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>normalize_design_space</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to normalize the design space variables between 0 and 1.</p> </li> <li> <code>acquisition_algorithm</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the algorithm to optimize the data acquisition criterion.             If empty, use the default algorithm with its default settings.</p> </li> <li> <code>acquisition_settings</code>               (<code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the algorithm             to optimize the data acquisition criterion.             Ignored when <code>acquisition_algorithm</code> is empty.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel.</p> </li> <li> <code>criterion</code>               (<code>AcquisitionCriterion</code>, default:                   <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> )           \u2013            <p>The acquisition criterion.</p> </li> <li> <code>doe_algorithm</code>               (<code>str</code>, default:                   <code>'OT_OPT_LHS'</code> )           \u2013            <p>The name of the DOE algorithm for the initial sampling.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>doe_settings</code>               (<code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the DOE algorithm for the initial sampling.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>doe_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Either the initial DOE size or 0 if it is inferred from <code>doe_settings</code>.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criteria in parallel.</p> </li> <li> <code>regression_algorithm</code>               (<code>str | BaseRegressor</code>, default:                   <code>'OTGaussianProcessRegressor'</code> )           \u2013            <p>The regression algorithm.</p> <pre><code>        Either the name of the regression algorithm\n        approximating the objective function over the design space\n        or the regression algorithm itself.\n</code></pre> </li> <li> <code>regression_file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the file to save the regression model.</p> <pre><code>        If empty, do not save the regression model.\n</code></pre> </li> <li> <code>regression_settings</code>               (<code>Mapping[str, Optional[Any]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the regression algorithm.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> </ul>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/","title":"Surrogate based optimization","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization","title":"surrogate_based_optimization","text":"<p>A library for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedAlgorithmDescription","title":"SurrogateBasedAlgorithmDescription  <code>dataclass</code>","text":"<pre><code>SurrogateBasedAlgorithmDescription(\n    library_name: str = \"gemseo-mlearning\",\n)\n</code></pre> <p>               Bases: <code>OptimizationAlgorithmDescription</code></p> <p>The description of a surrogate-based optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>library_name</code>               (<code>str</code>, default:                   <code>'gemseo-mlearning'</code> )           \u2013            </li> </ul>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedOptimization","title":"SurrogateBasedOptimization","text":"<p>               Bases: <code>BaseOptimizationLibrary</code></p> <p>A wrapper for surrogate-based optimization.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/","title":"Core","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/#gemseo_mlearning.algos.opt.core","title":"core","text":"<p>Core functionalities for the optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/","title":"Surrogate based optimizer","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer","title":"surrogate_based_optimizer","text":"<p>A class for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer","title":"SurrogateBasedOptimizer","text":"<pre><code>SurrogateBasedOptimizer(\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_settings: Mapping[\n        str, DriverLibrarySettingType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: str | BaseRegressor = __name__,\n    regression_settings: Mapping[\n        str, MLAlgoParameterType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    **acquisition_settings: DriverLibrarySettingType\n)\n</code></pre> <p>An optimizer based on surrogate models.</p> <p>Parameters:</p> <ul> <li> <code>acquisition_algorithm</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm to optimize the data acquisition criterion. N.B. this algorithm must handle integers if some of the optimization variables are integers.</p> </li> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>doe_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Either the size of the initial DOE or 0 if the size is inferred from doe_settings. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_algorithm</code>               (<code>str</code>, default:                   <code>'OT_OPT_LHS'</code> )           \u2013            <p>The name of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_settings</code>               (<code>Mapping[str, DriverLibrarySettingType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The settings of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_algorithm</code>               (<code>str | BaseRegressor</code>, default:                   <code>__name__</code> )           \u2013            <p>Either the name of the regression algorithm approximating the objective function over the design space or the regression algorithm itself.</p> </li> <li> <code>regression_settings</code>               (<code>Mapping[str, MLAlgoParameterType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The settings of the regression algorithm. If transformer is missing, use :attr:<code>.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER</code>. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the file to save the regression model. If empty, do not save the regression model.</p> </li> <li> <code>**acquisition_settings</code>               (<code>DriverLibrarySettingType</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the algorithm to optimize the data acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def __init__(\n    self,\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_settings: Mapping[str, DriverLibrarySettingType] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: str | BaseRegressor = OTGaussianProcessRegressor.__name__,\n    regression_settings: Mapping[str, MLAlgoParameterType] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    **acquisition_settings: DriverLibrarySettingType,\n) -&gt; None:\n    \"\"\"\n    Args:\n        acquisition_algorithm: The name of the algorithm to optimize the data\n            acquisition criterion.\n            N.B. this algorithm must handle integers if some of the optimization\n            variables are integers.\n        problem: The optimization problem.\n        doe_size: Either the size of the initial DOE\n            or 0 if the size is inferred from doe_settings.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_algorithm: The name of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_settings: The settings of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_algorithm: Either the name of the regression algorithm\n            approximating the objective function over the design space\n            or the regression algorithm itself.\n        regression_settings: The settings of the regression algorithm.\n            If transformer is missing,\n            use :attr:`.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER`.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_file_path: The path to the file to save the regression model.\n            If empty, do not save the regression model.\n        **acquisition_settings: The settings of the algorithm to optimize\n            the data acquisition criterion.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__problem = problem\n    database = problem.database\n    self.__initial_input_samples = tuple(database.keys())\n    if isinstance(regression_algorithm, BaseRegressor):\n        self.__dataset = regression_algorithm.learning_set\n    else:\n        # Store max_iter as it will be overwritten by DOELibrary\n        max_iter = problem.evaluation_counter.maximum\n        settings = dict(doe_settings)\n        if doe_size &gt; 0 and \"n_samples\" not in settings:\n            settings[\"n_samples\"] = doe_size\n\n        # Store the listeners as they will be cleared by DOELibrary.\n        new_iter_listeners, store_listeners = database.clear_listeners()\n        with LoggingContext(logging.getLogger(\"gemseo\")):\n            DOELibraryFactory().execute(problem, doe_algorithm, **settings)\n\n        for listener in new_iter_listeners:\n            database.add_new_iter_listener(listener)\n\n        for listener in store_listeners:\n            database.add_store_listener(listener)\n\n        self.__dataset = problem.to_dataset(opt_naming=False)\n        if self.__initial_input_samples:\n            self.__dataset = self.__dataset[len(self.__initial_input_samples) :]\n\n        _regression_settings = {\"transformer\": {\"inputs\": \"MinMaxScaler\"}}\n        _regression_settings.update(dict(regression_settings))\n        regression_algorithm = RegressorFactory().create(\n            regression_algorithm,\n            self.__dataset,\n            **_regression_settings,\n        )\n        # Add the first iteration to the current_iter reset by DOELibrary.\n        problem.evaluation_counter.current += 1\n        # And restore max_iter.\n        problem.evaluation_counter.maximum = max_iter\n\n    self.__active_learning_algo = ActiveLearningAlgo(\n        Minimum.__name__, problem.design_space, regression_algorithm\n    )\n    if acquisition_algorithm:\n        self.__active_learning_algo.set_acquisition_algorithm(\n            acquisition_algorithm, **acquisition_settings\n        )\n    self.__regression_file_path = regression_file_path\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer.execute","title":"execute","text":"<pre><code>execute(number_of_acquisitions: int) -&gt; str\n</code></pre> <p>Execute the surrogate-based optimization.</p> <p>Parameters:</p> <ul> <li> <code>number_of_acquisitions</code>               (<code>int</code>)           \u2013            <p>The number of learning points to be acquired.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The termination message.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def execute(self, number_of_acquisitions: int) -&gt; str:\n    \"\"\"Execute the surrogate-based optimization.\n\n    Args:\n        number_of_acquisitions: The number of learning points to be acquired.\n\n    Returns:\n        The termination message.\n    \"\"\"\n    regressor_distribution = self.__active_learning_algo.regressor_distribution\n    regressor_distribution.learn()\n    message = self.__STOP_BECAUSE_MAX_ACQUISITIONS\n    for _ in range(number_of_acquisitions):\n        input_data = self.__active_learning_algo.find_next_point()[0]\n        hashed_input_data = HashableNdarray(input_data)\n        if hashed_input_data in self.__problem.database and (\n            hashed_input_data not in self.__initial_input_samples\n        ):\n            message = self.__STOP_BECAUSE_ALREADY_KNOWN\n            break\n\n        output_data = self.__problem.evaluate_functions(\n            design_vector=input_data, design_vector_is_normalized=False\n        )[0]\n        extra_learning_set = IODataset()\n        variable_names_to_n_components = regressor_distribution.algo.sizes\n        extra_learning_set.add_group(\n            group_name=IODataset.INPUT_GROUP,\n            data=input_data[newaxis],\n            variable_names=regressor_distribution.input_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        output_names = regressor_distribution.output_names\n        extra_learning_set.add_group(\n            group_name=IODataset.OUTPUT_GROUP,\n            data=hstack([output_data[output_name] for output_name in output_names])[\n                newaxis\n            ],\n            variable_names=output_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        self.__dataset = concat(\n            [regressor_distribution.algo.learning_set, extra_learning_set],\n            ignore_index=True,\n        )\n        self.__dataset = self.__dataset.map(lambda x: x.real)\n        regressor_distribution.change_learning_set(self.__dataset)\n        self.__active_learning_algo.update_problem()\n\n        if self.__regression_file_path:\n            with Path(self.__regression_file_path).open(\"wb\") as file:\n                pickle.dump(regressor_distribution.algo, file)\n\n    return message\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/","title":"Smt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/#gemseo_mlearning.algos.opt.smt","title":"smt","text":"<p>Surrogate-based optimization using SMT.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/","title":"Ego settings","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings","title":"ego_settings","text":"<p>Settings for the multi-start algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion","title":"AcquisitionCriterion","text":"<p>               Bases: <code>StrEnum</code></p> <p>An acquisition criterion.</p> <p>In the following, the training output values already used and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.EI","title":"EI  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EI = 'EI'\n</code></pre> <p>The expected improvement.</p> <p>The acquisition criterion is \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y,0]\\) where \\(Y\\) is a Gaussian random variable with mean \\(\\mu(x)\\) and standard deviation \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.LCB","title":"LCB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LCB = 'LCB'\n</code></pre> <p>The lower confidence bound.</p> <p>The acquisition criterion is \\(\\mu(x)-3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.SBO","title":"SBO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SBO = 'SBO'\n</code></pre> <p>The surrogate-based optimization.</p> <p>The acquisition criterion is \\(\\mu(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy","title":"ParallelStrategy","text":"<p>               Bases: <code>StrEnum</code></p> <p>The strategy to set the outputs of the virtual points for parallel acquisition.</p> <p>In the following, the output variable, the training output values already used, and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(y\\), \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.CLmin","title":"CLmin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLmin = 'CLmin'\n</code></pre> <p>The minimum constant liar.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\min \\{y_1,\\ldots,y_n\\}\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KB","title":"KB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KB = 'KB'\n</code></pre> <p>The Kriging believer.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBLB","title":"KBLB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBLB = 'KBLB'\n</code></pre> <p>The Kriging believer lower bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)-3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBRand","title":"KBRand  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBRand = 'KBRand'\n</code></pre> <p>The Kriging believer random bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)+\\kappa(x)\\sigma(x)\\) where \\(\\kappa(x)\\) is the realization of a random variable distributed according to the standard normal distribution.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBUB","title":"KBUB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBUB = 'KBUB'\n</code></pre> <p>The Kriging believer upper bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)+3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.SMTEGOSettings","title":"SMTEGOSettings","text":"<p>               Bases: <code>BaseOptimizerSettings</code></p> <p>The settings of the SMT's SBO algorithm.</p> <p>Parameters:</p> <ul> <li> <code>criterion</code>               (<code>AcquisitionCriterion</code>, default:                   <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> )           \u2013            <p>The acquisition criterion.</p> </li> <li> <code>enable_tunneling</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to enable the penalization of points that have been already evaluated in EI criterion.</p> </li> <li> <code>n_doe</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of points of the initial LHS DOE to train the surrogate.</p> </li> <li> <code>n_max_optim</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The maximum number of iterations for each sub-optimization.</p> </li> <li> <code>n_parallel</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>n_start</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The number of sub-optimizations.</p> </li> <li> <code>qEI</code>               (<code>ParallelStrategy</code>, default:                   <code>&lt;ParallelStrategy.KBUB: 'KBUB'&gt;</code> )           \u2013            <p>The strategy to acquire points in parallel.</p> </li> <li> <code>random_state</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The Numpy RandomState object or seed number which controls random draws.</p> </li> <li> <code>surrogate</code>               (<code>Surrogate | SurrogateModel</code>, default:                   <code>&lt;Surrogate.KRG: 'KRG'&gt;</code> )           \u2013            <p>The SMT Kriging-based surrogate model used internally; either an instance of the surrogate before training or its class name.</p> </li> </ul>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate","title":"Surrogate","text":"<p>               Bases: <code>StrEnum</code></p> <p>A surrogate model.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.GPX","title":"GPX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GPX = 'GPX'\n</code></pre> <p>Kriging based on the <code>egobox</code> library.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KPLS","title":"KPLS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KPLS = 'KPLS'\n</code></pre> <p>Kriging using partial least squares (PLS) to reduce the input dimension.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KPLSK","title":"KPLSK  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KPLSK = 'KPLSK'\n</code></pre> <p>A variant of KPLS.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KRG","title":"KRG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KRG = 'KRG'\n</code></pre> <p>Kriging.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.MGP","title":"MGP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MGP = 'MGP'\n</code></pre> <p>A marginal Gaussian process (MGP) regressor.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/","title":"Smt ego","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego","title":"smt_ego","text":"<p>The efficient global optimization (EGO) algorithm of SMT.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego.SMTEGO","title":"SMTEGO","text":"<pre><code>SMTEGO(algo_name: str = 'SMT_EGO')\n</code></pre> <p>               Bases: <code>BaseOptimizationLibrary</code></p> <p>Surrogate-based optimizers from SMT.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>, default:                   <code>'SMT_EGO'</code> )           \u2013            <p>The algorithm name.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>When the algorithm is not in the library.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/smt/smt_ego.py</code> <pre><code>def __init__(self, algo_name: str = \"SMT_EGO\") -&gt; None:  # noqa: D107\n    super().__init__(algo_name)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/","title":"Problems","text":""},{"location":"reference/gemseo_mlearning/problems/#gemseo_mlearning.problems","title":"problems","text":"<p>Use cases to benchmark and illustrate active learning algorithms.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/","title":"Branin","text":""},{"location":"reference/gemseo_mlearning/problems/branin/#gemseo_mlearning.problems.branin","title":"branin","text":"<p>The Branin use case to benchmark and illustrate active learning algorithms.</p> <p>The Branin function \\(\\(f(x_1,x_2) = \\left(15x_2 - \\frac{5.1}{4\\pi^2}(15x_1-5)^2 + \\frac{5}{\\pi}(15x_1-5)-6\\right)^2 + \\left(10- \\frac{10}{8\\pi}\\right)\\cos(15x_1-5) +10\\)\\) is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([0,1]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/","title":"Branin discipline","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline","title":"branin_discipline","text":"<p>The Branin function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline.BraninDiscipline","title":"BraninDiscipline","text":"<pre><code>BraninDiscipline()\n</code></pre> <p>               Bases: <code>Discipline</code></p> <p>The Branin function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_input_data.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/","title":"Branin function","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function","title":"branin_function","text":"<p>The Branin function.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function.BraninFunction","title":"BraninFunction","text":"<pre><code>BraninFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Branin function.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Branin\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/","title":"Branin problem","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem","title":"branin_problem","text":"<p>A problem connecting the Branin function with its input space.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem.BraninProblem","title":"BraninProblem","text":"<pre><code>BraninProblem(use_uncertain_space: bool = True)\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Branin function with its input space.</p> <p>Parameters:</p> <ul> <li> <code>use_uncertain_space</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to consider the input space as an uncertain space.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/branin/branin_problem.py</code> <pre><code>def __init__(self, use_uncertain_space: bool = True) -&gt; None:\n    \"\"\"\n    Args:\n        use_uncertain_space: Whether to consider the input space\n            as an uncertain space.\n    \"\"\"  # noqa: D205 D212\n    input_space = BraninSpace()\n    if not use_uncertain_space:\n        input_space = input_space.to_design_space()\n\n    super().__init__(input_space)\n    self.objective = BraninFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/","title":"Branin space","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space","title":"branin_space","text":"<p>The uncertain space used in the Branin use case.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space.BraninSpace","title":"BraninSpace","text":"<pre><code>BraninSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Branin use case.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(f\"x{index + 1}\", \"OTUniformDistribution\")\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions","title":"functions","text":"<p>The Branin function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Branin function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The value of the gradient of the Branin function.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The value of the gradient of the Branin function.\n    \"\"\"\n    x0 = 15 * x[0] - 5\n    tmp = 15 * x[1] - __A * x0**2 + __B * x0 - 6\n    return array([-15 * (__C * sin(x0) + 2 * tmp * (2 * __A * x0 - __B)), 30 * tmp])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Branin function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The output value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The output value.\n    \"\"\"\n    x0 = 15 * x[0] - 5\n    return (15 * x[1] - __A * x0**2 + __B * x0 - 6) ** 2 + __C * cos(x0) + 10\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/","title":"Rosenbrock","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/#gemseo_mlearning.problems.rosenbrock","title":"rosenbrock","text":"<p>The Rosenbrock use case to benchmark and illustrate active learning algorithms.</p> <p>The Rosenbrock function $$f(x_1,x_2) = 100(x_2-x_1^2)^2 + (1-x_1)^2 $$ is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([-2,2]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions","title":"functions","text":"<p>The Rosenbrock function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Rosenbrock function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The value of the gradient of the Rosenbrock function.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The value of the gradient of the Rosenbrock function.\n    \"\"\"\n    return array([\n        4 * __A * (x[0] ** 3 - x[0] * x[1]) + 2 * (x[0] - 1),\n        2 * __A * (x[1] - x[0] ** 2),\n    ])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Rosenbrock function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The output value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The output value.\n    \"\"\"\n    return (1 - x[0]) ** 2 + __A * (x[1] - x[0] ** 2) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/","title":"Rosenbrock discipline","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline","title":"rosenbrock_discipline","text":"<p>The Rosenbrock function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline.RosenbrockDiscipline","title":"RosenbrockDiscipline","text":"<pre><code>RosenbrockDiscipline()\n</code></pre> <p>               Bases: <code>Discipline</code></p> <p>The Rosenbrock function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_input_data.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/","title":"Rosenbrock function","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function","title":"rosenbrock_function","text":"<p>The Rosenbrock function.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function.RosenbrockFunction","title":"RosenbrockFunction","text":"<pre><code>RosenbrockFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Rosenbrock function.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Rosenbrock\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/","title":"Rosenbrock problem","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem","title":"rosenbrock_problem","text":"<p>A problem connecting the Rosenbrock function with its uncertain space.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem.RosenbrockProblem","title":"RosenbrockProblem","text":"<pre><code>RosenbrockProblem(use_uncertain_space: bool = True)\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Rosenbrock function with its uncertain space.</p> <p>Parameters:</p> <ul> <li> <code>use_uncertain_space</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to consider the input space as an uncertain space.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem.py</code> <pre><code>def __init__(self, use_uncertain_space: bool = True) -&gt; None:\n    \"\"\"\n    Args:\n        use_uncertain_space: Whether to consider the input space\n            as an uncertain space.\n    \"\"\"  # noqa: D205 D212\n    input_space = RosenbrockSpace()\n    if not use_uncertain_space:\n        input_space = input_space.to_design_space()\n\n    super().__init__(input_space)\n    self.objective = RosenbrockFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/","title":"Rosenbrock space","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space","title":"rosenbrock_space","text":"<p>The uncertain space used in the Rosenbrock use case.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space.RosenbrockSpace","title":"RosenbrockSpace","text":"<pre><code>RosenbrockSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Rosenbrock use case.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(\n            f\"x{index + 1}\", \"OTUniformDistribution\", minimum=-2, maximum=2\n        )\n</code></pre>"},{"location":"reference/gemseo_mlearning/regression/","title":"Regression","text":""},{"location":"reference/gemseo_mlearning/regression/#gemseo_mlearning.regression","title":"regression","text":"<p>The regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/","title":"Smt regressor","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor","title":"smt_regressor","text":"<p>A regression model from SMT.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTRegressor","title":"SMTRegressor","text":"<p>               Bases: <code>BaseRegressor</code></p> <p>A regression model from SMT.</p> <p>Note</p> <p>SMT is an open-source Python package consisting of libraries of surrogate modeling methods, sampling methods, and benchmarking problems. Read this page for the list of surrogate models and options.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/","title":"Smt regressor settings","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings","title":"smt_regressor_settings","text":"<p>The settings for the SMT's regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings.SMTRegressorSettings","title":"SMTRegressorSettings","text":"<p>               Bases: <code>BaseRegressorSettings</code></p> <p>The settings for the SMT's regression models.</p> <p>Parameters:</p> <ul> <li> <code>model_class_name</code>               (<code>SurrogateModel</code>)           \u2013            <p>The class name of a surrogate model available in SMT,i.e. a subclass of<code>smt.surrogate_models.surrogate_model.SurrogateModel</code>.</p> </li> </ul>"},{"location":"user_guide/","title":"Introduction","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":""},{"location":"user_guide/active_learning/active_learning_algo/","title":"AL algorithm","text":""},{"location":"user_guide/active_learning/active_learning_algo/#active-learning-algorithm","title":"Active learning algorithm","text":"<p>The ActiveLearningAlgo class defines an active learning algorithm.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#in-a-nutshell","title":"In a nutshell","text":"<p>Given a coarse regressor \\(\\hat{f}\\) (a.k.a. surrogate model) of a model \\(f\\) (a.k.a. substituted model), this algorithm updates this regressor sequentially from input-output points maximizing (or minimizing) an acquisition criterion (a.k.a. infill criterion) chosen for a specific purpose.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-an-acquisition-criterion","title":"Choosing an acquisition criterion","text":"<p><code>gemseo-mlearning</code> includes five families of acquisition criteria corresponding to as many purposes:</p> <ul> <li>the Minimum family   aims to make the minimum of the surrogate model   tends towards the minimum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Maximum family   aims to make the maximum of the surrogate model   tends towards the maximum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the LevelSet family   aims to make a level set of the surrogate model   tends towards the corresponding level set of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Quantile family   aims to make a quantile of the surrogate model   tends towards the corresponding quantile of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Exploration family   aims to make the error of the surrogate model tends towards zero   when the number of acquired points tends to infinity.</li> </ul> <p>Minimum, Maximum and Quantile provides an estimation of the quantity of interest, namely the minimum, the maximum and a quantile respectively. This value is updated each time a training point is acquired and can be accessed via the attribute qoi.</p> <p>Whatever the family, the surrogate model can be used for prediction as any surrogate model. However, outside the Exploration family, it is important to bear in mind that its learning dataset has been designed to estimate a particular quantity of interest, and should therefore be used with caution. In other words, a surrogate model built to find the minimum can be bad at predicting the high output values. In such cases, the predicted standard-deviation that characterizes the uncertainty of the prediction can help to judge the accuracy of the surrogate prediction.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-a-surrogate-model","title":"Choosing a surrogate model","text":"<p>Many of these acquisition criteria consider that the surrogate model is a Gaussian process (GP) regressor \\(\\hat{f}\\) and are expressed from realizations or statistics of this GP \\(\\hat{F}\\), e.g. its mean \\(m_n(x)\\) and its variance \\(c_n(x,x)\\) at \\(x\\) where \\(c_n(\\cdot,\\cdot)\\) is its covariance function. These statistics and realizations can be obtained using the KrigingDistribution class which can be built from any regressor deriving from BaseRandomProcessRegressor, such as GaussianProcessRegressor based on scikit-learn and OTGaussianProcessRegressor based on OpenTURNS. By distribution we mean the probability distribution of a random function of which \\(f\\) is an instance. For non-GP regressors, this distribution is not a random process from the literature but an empirical distribution based on resampling techniques and is qualified as universal by its authors: RegressorDistribution.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#how-to-use-this-algorithm","title":"How to use this algorithm","text":"<p>A basic use of this class is</p> <ol> <li>instantiate    the ActiveLearningAlgo    from an input space of type DesignSpace,    a BaseRegressor    and the name of a family of acquisition criteria    (a default acquisition criterion will be set accordingly),</li> <li>update the regressor with the method <code>acquire_new_points</code>,</li> <li>get the updated regressor with the attribute <code>regressor</code>.</li> </ol> <p>For more advanced use, it is possible to change the acquisition algorithm, i.e. the optimization algorithm to minimize or maximize the acquisition criterion, as well as the acquisition criterion among the selected family.</p> <p>Lastly, the visualization subpackage offers plotting capabilities to draw the evolution of both the surrogate model and the acquisition criterion. They can be easily accessed from the ActiveLearningAlgo using its methods:</p> <ul> <li>plot_qoi_history   to visualize the estimation of the quantity of interest in function of the acquisition step,</li> <li>plot_acquisition_view   to visualize the discipline, the regressor, the acquisition criterion and the standard deviation   in the case of two scalar input variables.</li> </ul>"},{"location":"user_guide/active_learning/exploration/","title":"AL for exploration","text":""},{"location":"user_guide/active_learning/exploration/#active-learning-for-exploration","title":"Active learning for exploration","text":"<p>Active learning techniques are very intuitive for refining a surrogate model. Instead of spending the entire computational budget to create a training dataset in one-go, the idea is to start from a rough surrogate model and acquire new training points sequentially to increase its accuracy.</p> <p>The ActiveLearningAlgo presented in the previous section can be used to explore the input space in search of unlearned points, using the family of acquisition criteria Exploration.</p> <p>For now, these acquisition criteria do not have quantities of interest and do not support parallel acquisition.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Whatever surrogate model is chosen, given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p>"},{"location":"user_guide/active_learning/exploration/#distance","title":"Distance","text":"<p>The most naive approach is to learn the point furthest away from those in the training dataset. So, this criterion to maximize is the minimum distance between a new point and the point of the training dataset, scaled by the minimum distance between two distinct training points:</p> \\[D[x]=\\frac{\\min_{1\\leq i \\leq n} \\|x-x_i\\|_2}{\\min_{1\\leq i,j \\leq n, i\\neq j} \\|x_i-x_j\\|_2}\\] <p>where \\(\\|.\\|_2\\) is the Euclidean norm.</p>"},{"location":"user_guide/active_learning/exploration/#api","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Distance\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"Distance\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/exploration/#variance","title":"Variance","text":"<p>The previous criterion considers neither the output values nor the quality of the surrogate model. However, taking account of model uncertainty can be relevant and this is the reason why the variance of the prediction is the default criterion:</p> \\[V[x]=\\sigma^2(x).\\]"},{"location":"user_guide/active_learning/exploration/#api_1","title":"API","text":"<p>This criterion, named <code>\"Variance\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n)\n</code></pre>"},{"location":"user_guide/active_learning/exploration/#standard-deviation","title":"Standard deviation","text":"<p>This criterion is simply the square root of the previous one:</p> \\[S[x]=\\sigma(x).\\]"},{"location":"user_guide/active_learning/exploration/#api_2","title":"API","text":"<p>This criterion can be accessed via the name <code>\"StandardDeviation\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"StandardDeviation\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/","title":"AL for level sets","text":""},{"location":"user_guide/active_learning/level_set/#active-learning-for-level-set-estimation","title":"Active learning for level set estimation","text":"<p>Active learning techniques are very useful to estimate a level set, i.e. the input values for which the model output is equal to a specific value \\(y\\).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to approximate a particular level set, using the family of acquisition criteria LevelSet. As new points are acquired, the level set of the surrogate model will be increasingly similar to that of the substituted model.</p> <p>For now, these acquisition criteria do not have quantities of interest.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p> <p>The value characterizing the level set can be passed using the argument <code>output_value</code>,  e.g. <code>output_value=12</code>.</p> <p>All these acquisition criteria support parallel acquisition. The number of points to be acquired at a time and the sample size to estimate the statistics defined below can be set with the arguments <code>batch_size</code> (default: <code>1</code>) and <code>mc_size</code> (default: <code>10000</code>) respectively.</p>"},{"location":"user_guide/active_learning/level_set/#u-function","title":"U-function","text":"<p>The simplest criterion to approximate a level set \\(y\\) is the U-function<sup>1</sup>:</p> \\[U[x] = \\mathbb{E}\\left[\\left(\\frac{y-Y(x)}{\\sigma(x)}\\right)^2\\right]\\] <p>which can be simplified to</p> \\[U[x] = \\left(\\frac{y-\\mu(x)}{\\sigma(x)}\\right)^2.\\]"},{"location":"user_guide/active_learning/level_set/#api","title":"API","text":"<p>This criterion, named <code>\"U\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i \\leq q}\\left(\\left(\\frac{y-Y(x_i)}{\\sigma(x_i)}\\right)^2\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/level_set/#expected-feasibility","title":"Expected feasibility","text":"<p>Another criteria to approximate a level set \\(y\\) is the expected feasibility<sup>2</sup>:</p> \\[EF[x] =  \\mathbb{E}\\left[\\max(\\kappa\\sigma(x) - |y - Y(x)|,0)\\right].\\]"},{"location":"user_guide/active_learning/level_set/#api_1","title":"API","text":"<p>This criterion can be accessed via the name <code>\"EF\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EF\",\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#gaussian-case","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[ EF[x] = \\sigma(x) \\left( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) \\right) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mu(x)}{\\sigma(x)}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition_1","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left(\\max(\\kappa\\sigma(x_i) - |y - Y(x_i)|,0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/level_set/#expected-improvement","title":"Expected improvement","text":"<p>Another criteria to approximate a level set \\(y\\) is the expected improvement<sup>2</sup>:</p> \\[EI[x] = \\mathbb{E}\\left[\\max((\\kappa\\sigma(x))^2 - (y - Y(x))^2,0)\\right].\\]"},{"location":"user_guide/active_learning/level_set/#api_2","title":"API","text":"<p>This criterion can be accessed via the name <code>\"EI\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EI\",\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#gaussian-case_1","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[ EI[x] = \\sigma(x) \\left( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y - \\mu(x)}{\\sigma(x)}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition_2","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max((\\kappa\\sigma(x_i))^2 - (y - Y(x_i))^2,0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p> <ol> <li> <p>S. Roy and W. Notz. Estimating percentiles in computer experiments: a comparison of sequential-adaptive designs and fixed designs. Journal of Statistical Theory and Practice, 8:12\u201329, 2014.\u00a0\u21a9</p> </li> <li> <p>J. Bect, D. Ginsbourger, L. Li, V. Picheny, and E. Vazquez. Sequential design of computer experiments for the estimation of a probability of failure. Statistics and Computing, 22:773\u2013793, 2012.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/active_learning/optimization/","title":"AL for optimization","text":""},{"location":"user_guide/active_learning/optimization/#active-learning-for-optimization","title":"Active learning for optimization","text":"<p>Active learning techniques have become popular in global optimization, with the famous Bayesian optimization algorithms, including the efficient global optimization (EGO) one<sup>1</sup>. These approaches can be relevant when the functions of the optimization problem:</p> <ul> <li>are costly,</li> <li>are not accompanied by their gradient functions,</li> <li>depend on an important number of optimization variables,   making the estimation of their gradient   by finite differences impossible.</li> </ul> <p>In this case, multi-start gradient-based optimization cannot be used, and neither can classical global optimization algorithms (e.g. evolutionary algorithms).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to minimize (resp. maximize) a cost function (resp. performance function) using the family of acquisition criteria Minimum (resp. Maximum ). The quantity of interest is the minimum (resp. maximum) of the function.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria, first for the minimum, then for the maximum. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p> <p>For acquisition criteria supporting parallel acquisition, the number of points to be acquired at a time and the sample size to estimate the statistics defined below can be set with the arguments <code>batch_size</code> (default: <code>1</code>) and <code>mc_size</code> (default: <code>10000</code>) respectively.</p>"},{"location":"user_guide/active_learning/optimization/#minimum","title":"Minimum","text":""},{"location":"user_guide/active_learning/optimization/#output","title":"Output","text":"<p>The simplest criterion to minimize is the expectation:</p> \\[E[x] = \\mu(x),\\] <p>also called Kriging believer in the case of a Gaussian process regressor.</p>"},{"location":"user_guide/active_learning/optimization/#api","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Output\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"Output\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#lower-confidence-bound","title":"Lower confidence bound","text":"<p>A more advanced criterion to minimize is the lower confidence bound</p> \\[M[x;\\kappa] = \\mu(x) - \\kappa \\times \\sigma(x)\\] <p>where \\(\\kappa &gt; 0\\) (default: 2).</p>"},{"location":"user_guide/active_learning/optimization/#api_1","title":"API","text":"<p>This criterion can be accessed via the name <code>\"LCB\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"LCB\"\n)\n</code></pre> <p>and the \\(\\kappa\\) constant can be set using the argument <code>kappa</code>.</p>"},{"location":"user_guide/active_learning/optimization/#expected-improvement","title":"Expected improvement","text":"<p>The most popular criterion to maximize is the expected improvement<sup>1</sup></p> \\[EI[x] = \\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0)].\\] <p>where \\(y_1,\\dots,y_n\\) are the learning output values.</p>"},{"location":"user_guide/active_learning/optimization/#api_2","title":"API","text":"<p>This criterion, named <code>\"EI\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#gaussian-case","title":"Gaussian case","text":"<p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[EI[x] = (\\min(y_1,\\dots,y_n)-\\mu(x))\\Phi(t) + \\sigma(x)\\phi(t)\\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{y_{\\text{min}}-\\mu(x)}{\\sigma(x)}\\).</p>"},{"location":"user_guide/active_learning/optimization/#parallel-acquisition","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max(\\min(y_1,\\dots,y_n)-Y(x_i),0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/optimization/#maximum","title":"Maximum","text":""},{"location":"user_guide/active_learning/optimization/#output_1","title":"Output","text":"<p>The simplest criterion to maximize is the expectation</p> \\[E[x] = \\mu(x),\\] <p>also called Kriging believer in the case of a Gaussian process regressor.</p>"},{"location":"user_guide/active_learning/optimization/#api_3","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Output\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n  \"Maximum\",\n  input_space,\n  initial_regressor,\n  criterion_name=\"Output\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#upper-confidence-bound","title":"Upper confidence bound","text":"<p>A more advanced criterion to maximize is the upper confidence bound</p> \\[M[x;\\kappa] = \\mu(x) + \\kappa \\times \\sigma(x)\\] <p>where \\(\\kappa &gt; 0\\) (default: 2).</p>"},{"location":"user_guide/active_learning/optimization/#api_4","title":"API","text":"<p>This criterion can be accessed via the name <code>\"UCB\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"LCB\"\n)\n</code></pre> <p>and the \\(\\kappa\\) constant can be set using the argument <code>kappa</code>.</p>"},{"location":"user_guide/active_learning/optimization/#expected-improvement_1","title":"Expected improvement","text":"<p>The most popular criterion to maximize is the expected improvement<sup>1</sup></p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-\\max(y_1,\\dots,y_n)},0)]\\] <p>where \\(y_1,\\dots,y_n\\) are the learning output values.</p>"},{"location":"user_guide/active_learning/optimization/#api_5","title":"API","text":"<p>This criterion, named <code>\"EI\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Maximum\",\n    input_space,\n    initial_regressor\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#gaussian-case_1","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[EI[x] = (\\mu(x) - \\max(y_1,\\dots,y_n)})\\Phi(t) + \\sigma(x)\\phi(t)\\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{\\mu(x) - \\max(y_1,\\dots,y_n)}}{\\sigma(x)}\\).</p>"},{"location":"user_guide/active_learning/optimization/#parallel-acquisition_1","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max(Y(x_i)-\\max(y_1,\\dots,y_n)},0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p> <ol> <li> <p>D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/active_learning/quantile/","title":"AL for quantiles","text":""},{"location":"user_guide/active_learning/quantile/#quantile","title":"Quantile","text":"<p>Active learning techniques are very useful to estimate a level set,  i.e. the input values for which the model output is equal to a specific value \\(y\\).</p> <p>In particular, this specific value can be the \\(\\alpha\\)-quantile for a specific \\(\\alpha\\in]0,1[\\).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to approximate a particular quantile, using the family of acquisition criteria Quantile. These criteria are the same as for the estimation of a level set. For this reason, we refer you to this page listing these acquisition criteria.</p> <p>In this page, we simply indicate that the quantile level and the uncertain space are set using the arguments <code>level</code> and <code>uncertain_space</code> respectively, and the user code looks like below.</p>"},{"location":"user_guide/active_learning/quantile/#u-function","title":"U-function","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/quantile/#expected-feasibility","title":"Expected feasibility","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EF\",\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/quantile/#expected-improvement","title":"Expected improvement","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EI\",\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/what_active_learning_is/","title":"What AL is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#active-learning","title":"Active learning","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#what-active-learning-is","title":"What active learning is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#introduction","title":"Introduction","text":"<p>Active learning techniques are iterative methods that aim to sequentially estimate various quantities of interest: optimas, contours, failure probabilities, quantiles, expected values, etc. These methods generally start by constructing a rough surrogate model (an approximation of a costly exact physical model much faster to run). Then, they iteratively look for new simulations to run, and update the surrogate model and the estimated target quantities until a budget or accuracy criterion is reached. This search is guided by an acquisition criterion, also called infill criterion, to be minimized or maximized. This approach is inspired by the efficient global optimization (EGO) algorithm <sup>1</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#surrogate-modeling","title":"Surrogate modeling","text":"<p>Surrogate models<sup>2</sup> are approximations of a costly exact model \\(f:\\mathbf{x} \\in \\mathbb{X}\\subset\\mathbb{R}^d\\mapsto y \\in \\mathbb{R}\\) much faster to run. There are many types of surrogates, but one of the most popular, in particular in the active learning literature, is the Kriging model, a.k.a. Gaussian processes (GP) regressor <sup>3</sup>. One reason is that GP models provide a characterization of the prediction uncertainty. The exact simulator \\(f\\) is considered as a realization of a GP, \\(F\\), with trend \\(m : \\mathbb{X} \\rightarrow \\mathbb{R}\\) and covariance kernel \\(c : \\mathbb{X}\\times \\mathbb{X} \\rightarrow \\mathbb{R}\\). Conditioned by \\(n\\) observations \\(\\mathcal{A}_n = \\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n)\\}\\), \\(F_n  = F|\\mathcal{A}_n\\) is also a GP, with trend \\(m_n(\\cdot)\\) and covariance function \\(c_n(\\cdot,\\cdot)\\) that express differently depending on whether the trend function is known or not. The variance of the output prediction is denoted by \\(s_n^2(\\cdot) = c_n(\\cdot,\\cdot)\\).</p> <p>Note that the implementation of active learning strategies is not limited to the use of GP surrogates. In this case, a critical step of the method is the characterization of the uncertainty of the surrogate prediction, which can be achieved using bootstrap-based techniques<sup>4</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#the-rosenbrock-function","title":"The Rosenbrock function","text":"<p>The Rosenbrock function<sup>5</sup>, which will be used for illustrative purposes, expresses as :</p> \\[ f(\\mathbf{x}) = \\sum_{i = 1}^{d-1} [100(x_{i+1} - x_i^2)^2 +(1-x_i)^2], \\] <p>with \\(\\mathbf{x}\\) a \\(d\\)-dimensional input, \\(d \\geqslant 2\\). An illustration of the function with a two-dimensional input is depicted below:</p>  Bidimensional Rosenbrock function  <p>In the case of quantile estimation, \\(x_1\\) and \\(x_2\\) are replaced by the independent random variables \\(X_1\\) and \\(X_2\\) uniformly distributed over \\([-2,2]\\).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#acquisition-criteria","title":"Acquisition criteria","text":"<p>One of the key step of the active learning methodology is the choice of the acquisition criterion that guides the enrichment of the surrogate model. It is noted \\(J_n(\\cdot)\\) in the following. Several criteria have been developed for different purposes <sup>6</sup>. Typical applications of this methodology include the improvement of the surrogate model, the minimization of black-box functions and contour estimation, among others. Some popular criteria are detailed next for the objectives listed above.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#improving-the-quality-of-the-surrogate","title":"Improving the quality of the surrogate","text":"<p>A straightforward method to improve a surrogate model consists in choosing as enrichment points those with the highest predicted uncertainty, giving for the infill criterion \\(J_n(\\cdot)\\) :</p> \\[ J_n(\\mathbf{x}) = s_n^2(\\mathbf{x}),\\]  Level plot of the predicted uncertainty. The learning points are denoted with a red star.  <p>Note in this figure that, the predicted uncertainty (in fact the infill criterion) is the highest in the areas where the amount of learning points for the training of the surrogate is the smallest.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#finding-optimas","title":"Finding optimas","text":"<p>One of the most popular acquisition criterion to minimize (alternatively to maximize) black-box functions is the expected improvement <sup>1</sup> defined as:</p> \\[ J_n(\\mathbf{x}) =  \\text{EI}_n(\\mathbf{x})  = \\mathbb{E}[\\max(\\mathbf{y}_n - F_n(\\mathbf{x}),0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). In the previous equation, \\(\\mathbf{y}_n\\) corresponds to the output values of the database \\(\\mathcal{A}_n\\) and stands for \\(\\mathbf{y}_n = (y_i)_{i \\in [1, n]}\\).</p>  Level plot of the expected improvement. The learning points are denoted with a red star.  The global minimum is set in green.  <p>In this figure, some learning points are already close to the global minimum, so the infill criterion rather seeks here to improve the GP overall prediction.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#estimating-contour-sets","title":"Estimating contour sets","text":"<p>A usual criterion to estimate the level set<sup>7</sup> associated to value \\(a\\) is :</p> \\[ J_n(\\mathbf{x}) =  \\mathbb{E}[\\max((2 s_n^2(\\mathbf{x})) -\\vert{a - F_n(\\mathbf{x})}\\vert^2,0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). With an appropriate choice of \\(a\\), this criterion can be used to estimate quantiles or failure probabilities. In the following, it is implemented to estimate the \\(35\\%\\) quantile of the Rosenbrock function.</p>  Level plot of the acquisition criterion for level set estimation. The learning points are denoted with a red star. The level set associated to the quantile to estimate is drawn in black.  <p>In this figure, the areas with the highest acquisition criterion values refer to those that are located around the level set corresponding to the target quantile (which is here the objective study) and far from the learning points (where knowledge is already available).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#parallelization","title":"Parallelization","text":"<p>Most of the time, a single observation is added at the different steps of the active learning algorithm. However, to reduce the number of iterations of the active learning strategy, a batch of several points can be added <sup>8</sup> instead of considering a single one. This implies several changes, among which the expression of the infill criterion and its maximization. Let emphasize that parallelizing the active learning methodology significantly increases the computational burden which can still be alleviated by considering approximated heuristics <sup>8</sup>.</p> <p>A key challenge of this method is defining the size of the batch of points added at each iteration. In theory, it should be as high as possible, because for a fixed number of black-box evaluations, the higher it is, the lower the number of iterations is. However, this is synonymous with an increase of the numerical complexity of the method, and a balance must be therefore struck between the computational costs and the efficiency of the active learning strategy.</p> <p>In the next section, we will present the active learning algorithm available in gemseo-mlearning.</p> <ol> <li> <p>D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Forrester, A. Sobester, and A. Keane. Engineering design via surrogate modelling: a practical guide. John Wiley &amp; Sons, 2008.\u00a0\u21a9</p> </li> <li> <p>C. Williams and C. Rasmussen. Gaussian processes for machine learning. Volume 2. MIT press Cambridge, MA, 2006.\u00a0\u21a9</p> </li> <li> <p>M. Ben Salem, O. Roustant, F. Gamboa, and L. Tomaso. Universal prediction distribution for surrogate models. SIAM/ASA Journal on Uncertainty Quantification, 5:, 12 2015. doi:10.1137/15M1053529.\u00a0\u21a9</p> </li> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> <li> <p>F. Viana, C. Gogu, and T. Goel. Surrogate modeling: tricks that endured the test of time and some recent developments. Structural and Multidisciplinary Optimization, pages 1\u201328, 2021.\u00a0\u21a9</p> </li> <li> <p>P. Ranjan, D. Bingham, and G. Michailidis. Sequential experiment design for contour estimation from complex computer codes. Technometrics, 50(4):527\u2013541, 2008.\u00a0\u21a9</p> </li> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/optimization/al/","title":"AL package","text":""},{"location":"user_guide/optimization/al/#surrogate-based-optimizers-using-the-al-algorithm","title":"Surrogate-based optimizers using the AL algorithm","text":"<p>The ActiveLearningAlgo presented in this page can be used to minimize a cost function using the family of acquisition criteria Minimum</p> <p>SurrogateBasedOptimization is a collection of surrogate-based optimization algorithms built on top of ActiveLearningAlgo and Minimum.</p>"},{"location":"user_guide/optimization/al/#basic-usage","title":"Basic usage","text":"<p>SurrogateBasedOptimization includes a single optimization algorithm, called <code>\"SBO\"</code>.</p> <p>Given a maximum number of iterations, it can be used as is by any OptimizationProblem: <pre><code>execute_algo(optimization_problem, \"SBO\", max_iter=50)\n</code></pre> and any MDOScenario: <pre><code>scenario.execute({\"algo_name\": \"SBO\", \"max_iter\": 50})\n</code></pre></p> <p>In this case, the settings are</p> <ul> <li>the expected improvement as acquisition criterion,</li> <li>1 point acquired at a time,</li> <li>the OTGaussianProcessRegressor   wrapping the Kriging model from OpenTURNS,</li> <li>10 initial training points based on an optimized latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion   with a gradient-based algorithm   from 20 start points with a limit of 200 iterations per local optimization.</li> </ul>"},{"location":"user_guide/optimization/al/#settings","title":"Settings","text":"<p>In the following, the training output values already acquired and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"user_guide/optimization/al/#general","title":"General","text":"<p>The settings for the surrogate-based optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>normalize_design_space</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to normalize the design space variables between 0 and 1.</p> </li> <li> <code>acquisition_algorithm</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the algorithm to optimize the data acquisition criterion.             If empty, use the default algorithm with its default settings.</p> </li> <li> <code>acquisition_settings</code>               (<code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the algorithm             to optimize the data acquisition criterion.             Ignored when <code>acquisition_algorithm</code> is empty.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel.</p> </li> <li> <code>criterion</code>               (<code>AcquisitionCriterion</code>, default:                   <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> )           \u2013            <p>The acquisition criterion.</p> </li> <li> <code>doe_algorithm</code>               (<code>str</code>, default:                   <code>'OT_OPT_LHS'</code> )           \u2013            <p>The name of the DOE algorithm for the initial sampling.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>doe_settings</code>               (<code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the DOE algorithm for the initial sampling.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>doe_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Either the initial DOE size or 0 if it is inferred from <code>doe_settings</code>.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criteria in parallel.</p> </li> <li> <code>regression_algorithm</code>               (<code>str | BaseRegressor</code>, default:                   <code>'OTGaussianProcessRegressor'</code> )           \u2013            <p>The regression algorithm.</p> <pre><code>        Either the name of the regression algorithm\n        approximating the objective function over the design space\n        or the regression algorithm itself.\n</code></pre> </li> <li> <code>regression_file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the file to save the regression model.</p> <pre><code>        If empty, do not save the regression model.\n</code></pre> </li> <li> <code>regression_settings</code>               (<code>Mapping[str, Optional[Any]]</code>, default:                   <code>{}</code> )           \u2013            <p>The settings of the regression algorithm.</p> <pre><code>        This argument is ignored\n        when regression_algorithm is a\n        [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n</code></pre> </li> </ul>"},{"location":"user_guide/optimization/al/#acquisition-criteria","title":"Acquisition criteria","text":"<p>The three acquisition criteria are</p> Value Name Expression <code>\"EI\"</code> Expected improvement \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0]\\) <code>\"LCB\"</code> Lower confidence bound \\(\\mu(x)-\\kappa\\times\\sigma(x)\\) with \\(\\kappa&gt;0\\) <code>\"Output\"</code> Mean output \\(\\mu(x)\\) <p>where \\(Y\\) is a random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\) at input point \\(x\\). Most of the time, \\(Y\\) is a Gaussian process and \\(\\hat{f}\\) is a Kriging model.</p>"},{"location":"user_guide/optimization/al/#initial-training-dataset","title":"Initial training dataset","text":"<p>Any DOE algorithm known by GEMSEO can be used to generate the initial training dataset.</p>"},{"location":"user_guide/optimization/al/#parallel-acquisition","title":"Parallel acquisition","text":"<p>Points can be acquired by batch of \\(q&gt;1\\) points, as Kriging is well-suited to parallelize optimization<sup>1</sup>. To this aim, when <code>batch_size</code> is greater than 1, ActiveLearningAlgo uses a parallel version of the expected improvement criterion. Unfortunately, the latter has no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte-Carlo.</p>"},{"location":"user_guide/optimization/al/#surrogate-models","title":"Surrogate models","text":"<p>SurrogateBasedOptimization is compatible with all regressors, whose classes derive from BaseRegressor.</p> <p>It can also be used from an existing surrogate models. In this case, it will skip the construction and learning of the initial training dataset.</p> <ol> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/optimization/smt/","title":"SMT library","text":""},{"location":"user_guide/optimization/smt/#smts-surrogate-based-optimizers","title":"SMT's surrogate-based optimizers","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling with a focus on derivatives <sup>1</sup><sup>2</sup>.</p> <p><code>gemseo-mlearning</code> proposes the SMTEGO optimization library to easily use the surrogate-based optimizers available in SMT, through its <code>EGO</code> class.</p>"},{"location":"user_guide/optimization/smt/#basic-usage","title":"Basic usage","text":"<p>SMTEGO includes a single optimization algorithm, called <code>\"SMT_EGO\"</code>.</p> <p>Given a maximum number of iterations, it can be used as is by any OptimizationProblem: <pre><code>execute_algo(optimization_problem, \"SMT_EGO\", max_iter=50)\n</code></pre> and any MDOScenario: <pre><code>scenario.execute({\"algo_name\": \"SMT_EGO\", \"max_iter\": 50})\n</code></pre></p> <p>In this case, the settings are</p> <ul> <li>the expected improvement as acquisition criterion,</li> <li>1 point acquired at a time,</li> <li>the Kriging-based surrogate model <code>\"KRG\"</code>,</li> <li>10 initial training points based on a latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion   from 50 start points with a limit of 20 iterations per local optimization.</li> </ul>"},{"location":"user_guide/optimization/smt/#settings","title":"Settings","text":"<p>Regarding the settings of the SMT's <code>EGO</code> class, you will find more information in the SMT's user guide by looking at the table at the bottom of the page.</p> <p>In the following, the training output values already used and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"user_guide/optimization/smt/#general","title":"General","text":"<p>The settings of the SMT's SBO algorithm.</p> <p>Parameters:</p> <ul> <li> <code>criterion</code>               (<code>AcquisitionCriterion</code>, default:                   <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> )           \u2013            <p>The acquisition criterion.</p> </li> <li> <code>enable_tunneling</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to enable the penalization of points that have been already evaluated in EI criterion.</p> </li> <li> <code>n_doe</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of points of the initial LHS DOE to train the surrogate.</p> </li> <li> <code>n_max_optim</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The maximum number of iterations for each sub-optimization.</p> </li> <li> <code>n_parallel</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>n_start</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The number of sub-optimizations.</p> </li> <li> <code>qEI</code>               (<code>ParallelStrategy</code>, default:                   <code>&lt;ParallelStrategy.KBUB: 'KBUB'&gt;</code> )           \u2013            <p>The strategy to acquire points in parallel.</p> </li> <li> <code>random_state</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The Numpy RandomState object or seed number which controls random draws.</p> </li> <li> <code>surrogate</code>               (<code>Surrogate | SurrogateModel</code>, default:                   <code>&lt;Surrogate.KRG: 'KRG'&gt;</code> )           \u2013            <p>The SMT Kriging-based surrogate model used internally; either an instance of the surrogate before training or its class name.</p> </li> </ul>"},{"location":"user_guide/optimization/smt/#acquisition-criteria","title":"Acquisition criteria","text":"<p>The three acquisition criteria are</p> Value Name Expression <code>\"EI\"</code> Expected improvement \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y,0]\\) <code>\"LCB\"</code> Lower confidence bound \\(\\mu(x)-3\\times\\sigma(x)\\) <code>\"SBO\"</code> Kriging believer \\(\\mu(x)\\) <p>where \\(Y\\) is a Gaussian random variable with mean \\(\\mu(x)\\) and standard deviation \\(\\sigma(x)\\),</p>"},{"location":"user_guide/optimization/smt/#parallel-acquisition","title":"Parallel acquisition","text":"<p>Points can be acquired by batch of \\(q&gt;1\\) points, as Kriging is well-suited to parallelize optimization<sup>3</sup>. To this aim, when <code>n_parallel</code> is greater than 1, SMT uses a technique of virtual points to update the training dataset with \\(k\\leq q\\) training points whose output value mimics the substituted model using a strategy. The four strategies are:</p> Value Name Expression <code>\"CLmin\"</code> Minimum constant liar \\(\\min \\{y_1,\\ldots,y_n\\}\\) <code>\"KB\"</code> Kriging believer \\(\\mu(x)\\) <code>\"KBLB\"</code> Kriging believer lower bound \\(\\mu(x)-3\\sigma(x)\\) <code>\"KBRand\"</code> Kriging believer random bound \\(\\mu(x)+\\kappa(x)\\sigma(x)\\) <code>\"KBUB\"</code> Kriging believer upper bound \\(\\mu(x)+3\\sigma(x)\\) <p>where \\(\\kappa(x)\\) is the realization of a random variable distributed according to the standard normal distribution.</p>"},{"location":"user_guide/optimization/smt/#surrogate-models","title":"Surrogate models","text":"Value Name <code>\"GPX\"</code> Kriging based on the egobox library written in Rust <code>\"KRG\"</code> Kriging <code>\"KPLS\"</code> Kriging using partial least squares (PLS) to reduce the input dimension <code>\"KPLSK\"</code> A variant of KPLS <code>\"MGP\"</code> A marginal Gaussian process (MGP) regressor <ol> <li> <p>B. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, M. Morlier, and J. R. R. A. Martins. A python surrogate modeling framework with derivatives. Advances in Engineering Software, pages 102662, 2019. doi:https://doi.org/10.1016/j.advengsoft.2019.03.005.\u00a0\u21a9</p> </li> <li> <p>P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, and J. R. R. A. Martins. SMT 2.0: A surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes. Advances in Engineering Sofware, 188:103571, 2024. doi:https://doi.org/10.1016/j.advengsoft.2023.103571.\u00a0\u21a9</p> </li> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/regression/smt/","title":"SMT library","text":""},{"location":"user_guide/regression/smt/#smts-surrogate-models","title":"SMT's surrogate models","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling with a focus on derivatives <sup>1</sup><sup>2</sup>.</p> <p><code>gemseo-mlearning</code> proposes the SMTRegressor to easily use any SMT's surrogate model in your GEMSEO processes.</p>"},{"location":"user_guide/regression/smt/#basic-usage","title":"Basic usage","text":"<p>You only have to instantiate this SMTRegressor class from:</p> <ul> <li>the IODataset including your input and output samples,</li> <li>the name of an SMT's surrogate model (e.g. <code>\"KRG\"</code> for Kriging or <code>\"RBF\"</code> for radial basis function),</li> <li>the options of this surrogate model,</li> <li>and the usual options of a BaseRegressor,   namely   <code>transformer</code> to transform the input and output data,   <code>input_names</code> to use a subset of input variables and   <code>output_names</code> to use a subset of output variables.</li> </ul> <p>Here's how to build an SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>model = SMTRegressor(training_dataset, model_class_name=\"RBF\", parameters={\"d0\": 2})\nmodel.learn()\n</code></pre>"},{"location":"user_guide/regression/smt/#options","title":"Options","text":"<p>Regarding the options of the SMT's surrogate models, you will find more information in the SMT's user guide by looking at the tables at the bottom of the pages.</p>"},{"location":"user_guide/regression/smt/#derivatives","title":"Derivatives","text":""},{"location":"user_guide/regression/smt/#differentiation","title":"Differentiation","text":"<p>You can use the <code>predict_jacobian</code> method of any SMTRegressor as long as the corresponding SMT's surrogate model can be derived with respect to its inputs.</p> <p>This feature is particularly useful in the case of gradient-based optimization.</p>"},{"location":"user_guide/regression/smt/#gradient-enhanced","title":"Gradient-enhanced","text":"<p>You can wrap the gradient-enhanced surrogate models available in SMT, such as <code>\"GEKPLS\"</code> and <code>\"GENN\"</code>, as long as your training dataset includes both output and Jacobian samples (the Jacobian samples have to be stored in the <code>\"gradients\"</code> group of the IODataset).</p> <p>This feature can improve the quality of the surrogate model without increasing the size of the training dataset.</p> <ol> <li> <p>B. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, M. Morlier, and J. R. R. A. Martins. A python surrogate modeling framework with derivatives. Advances in Engineering Software, pages 102662, 2019. doi:https://doi.org/10.1016/j.advengsoft.2019.03.005.\u00a0\u21a9</p> </li> <li> <p>P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, and J. R. R. A. Martins. SMT 2.0: A surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes. Advances in Engineering Sofware, 188:103571, 2024. doi:https://doi.org/10.1016/j.advengsoft.2023.103571.\u00a0\u21a9</p> </li> </ol>"}]}