{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-mlearning","title":"gemseo-mlearning","text":"<p><code>gemseo-mlearning</code> is a plugin of the library GEMSEO, dedicated to machine learning. This package is open-source, under the LGPL v3 license.</p>"},{"location":"#overview","title":"Overview","text":"<p>This package adds new regression models based on SMT.</p> <p>A package for active learning is also available, deeply based on the core GEMSEO objects for optimization, as well as a SurrogateBasedOptimization library built on its top. An effort is being made to improve both content and performance in future versions.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest stable version with <code>pip install gemseo-mlearning</code>.</p> <p>Install the development version with <code>pip install gemseo-mlearning@git+https://gitlab.com/gemseo/dev/gemseo-mlearning.git@develop</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Antoine Dechaume</li> <li>Beno\u00eet Pauwels</li> <li>Cl\u00e9ment Laboulfie</li> <li>Matthias De Lozzo</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#develop","title":"Develop","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>ActiveLearningAlgo   can acquire points by batch using the <code>batch_size</code>   and <code>mc_size</code> argument,   when the regressor is based on a random process   such as <code>GaussianProcessRegressor</code>   and <code>OTGaussianProcessRegressor</code>.   This option is only available for criteria dedicated   to level set LevelSet   (alternatively quantile estimation Quantile)   and the expected improvement for maximum/minimum estimation Maximum.</li> <li>The Branin and   Rosenbrock problems   can be used to benchmark   the efficiency of the active learning algorithms   and estimate several quantities of interest,   optimas or quantiles for instance.</li> <li>AcquisitionView   can be used to plot   both the output of the original model,   the prediction of the surrogate model,   the standard deviation of the surrogate model   and the acquisition criterion,   when the input dimension is 1 or 2.</li> <li>SMTRegressor   can be any surrogate model available in the Python package SMT.</li> <li>SurrogateBasedOptimization   can use an existing BaseMLRegressionAlgo   and save the BaseMLRegressionAlgo that it enriches   using the <code>regression_file_path</code> option.</li> <li>Given a sequence of input points,   OTGaussianProcessRegressor   can generate samples of the conditioned Gaussian process   with its method compute_samples.</li> <li>The <code>multi_start_n_samples</code>, <code>multi_start_algo_name</code> and <code>multi_start_algo_options</code> arguments of   OTGaussianProcessRegressor   allows to use multi-start optimization for the covariance model parameters;   by default, the number of starting points <code>multi_start_n_samples</code> is set to 10.</li> <li>The <code>optimization_space</code> argument of   OTGaussianProcessRegressor   allows to set the lower and upper bounds of the covariance model parameters   by means of a DesignSpace.</li> <li>The <code>covariance_model</code> argument of   OTGaussianProcessRegressor   allows to use any covariance model proposed by OpenTURNS   and facilitates the use of some of them through the enumeration   CovarianceModel;   its default value is Mat\u00e9rn 5/2.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>BREAKING CHANGE: The argument <code>distribution</code> of   ActiveLearningAlgo.   renamed to <code>regressor</code>; it can be either a regressor   gemseo.mlearning.regression.algos.base_regressor.BaseRegressor   or a   gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.</li> <li>BREAKING CHANGE: The method <code>compute_next_input_data</code> of   ActiveLearningAlgo.   renamed to   find_next_point.</li> <li>BREAKING CHANGE: The method <code>update_algo</code> of   ActiveLearningAlgo.   renamed to   acquire_new_points.</li> <li>BREAKING CHANGE: The argument <code>trend_type</code> and the attribute <code>TrendType</code> of   OTGaussianProcessRegressor   renamed to <code>trend</code> and <code>Trend</code> respectively.</li> <li>BREAKING CHANGE: <code>MaxExpectedImprovement</code> renamed to <code>`Maximum</code>.</li> <li>BREAKING CHANGE: <code>MinExpectedImprovement</code> renamed to <code>`Minimum</code>.</li> <li>BREAKING CHANGE: <code>ExpectedImprovement</code> removed.</li> <li>BREAKING CHANGE: the acquisition criterion <code>LimitState</code> renamed to   LevelSet</li> <li>BREAKING CHANGE: each acquisition criterion class has a specific module   in gemseo_mlearning.active_learning.acquisition_criteria   whose name is the snake-case version of it class name, i.e. <code>nice_criterion.py</code> contains <code>NiceCriterion</code>.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.adaptive.distribution.MLRegressorDistribution</code> renamed to   gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.lib_surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.surrogate_based_optimization.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.core.surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.</li> <li>BREAKING CHANGE: <code>MLDataAcquisition</code> renamed to   ActiveLearningAlgo.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterion</code> renamed to   BaseAcquisitionCriterion   and moved to</li> <li>gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterionFactory</code> renamed to   AcquisitionCriterionFactory,   moved to   gemseo_mlearning.active_learning.acquisition_criteria   and without the property <code>available_criteria</code> (use <code>AcquisitionCriterionFactory.class_names</code>).</li> <li>BREAKING CHANGE: <code>gemseo.adaptive</code> renamed to gemseo_mlearning.active_learning.</li> <li>BREAKING CHANGE: <code>gemseo.adaptive.criteria</code> renamed to   gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE:   The module <code>gemseo_mlearning.api</code> no longer exists;   the functions   sample_discipline   and sample_disciplines   must be imported from gemseo_mlearning.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The data transformer can be set with the <code>\"transformer\"</code> key of the <code>regression_options</code> dictionary   passed to SurrogateBasedOptimization.</li> <li>The Quantile   estimates the quantile by Monte Carlo sampling   by means of the probability distributions of the input variables;   this distributions are defined with its new argument <code>uncertain_space</code>.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>sample_discipline</code>; use sample_disciplines from <code>gemseo</code> instead.</li> <li><code>sample_disciplines</code>; move to <code>gemseo</code>: sample_disciplines.</li> <li><code>MAEMeasure</code>; moved to <code>gemseo</code>: MAEMeasure.</li> <li><code>MEMeasure</code>; moved to <code>gemseo</code>: MEMeasure.</li> <li><code>GradientBoostingRegressor</code>; moved to <code>gemseo</code>: GradientBoostingRegressor.</li> <li><code>MLPRegressor</code>; moved to <code>gemseo</code>: MLPRegressor.</li> <li><code>OTGaussianProcessRegressor</code>; moved to <code>gemseo</code>: OTGaussianProcessRegressor.</li> <li><code>RegressorChain</code>; moved to <code>gemseo</code>: RegressorChain.</li> <li><code>SVMRegressor</code>; moved to <code>gemseo</code>: SVMRegressor.</li> <li><code>TPSRegressor</code>; moved to <code>gemseo</code>: TPSRegressor.</li> </ul>"},{"location":"changelog/#version-112-december-2023","title":"Version 1.1.2 (December 2023)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> <li>OTGaussianProcessRegressor   has a new optional argument <code>optimizer</code>   to select the OpenTURNS optimizer for the covariance model parameters.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-111-september-2023","title":"Version 1.1.1 (September 2023)","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>OTGaussianProcessRegressor.predict_std   no longer returns the variance of the output but its standard deviation.</li> </ul>"},{"location":"changelog/#version-110-june-2023","title":"Version 1.1.0 (June 2023)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>An argument <code>trend_type</code> of type   OTGaussianProcessRegressor.TrendType   to OTGaussianProcessRegressor;   the trend type of the Gaussian process regressor can be either constant,   linear or quadratic.</li> <li>A new optimization library   SurrogateBasedOptimization   to perform EGO-like surrogate-based optimization on unconstrained problems.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>The output of an MLDataAcquisitionCriterion   based on a regressor built from constant output values is no longer <code>nan</code>.</li> </ul>"},{"location":"changelog/#version-101-february-2022","title":"Version 1.0.1 (February 2022)","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>BaseRegressorDistribution   can now use a regression algorithm instantiated with transformers.</li> </ul>"},{"location":"changelog/#version-100-july-2022","title":"Version 1.0.0 (July 2022)","text":"<p>First release.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>The developers thank all the open source libraries making <code>gemseo-mlearning</code> possible.</p>"},{"location":"credits/#exec-1--external-dependencies","title":"External Dependencies","text":"<p><code>gemseo-mlearning</code> depends on software with compatible licenses that are listed below.</p> Project License <code>Python</code> Python Software License <code>gemseo</code> GNU Lesser General Public License v3 <code>numpy</code> BSD License <code>openturns</code> LGPL <code>scikit-learn</code> BSD License <code>scipy</code> BSD License <code>smt</code> BSD-3"},{"location":"credits/#exec-1--external-applications","title":"External applications","text":"<p>Some external applications are used by <code>gemseo-mlearning</code>, but not linked with the application, for testing, documentation generation, training or example purposes.</p> Project License <code>black</code> MIT <code>commitizen</code> MIT License <code>covdefaults</code> MIT License <code>docformatter</code> MIT License <code>griffe-inherited-docstrings</code> ISC <code>insert-license</code> MIT <code>markdown-exec</code> ISC <code>mike</code> BSD-3-Clause <code>mkdocs-bibtex</code> BSD-3-Clause-LBNL <code>mkdocs-gallery</code> BSD 3-Clause <code>mkdocs-gen-files</code> MIT License <code>mkdocs-include-markdown-plugin</code> Apache Software License <code>mkdocs-literate-nav</code> MIT License <code>mkdocs-material</code> MIT License <code>mkdocs-section-index</code> MIT License <code>mkdocstrings</code> ISC <code>pre-commit</code> MIT License <code>pygrep-hooks</code> MIT <code>pytest</code> MIT License <code>pytest-cov</code> MIT License <code>pytest-xdist</code> MIT License <code>ruff</code> MIT License <code>setuptools</code> MIT License <code>setuptools-scm</code> MIT License"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-mlearning</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-mlearning</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-mlearning</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"developer_guide/active_learning/","title":"Active learning","text":""},{"location":"developer_guide/active_learning/#active-learning","title":"Active learning","text":"<p>This section describes the design of the active_learning subpackage.</p> <p>Info</p> <p>Open the user guide for general information, e.g. concepts, API, examples, etc.</p>"},{"location":"developer_guide/active_learning/#tree-structure","title":"Tree structure","text":"<pre><code>\ud83d\udcc1 gemseo_mlearning\n\u2514\u2500\u2500 \ud83d\udcc1 active_learning # Subpackage for active learning (AL)\n    \u251c\u2500\u2500 \ud83d\udcc4 active_learning_algo.py # Class to set and solve an AL problem\n    \u251c\u2500\u2500 \ud83d\udcc1 acquisition_criteria # Acquisition criteria (ACs)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 exploration # ACs to improve the regressor\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_exploration.py # Base class for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 distance.py # AC (distance to the learning set)\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 exploration.py # Specific AC family\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 factory.py # Factory for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 standard_deviation.py # An AC (regressor's standard deviation)\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 variance.py # AC (regressor's variance)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 level_set/ # ACs to approximate a level set\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 maximum/ # ACs to approximate the global maximum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 minimum/ # ACs to approximate the global minimum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 quantile/ # ACs to approximate a quantile\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion.py # Base class for ACs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion_family.py # Base class for AC families\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 base_factory.py # Base class for AC\n    \u251c\u2500\u2500 \ud83d\udcc1 distributions # Regressor distributions (RD)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_regressor_distribution.py # The base class for RDs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 kriging_distribution.py # The RD for Kriging regressors\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 regressor_distribution.py # The RD for any regressor\n    \u2514\u2500\u2500 \ud83d\udcc1 visualization # Visualization tools\n        \u2514\u2500\u2500 \ud83d\udcc4 acquisition_view.py # Plot the acquisition process (in 2D only)\n</code></pre>"},{"location":"developer_guide/active_learning/#class-diagram","title":"Class diagram","text":"<pre><code>classDiagram\n\n    ActiveLearning *-- OptimizationProblem\n    ActiveLearning o-- DesignSpace\n    ActiveLearning --&gt; AcquisitionCriterionFamilyFactory: criterion_family_name\n    ActiveLearning --&gt; BaseAcquisitionCriterionFactory: criterion_name \\n criterion_options\n    ActiveLearning --&gt; BaseRegressorDistribution: regressor\n    BaseDriverLibrary --* ActiveLearning\n    BaseDOELibrary --* BaseDriverLibrary\n    BaseOptimizationLibrary --* BaseDriverLibrary\n    Database --* ActiveLearning\n    AcquisitionView --* ActiveLearning\n\n    OptimizationProblem o-- BaseAcquisitionCriterion\n\n    AcquisitionCriterionFamilyFactory --&gt; BaseAcquisitionCriterionFamily\n    BaseAcquisitionCriterionFamily --&gt; BaseAcquisitionCriterionFactory\n    BaseAcquisitionCriterionFactory --&gt; BaseAcquisitionCriterion\n\n    BaseAcquisitionCriterion --|&gt; MDOFunction\n    BaseAcquisitionCriterion o-- BaseRegressorDistribution\n\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterion\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFactory\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFamily\n    &lt;&lt;abstract&gt;&gt; BaseRegressorDistribution\n\n    class ActiveLearning {\n        +default_algo_name\n        +default_doe_options\n        +default_opt_options\n        +acquisition_criterion\n        +input_space\n        +n_initial_samples\n        +qoi\n        +qoi_history\n        +regressor\n        +regressor_distribution\n        +set_acquisition_algorithm()\n        +find_nex_point()\n        +acquire_new_points()\n        +plot_acquisition_view()\n        +plot_qoi_history()\n        +update_problem()\n    }\n\n    class AcquisitionCriterionFamilyFactory {\n        +create()\n    }\n\n    class BaseAcquisitionCriterionFamily {\n        +ACQUISITION_CRITERION_FACTORY\n    }\n\n    class BaseAcquisitionCriterionFactory {\n        #DEFAULT_CLASS_NAME\n    }</code></pre>"},{"location":"developer_guide/active_learning/#how-to","title":"How to...","text":"<code>... create a new family of acquisition criteria?</code> <ol> <li>Derive an abstract class <code>BaseNewAcquisitionCriterion</code> from    BaseAcquisitionCriterion.</li> <li>Derive <code>FirstAcquisitionCriterion</code>, <code>SecondAcquisitionCriterion</code>, ... from <code>BaseNewAcquisitionCriterion</code>    in modules located in <code>root_package_name.subpackage_name.package_name</code>.</li> <li>Derive <code>NewAcquisitionCriterionFactory</code> from    BaseAcquisitionCriterionFactory    and set the class attributes:    <pre><code>_CLASS = BaseNewAcquisitionCriterion\n_DEFAULT_CLASS_NAME = \"FirstAcquisitionCriterion\"\n_MODULE_NAMES = (\"root_package.subpackage_name.package_name\",)\n</code></pre></li> <li>Derive <code>NewAcquisitionCriterionFamily</code> from    BaseAcquisitionCriterionFamily    and set the class attribute <code>ACQUISITION_CRITERION_FACTORY = NewAcquisitionCriterionFactory</code>.</li> </ol> <p>Now, the user can instantiate the ActiveLearningAlgo</p> <ul> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   to use this family of acquisition criteria,</li> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   and <code>criterion_name=\"SecondAcquisitionCriterion\"</code>   to use a specific acquisition criterion from this family.</li> </ul>"},{"location":"generated/examples/active_learning/","title":"Active learning","text":""},{"location":"generated/examples/active_learning/#active-learning","title":"Active learning","text":"<p> Efficient global optimization using a Gaussian process regressor. </p> <p> Quantile estimation using a Gaussian process regressor. </p> <p> Expected improvement using a Gaussian process regressor. </p> <p> Expected improvement based on bootstrap. </p> <p> Expected improvement based on cross-validation. </p> <p> EGO based on resampling. </p> <p> Cross-validation vs bootstrap. </p> <p> Download all examples in Python source code: active_learning_python.zip</p> <p> Download all examples in Jupyter notebooks: active_learning_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_gpr/","title":"Efficient global optimization using a Gaussian process regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_gpr/#efficient-global-optimization-using-a-gaussian-process-regressor","title":"Efficient global optimization using a Gaussian process regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configure\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.disciplines.analytic import AnalyticDiscipline\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\n\nconfigure(False, False, True, False, False, False, False)\nconfigure_logger()\n</code></pre> <p>We consider the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\):</p> <pre><code>discipline = AnalyticDiscipline({\"z\": \"(1-x)**2+100*(y-x**2)**2\"}, name=\"Rosenbrock\")\n</code></pre> <p>defined over the input space \\([-2,2]^2\\):</p> <pre><code>input_space = DesignSpace()\ninput_space.add_variable(\"x\", lower_bound=-2, upper_bound=2, value=1.0)\ninput_space.add_variable(\"y\", lower_bound=-2, upper_bound=2, value=1.0)\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 30 samples:</p> <pre><code>learning_dataset = sample_disciplines([discipline], input_space, \"z\", 30, \"OT_OPT_LHS\")\n</code></pre> <p>and an initial Gaussian process regressor:</p> <pre><code>gpr = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Thirdly, we look for 20 points that will help us get closer to the minimum; by default, for this purpose, the active learning algorithm looks for the point maximizing the expected improvement.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Minimum\", input_space, gpr)\nactive_learning.set_acquisition_algorithm(\"DIFFERENTIAL_EVOLUTION\", max_iter=1000)\n# active_learning.set_acquisition_algorithm(\"fullfact\", n_samples=30 * 30)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Lastly, we plot the training points, the original model, the Gaussian process regressor and the expected improvement after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_gpr.py</p> <p> Download Jupyter notebook: ego_gpr.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_rbf/","title":"EGO based on resampling.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_rbf/#ego-based-on-resampling","title":"EGO based on resampling.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nfrom gemseo import configure\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.disciplines.analytic import AnalyticDiscipline\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import linspace\nfrom numpy import zeros\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei import EI\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nconfigure(False, False, True, False, False, False, False)\nconfigure_logger()\n\nn_test = 20\n</code></pre> <p>Definition of the discipline</p> <pre><code>discipline = AnalyticDiscipline({\"z\": \"(1-x)**2+100*(y-x**2)**2\"}, name=\"Rosenbrock\")\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#definition-of-the-input-space","title":"Definition of the input space","text":"<pre><code>input_space = DesignSpace()\ninput_space.add_variable(\"x\", lower_bound=-2, upper_bound=2, value=1.0)\ninput_space.add_variable(\"y\", lower_bound=-2, upper_bound=2, value=1.0)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>learning_dataset = sample_disciplines([discipline], input_space, \"z\", 30, \"OT_OPT_LHS\")\nalgo = RBFRegressor(learning_dataset)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#universal-distribution-for-this-surrogate-model","title":"Universal distribution for this surrogate model","text":"<pre><code>distribution = RegressorDistribution(algo, bootstrap=False)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#data-acquisition-to-improve-the-surrogate-model","title":"Data acquisition to improve the surrogate model","text":"<pre><code>acquisition = ActiveLearningAlgo(\"Minimum\", input_space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\", n_samples=1000)\nacquisition.acquire_new_points(discipline, 20)\n\nopt = distribution.algo.learning_set.get_view(variable_names=[\"x\", \"y\"]).to_numpy()\nopt_x = opt[:, 0]\nopt_y = opt[:, 1]\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#evaluation-of-discipline-and-expected-improvement","title":"Evaluation of discipline and expected improvement","text":"<pre><code>crit = EI(distribution)\nx_test = linspace(-2, 2, n_test)\ndisc_data = zeros((n_test, n_test))\ncrit_data = zeros((n_test, n_test))\nsurr_data = zeros((n_test, n_test))\nfor i in range(n_test):\n    for j in range(n_test):\n        xij = array([x_test[j], x_test[i]])\n        input_data = {\"x\": array([xij[0]]), \"y\": array([xij[1]])}\n        disc_data[i, j] = discipline.execute(input_data)[\"z\"][0]\n        crit_data[i, j] = crit.func(xij)\n        surr_data[i, j] = algo.predict(xij)[0]\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rbf/#plotting","title":"Plotting","text":"<pre><code>train = learning_dataset.get_view(variable_names=[\"x\", \"y\"]).to_numpy()\nx_train = train[:, 0]\ny_train = train[:, 1]\nfig = plt.figure(constrained_layout=True)\nspec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\naxes = [[None, None], [None, None]]\ntitles = [[\"Discipline\", \"Infill criterion\"], [\"Surrogate model\", None]]\ndata = [[disc_data, crit_data], [surr_data, None]]\nfor i in range(2):\n    for j in range(2):\n        if [i, j] != [1, 1]:\n            axes[i][j] = fig.add_subplot(spec[i, j])\n            axes[i][j].contourf(x_test, x_test, data[i][j])\n            axes[i][j].axhline(1, color=\"white\", alpha=0.5)\n            axes[i][j].axvline(1, color=\"white\", alpha=0.5)\n            axes[i][j].plot(x_train, y_train, \"w+\", ms=1)\n            for index, _ in enumerate(opt_x):\n                axes[i][j].plot(opt_x[index], opt_y[index], \"wo\", ms=1)\n                axes[i][j].annotate(\n                    index + 1,\n                    (opt_x[index] + 0.05, opt_y[index] + 0.05),\n                    color=\"white\",\n                )\n            axes[i][j].set_title(titles[i][j])\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_rbf.py</p> <p> Download Jupyter notebook: ego_rbf.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/","title":"Expected improvement based on bootstrap.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#expected-improvement-based-on-bootstrap","title":"Expected improvement based on bootstrap.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb import UCB\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei import EI\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb import LCB\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>algo = RBFRegressor(dataset)\nalgo.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#create-mlalgosampler","title":"Create MLAlgoSampler","text":"<pre><code>distribution = RegressorDistribution(algo)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#filling-objectives","title":"Filling objectives","text":"<pre><code>ego = EI(distribution)\nlower = LCB(distribution, kappa=2.0)\nupper = UCB(distribution, kappa=2.0)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#find-next-training-point","title":"Find next training point","text":"<pre><code>space = DesignSpace()\nspace.add_variable(\"x\", lower_bound=x_l, upper_bound=x_u, value=1.5)\n\nacquisition = ActiveLearningAlgo(\"Minimum\", space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\")\nopt = acquisition.find_next_point()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#evaluation-of-discipline-surrogate-model-and-expected-improvement","title":"Evaluation of discipline, surrogate model and expected improvement","text":"<pre><code>x_test = linspace(x_l, x_u, n_test)\nego_data = []\nsurr_data = []\nlower_data = []\nupper_data = []\ny_test = f(x_test)\nfor x_i in x_test:\n    surr_data.append(algo.predict(array([x_i]))[0])\n    ego_data.append(ego.func(array([x_i]))[0])\n    lower_data.append(lower.func(array([x_i]))[0] * lower.output_range)\n    upper_data.append(upper.func(array([x_i]))[0] * upper.output_range)\nego_data = array(ego_data)\nlower_data = array(lower_data)\nupper_data = array(upper_data)\n\ndisc_data = IODataset()\ndisc_data.add_input_variable(\"x\", x_test)\ndisc_data.add_output_variable(\"y\", y_test)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_bootstrap/#plotting","title":"Plotting","text":"<pre><code>fig, ax = plt.subplots(2, 1)\nfor algo_b in distribution.algos:\n    algo_data = [algo_b.predict(array([x_i])) for x_i in x_test]\n    ax[0].plot(x_test, algo_data, \"gray\", alpha=0.2)\nax[0].plot(\n    x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n)\nax[0].plot(\n    x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n)\nax[0].plot(x_test, surr_data, \"b\", label=\"surrogate\")\nax[0].fill_between(\n    x_test, lower_data, upper_data, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax[0].legend(loc=\"upper right\")\nax[0].axvline(x=opt[0])\nax[1].plot(x_test, ego_data, \"r\", label=\"EGO\")\nax[1].axvline(x=opt[0])\nax[1].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: expected_improvement_bootstrap.py</p> <p> Download Jupyter notebook: expected_improvement_bootstrap.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/expected_improvement_cv/","title":"Expected improvement based on cross-validation.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#expected-improvement-based-on-cross-validation","title":"Expected improvement based on cross-validation.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb import UCB\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei import EI\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb import LCB\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>algo = RBFRegressor(dataset)\nalgo.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#create-mlalgosampler","title":"Create MLAlgoSampler","text":"<pre><code>distribution = RegressorDistribution(algo, bootstrap=False, loo=True)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#filling-objectives","title":"Filling objectives","text":"<pre><code>ego = EI(distribution)\nlower = LCB(distribution, kappa=2.0)\nupper = UCB(distribution, kappa=2.0)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#find-next-training-point","title":"Find next training point","text":"<pre><code>space = DesignSpace()\nspace.add_variable(\"x\", lower_bound=x_l, upper_bound=x_u, value=1.5)\n\nacquisition = ActiveLearningAlgo(\"Minimum\", space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\")\nopt = acquisition.find_next_point()\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#evaluation-of-discipline-surrogate-model-and-expected-improvement","title":"Evaluation of discipline, surrogate model and expected improvement","text":"<pre><code>x_test = linspace(x_l, x_u, n_test)\nego_data = []\nsurr_data = []\nlower_data = []\nupper_data = []\ny_test = f(x_test)\nfor x_i in x_test:\n    surr_data.append(algo.predict(array([x_i]))[0])\n    ego_data.append(ego.func(array([x_i]))[0])\n    lower_data.append(lower.func(array([x_i]))[0] * lower.output_range)\n    upper_data.append(upper.func(array([x_i]))[0] * upper.output_range)\nego_data = array(ego_data)\nlower_data = array(lower_data)\nupper_data = array(upper_data)\n\ndisc_data = IODataset()\ndisc_data.add_input_variable(\"x\", x_test)\ndisc_data.add_output_variable(\"y\", y_test)\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv/#plotting","title":"Plotting","text":"<pre><code>fig, ax = plt.subplots(2, 1)\nfor algo_b in distribution.algos:\n    algo_data = [algo_b.predict(array([x_i])) for x_i in x_test]\n    ax[0].plot(x_test, algo_data, \"gray\", alpha=0.2)\nax[0].plot(\n    x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n)\nax[0].plot(\n    x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n)\nax[0].plot(x_test, surr_data, \"b\", label=\"surrogate\")\nax[0].fill_between(\n    x_test, lower_data, upper_data, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax[0].legend(loc=\"upper right\")\nax[0].axvline(x=opt[0])\nax[1].plot(x_test, ego_data, \"r\", label=\"EGO\")\nax[1].axvline(x=opt[0])\nax[1].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: expected_improvement_cv.py</p> <p> Download Jupyter notebook: expected_improvement_cv.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/expected_improvement_cv_boot/","title":"Cross-validation vs bootstrap.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/expected_improvement_cv_boot/#cross-validation-vs-bootstrap","title":"Cross-validation vs bootstrap.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb import UCB\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei import EI\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb import LCB\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv_boot/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n\nax = [[None, None], [None, None]]\nax[0][0] = plt.subplot(221)\nax[0][1] = plt.subplot(222, sharey=ax[0][0])\nax[1][0] = plt.subplot(223)\nax[1][1] = plt.subplot(224, sharey=ax[1][0])\n</code></pre>"},{"location":"generated/examples/active_learning/expected_improvement_cv_boot/#active-learning","title":"Active learning","text":"<p>We compare the bootstrap and leave-one-out methods in search of a new point to learn in order to estimate the minimum of the discipline</p> <pre><code>for index, bootstrap in enumerate([False, True]):\n    # Train a RBF regression model\n    algo = RBFRegressor(dataset)\n    algo.learn()\n\n    # Build a regressor distribution\n    distribution = RegressorDistribution(algo, bootstrap=bootstrap, loo=not bootstrap)\n    distribution.learn()\n\n    # Define the expected improvement measure\n    ego = EI(distribution)\n\n    # Define confidence bounds, equal to mean +/- 2*sigma\n    lower = LCB(distribution, kappa=2.0)\n    upper = UCB(distribution, kappa=2.0)\n\n    # Define the input_space\n    input_space = DesignSpace()\n    input_space.add_variable(\"x\", lower_bound=x_l, upper_bound=x_u, value=1.5)\n\n    # Define the data acquisition process\n    acquisition = ActiveLearningAlgo(\"Minimum\", input_space, distribution)\n    acquisition.set_acquisition_algorithm(\"fullfact\")\n\n    # Compute the next input data\n    opt = acquisition.find_next_point()\n\n    # Plot the results\n    x_test = linspace(x_l, x_u, n_test)\n    ego_data = []\n    surr_data = []\n    lower_data = []\n    upper_data = []\n    y_test = f(x_test)\n    for x_i in x_test:\n        surr_data.append(algo.predict(array([x_i]))[0])\n        ego_data.append(ego.func(array([x_i]))[0])\n        lower_data.append(lower.func(array([x_i]))[0] * lower.output_range)\n        upper_data.append(upper.func(array([x_i]))[0] * upper.output_range)\n\n    disc_data = IODataset()\n    disc_data.add_input_variable(\"x\", x_test)\n    disc_data.add_output_variable(\"y\", y_test)\n\n    for algo in distribution.algos:\n        algo_data = [algo.predict(array([x_i])) for x_i in x_test]\n        ax[0][index].plot(x_test, algo_data, \"gray\", alpha=0.2)\n\n    ax[0][index].plot(\n        x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n    )\n    ax[0][index].plot(\n        x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n    )\n    ax[0][index].plot(x_test, surr_data, \"b\", label=\"surrogate\")\n    ax[0][index].fill_between(\n        x_test,\n        array(lower_data),\n        array(upper_data),\n        color=\"b\",\n        alpha=0.1,\n        label=\"CI(95%)\",\n    )\n    ax[0][index].legend(loc=\"upper right\")\n    ax[0][index].axvline(x=opt[0])\n    ax[1][index].plot(x_test, array(ego_data), \"r\", label=\"EGO\")\n    ax[1][index].axvline(x=opt[0])\n    ax[1][index].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: expected_improvement_cv_boot.py</p> <p> Download Jupyter notebook: expected_improvement_cv_boot.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/expected_improvement_gpr/","title":"Expected improvement using a Gaussian process regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/expected_improvement_gpr/#expected-improvement-using-a-gaussian-process-regressor","title":"Expected improvement using a Gaussian process regressor.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb import UCB\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei import EI\nfrom gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb import LCB\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.kriging_distribution import (\n    KrigingDistribution,\n)\n</code></pre> <p>We consider the function \\(f(x)=10\\cos(2x)+15-5x+x^2/50\\):</p> <pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n</code></pre> <p>defined over the input space \\([-3,3]\\):</p> <pre><code>input_space = DesignSpace()\nlower_bound = -3.0\nupper_bound = 3.0\ninput_space.add_variable(\n    \"x\", lower_bound=lower_bound, upper_bound=upper_bound, value=1.5\n)\n</code></pre> <p>First, we create an initial training dataset:</p> <pre><code>x_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ntraining_dataset = IODataset()\ntraining_dataset.add_input_variable(\"x\", x_train)\ntraining_dataset.add_output_variable(\"y\", y_train)\n</code></pre> <p>and an initial Gaussian process regressor:</p> <pre><code>gpr = GaussianProcessRegressor(training_dataset)\n</code></pre> <p>Then, we create a Kriging distribution:</p> <pre><code>kriging_distribution = KrigingDistribution(gpr)\nkriging_distribution.learn()\n</code></pre> <p>as well as three acquisition criteria:</p> <pre><code>expected_improvement = EI(kriging_distribution)\nmean_minus_2sigma = LCB(kriging_distribution, kappa=2.0)\nmean_plus_2sigma = UCB(kriging_distribution, kappa=2.0)\n</code></pre> <p>Thirdly, we look for the point that will help us get closer to the minimum; by default, for this purpose, the active learning algorithm looks for the point maximizing the expected improvement.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Minimum\", input_space, kriging_distribution)\nactive_learning.set_acquisition_algorithm(\"TNC\")\nnext_input_data = active_learning.find_next_point()[0]\n</code></pre> <p>Fourthly, we evaluate the different acquisition criteria over a fine grid as well as the original function and the Gaussian process regressor:</p> <pre><code>ei_values = []\npredictions = []\nmm2s_values = []\nmp2s_values = []\nx_test = linspace(lower_bound, upper_bound, 200)\ny_test = f(x_test)\nfor x_i in x_test:\n    x_i = array([x_i])\n    predictions.append(gpr.predict(x_i)[0])\n    ei_values.append(expected_improvement.func(x_i)[0])\n    mm2s_values.append(mean_minus_2sigma.func(x_i)[0] * mean_minus_2sigma.output_range)\n    mp2s_values.append(mean_plus_2sigma.func(x_i)[0] * mean_plus_2sigma.output_range)\n</code></pre> <p>Lastly, we plot the training points, the original model, the Gaussian process regressor and the 95% confidence interval on a first sub-plot:</p> <pre><code>fig, (ax1, ax2) = plt.subplots(2, 1)\nax1.plot(x_train, y_train, \"ro\", label=\"Training points\")\nax1.plot(x_test, y_test, \"r\", label=\"Original model\")\nax1.plot(x_test, predictions, \"b\", label=\"GP regressor\")\nax1.fill_between(\n    x_test, mm2s_values, mp2s_values, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax1.grid()\n</code></pre> <p>and the expected improvement on a second plots:</p> <pre><code>ax1.legend(loc=\"upper right\")\nax1.axvline(x=next_input_data)\nax2.plot(x_test, ei_values, \"r\", label=\"EI\")\nax2.axvline(x=next_input_data)\nax2.legend()\nax2.grid()\nplt.show()\n</code></pre> <p>The vertical blue line indicates the point maximizing the expected improvement.</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: expected_improvement_gpr.py</p> <p> Download Jupyter notebook: expected_improvement_gpr.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile_gpr/","title":"Quantile estimation using a Gaussian process regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile_gpr/#quantile-estimation-using-a-gaussian-process-regressor","title":"Quantile estimation using a Gaussian process regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configure\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.algos.parameter_space import ParameterSpace\nfrom gemseo.disciplines.analytic import AnalyticDiscipline\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\n\nconfigure(False, False, True, False, False, False, False)\nconfigure_logger()\n</code></pre> <p>We consider the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\):</p> <pre><code>discipline = AnalyticDiscipline({\"z\": \"(1-x)**2+100*(y-x**2)**2\"}, name=\"Rosenbrock\")\n</code></pre> <p>defined over the input space \\([-2,2]^2\\):</p> <pre><code>input_space = DesignSpace()\ninput_space.add_variable(\"x\", lower_bound=-2, upper_bound=2, value=1.0)\ninput_space.add_variable(\"y\", lower_bound=-2, upper_bound=2, value=1.0)\n</code></pre> <p>with the uncertain space:</p> <pre><code>uncertain_space = ParameterSpace()\nuncertain_space.add_random_variable(\"x\", \"OTUniformDistribution\", minimum=-2, maximum=2)\nuncertain_space.add_random_variable(\"y\", \"OTUniformDistribution\", minimum=-2, maximum=2)\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 30 samples:</p> <pre><code>learning_dataset = sample_disciplines([discipline], input_space, \"z\", 10, \"OT_OPT_LHS\")\n</code></pre> <p>and an initial Gaussian process regressor:</p> <pre><code>gpr = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Then, we look for 20 points that will help us get closer to the minimum; by default, for this purpose, the active learning algorithm looks for the point maximizing the U-function.</p> <pre><code>level = 0.35\nactive_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    gpr,\n    level=level,\n    uncertain_space=uncertain_space,\n    # criterion_name=\"EF\",\n)\n# active_learning.set_acquisition_algorithm(\"fullfact\", n_samples=30 * 30)\n# active_learning.set_acquisition_algorithm(\"NELDER-MEAD\")\n# active_learning.set_acquisition_algorithm(\"SLSQP\")\nactive_learning.set_acquisition_algorithm(\"DIFFERENTIAL_EVOLUTION\")\nactive_learning.acquire_new_points(discipline, 90)\n</code></pre> <p>Lastly, we plot the history of the quantity of interest</p> <pre><code>active_learning.plot_qoi_history()\n</code></pre> <p>as well as the training points, the original model, the Gaussian process regressor and the U-function after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n\ndataset = sample_disciplines(\n    [discipline], uncertain_space, \"z\", 100000, \"OT_MONTE_CARLO\"\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"z\"]).compute_quantile(level)\nprint(reference_quantile, active_learning.qoi)  # noqa\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: quantile_gpr.py</p> <p> Download Jupyter notebook: quantile_gpr.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/","title":"Optimization","text":""},{"location":"generated/examples/optimization/#surrogate-based-optimization","title":"Surrogate-based optimization","text":"<p> Surrogate-based optimization of Rastrigin's function. </p> <p> Download all examples in Python source code: optimization_python.zip</p> <p> Download all examples in Jupyter notebooks: optimization_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/sbo_rastrigin/","title":"Surrogate-based optimization of Rastrigin's function.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/sbo_rastrigin/#surrogate-based-optimization-of-rastrigins-function","title":"Surrogate-based optimization of Rastrigin's function.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configure_logger\nfrom gemseo import execute_algo\nfrom gemseo import execute_post\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\n\nconfigure_logger()\n\nproblem = Rastrigin()\n\nexecute_algo(\n    problem,\n    algo_name=\"SBO\",\n    acquisition_options={\n        \"max_iter\": 1000,\n        \"popsize\": 50,\n        \"stop_crit_n_x\": 10000,\n    },\n    max_iter=20,\n)\n\nexecute_post(problem, \"OptHistoryView\", save=False, show=True)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: sbo_rastrigin.py</p> <p> Download Jupyter notebook: sbo_rastrigin.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/","title":"Regression models","text":""},{"location":"generated/examples/regression/#regression-models","title":"Regression models","text":"<p> SMT's surrogate model. </p> <p> Download all examples in Python source code: regression_python.zip</p> <p> Download all examples in Jupyter notebooks: regression_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/mg_execution_times/","title":"Computation times","text":"<p>00:01.769 total execution time for generated_examples_regression files:</p> <p>+------------------------------------------------------------------+-----------+--------+ | plot_smt (docs/examples/regression/plot_smt.py) | 00:01.769 | 0.0 MB | +------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/regression/plot_smt/","title":"SMT's surrogate model.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/regression/plot_smt/#smts-surrogate-model","title":"SMT's surrogate model.","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling, with a focus on derivatives. The SMTRegressor class allows you to use any SMT's surrogate model in your GEMSEO processes, including the gradient-enhanced surrogate models as long as your training dataset includes both output and gradient samples as explained at the end of this page.</p> <p>In this example, we will approximate the [Rosenbrock function]<sup>1</sup></p> \\[f(x,y) = (1-x)^2 + 100(y-x^2)^2\\] <p>over the domain \\([-2,2]^2\\).</p> <pre><code>from gemseo import compute_doe\nfrom gemseo import sample_disciplines\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom gemseo.post.dataset.zvsxy import ZvsXY\n\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\nfrom gemseo_mlearning.regression.smt_regressor import SMTRegressor\n</code></pre> <p>First, we create the Rosenbrock discipline:</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>and the input space:</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/openturns/joint.py:54: DocstringInheritanceWarning: in OTJointDistribution._create_distribution: section Args: the docstring for the argument 'copula' is missing.\n  def _create_distribution(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/openturns/joint.py:54: DocstringInheritanceWarning: in OTJointDistribution._create_distribution: section Args: the docstring for the argument 'distributions' is missing.\n  def _create_distribution(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/openturns/joint.py:74: DocstringInheritanceWarning: in OTJointDistribution.compute_cdf: section Args: the docstring for the argument 'vector' is missing.\n  def compute_cdf(  # noqa: D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/openturns/joint.py:85: DocstringInheritanceWarning: in OTJointDistribution.compute_inverse_cdf: section Args: the docstring for the argument 'vector' is missing.\n  def compute_inverse_cdf(  # noqa: D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/openturns/distribution.py:180: DocstringInheritanceWarning: in OTDistribution._pdf: section Args: the docstring for the argument 'value' is missing.\n  def _pdf(self, value: float) -&gt; float:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/scipy/joint.py:69: DocstringInheritanceWarning: in SPJointDistribution._create_distribution: section Args: the docstring for the argument '**options' is missing.\n  def _create_distribution(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/scipy/joint.py:75: DocstringInheritanceWarning: in SPJointDistribution.compute_cdf: section Args: the docstring for the argument 'vector' is missing.\n  def compute_cdf(  # noqa: D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/scipy/joint.py:84: DocstringInheritanceWarning: in SPJointDistribution.compute_inverse_cdf: section Args: the docstring for the argument 'vector' is missing.\n  def compute_inverse_cdf(  # noqa: D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/uncertainty/distributions/scipy/distribution.py:147: DocstringInheritanceWarning: in SPDistribution._pdf: section Args: the docstring for the argument 'value' is missing.\n  def _pdf(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/pydantic/main.py:209: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n</code></pre> <p>Then, we use an optimized Latin hypercube sampling (LHS) technique to generate 20 samples:</p> <pre><code>training_data = sample_disciplines([discipline], input_space, \"y\", 20, \"OT_OPT_LHS\")\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:81: DocstringInheritanceWarning: in GraphView.node: section Args: the docstring for the argument 'label' is missing.\n  def node(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:81: DocstringInheritanceWarning: in GraphView.node: section Args: the docstring for the argument '_attributes' is missing.\n  def node(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:81: DocstringInheritanceWarning: in GraphView.node: section Args: the docstring for the argument '**attrs' is missing.\n  def node(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:88: DocstringInheritanceWarning: in GraphView.edge: section Args: the docstring for the argument 'head_name' is missing.\n  def edge(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:88: DocstringInheritanceWarning: in GraphView.edge: section Args: the docstring for the argument 'label' is missing.\n  def edge(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:88: DocstringInheritanceWarning: in GraphView.edge: section Args: the docstring for the argument '_attributes' is missing.\n  def edge(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/post/_graph_view.py:88: DocstringInheritanceWarning: in GraphView.edge: section Args: the docstring for the argument '**attrs' is missing.\n  def edge(  # noqa:D102\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:139: DocstringInheritanceWarning: in _RealJacobianOperator._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:142: DocstringInheritanceWarning: in _RealJacobianOperator._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:157: DocstringInheritanceWarning: in _AdjointJacobianOperator._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:160: DocstringInheritanceWarning: in _AdjointJacobianOperator._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:174: DocstringInheritanceWarning: in _IdentityOperator._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:177: DocstringInheritanceWarning: in _IdentityOperator._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:208: DocstringInheritanceWarning: in _SumOperation._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:211: DocstringInheritanceWarning: in _SumOperation._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:218: DocstringInheritanceWarning: in _SumOperationWithArray._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:221: DocstringInheritanceWarning: in _SumOperationWithArray._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:228: DocstringInheritanceWarning: in _SubOperation._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:231: DocstringInheritanceWarning: in _SubOperation._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:238: DocstringInheritanceWarning: in _SubOperationWithArray._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:241: DocstringInheritanceWarning: in _SubOperationWithArray._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:279: DocstringInheritanceWarning: in _ComposedOperationArrayOperator._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:282: DocstringInheritanceWarning: in _ComposedOperationArrayOperator._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:291: DocstringInheritanceWarning: in _ComposedOperationOperatorOperator._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:294: DocstringInheritanceWarning: in _ComposedOperationOperatorOperator._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:303: DocstringInheritanceWarning: in _ComposedOperationOperatorArray._matvec: section Args: the docstring for the argument 'x' is missing.\n  def _matvec(self, x: RealArray) -&gt; RealArray:\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib/python3.9/site-packages/gemseo/core/derivatives/jacobian_operator.py:306: DocstringInheritanceWarning: in _ComposedOperationOperatorArray._rmatvec: section Args: the docstring for the argument 'x' is missing.\n  def _rmatvec(self, x: RealArray) -&gt; RealArray:\n</code></pre> <p>From this learning dataset, we train an SMTRegressor based on the SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>surrogate_model = SMTRegressor(training_data, \"RBF\", d0=2)\nsurrogate_model.learn()\n</code></pre> <p>Finally, we assess its quality:</p> <pre><code>test_data = sample_disciplines([discipline], input_space, \"y\", 1000, \"OT_MONTE_CARLO\")\nr2 = R2Measure(surrogate_model)\nf\"Learning R2: {r2.compute_learning_measure()[0]}; test R2: {r2.compute_test_measure(test_data)[0]}\"\n</code></pre> <p>Out:</p> <pre><code>'Learning R2: 1.0; test R2: 0.9617160347610478'\n</code></pre> <p>see how good it is with its R2 close to 1 on the test dataset, and plot its output over a 20x20 grid:</p> <pre><code>input_data = compute_doe(input_space, \"fullfact\", 400)\noutput_data = surrogate_model.predict(input_data)\npredictions = IODataset()\npredictions.add_input_group(input_data, variable_names=[\"x1\", \"x2\"])\npredictions.add_output_group(output_data, variable_names=[\"y\"])\n\nplot = ZvsXY(predictions, \"x1\", \"x2\", \"y\", other_datasets=(training_data,))\nplot.color = \"white\"\nplot.colormap = \"viridis\"\nplot.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Total running time of the script: ( 0 minutes  1.769 seconds)</p> <p> Download Python source code: plot_smt.py</p> <p> Download Jupyter notebook: plot_smt.ipynb</p> <p>Gallery generated by mkdocs-gallery</p> <ol> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_mlearning<ul> <li>active_learning<ul> <li>acquisition_criteria<ul> <li>base_acquisition_criterion</li> <li>base_acquisition_criterion_family</li> <li>base_factory</li> <li>exploration<ul> <li>base_exploration</li> <li>distance</li> <li>exploration</li> <li>factory</li> <li>standard_deviation</li> <li>variance</li> </ul> </li> <li>level_set<ul> <li>base_ei_ef</li> <li>base_level_set</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>level_set</li> <li>u</li> </ul> </li> <li>maximum<ul> <li>base_maximum</li> <li>ei</li> <li>factory</li> <li>maximum</li> <li>output</li> <li>ucb</li> </ul> </li> <li>minimum<ul> <li>base_minimum</li> <li>ei</li> <li>factory</li> <li>lcb</li> <li>minimum</li> <li>output</li> </ul> </li> <li>quantile<ul> <li>base_quantile</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>quantile</li> <li>u</li> </ul> </li> </ul> </li> <li>active_learning_algo</li> <li>distributions<ul> <li>base_regressor_distribution</li> <li>kriging_distribution</li> <li>regressor_distribution</li> </ul> </li> <li>visualization<ul> <li>acquisition_view</li> <li>qoi_history_view</li> </ul> </li> </ul> </li> <li>algos<ul> <li>opt<ul> <li>core<ul> <li>surrogate_based_optimizer</li> </ul> </li> <li>surrogate_based_optimization</li> </ul> </li> </ul> </li> <li>problems<ul> <li>branin<ul> <li>branin_discipline</li> <li>branin_function</li> <li>branin_problem</li> <li>branin_space</li> <li>functions</li> </ul> </li> <li>rosenbrock<ul> <li>functions</li> <li>rosenbrock_discipline</li> <li>rosenbrock_function</li> <li>rosenbrock_problem</li> <li>rosenbrock_space</li> </ul> </li> </ul> </li> <li>regression<ul> <li>smt_regressor</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/gemseo_mlearning/","title":"API documentation","text":""},{"location":"reference/gemseo_mlearning/#gemseo_mlearning","title":"gemseo_mlearning","text":"<p>A GEMSEO extension for advanced machine learning.</p> <p>GEMSEO includes the main machine learning capabilities.</p>"},{"location":"reference/gemseo_mlearning/active_learning/","title":"Active learning","text":""},{"location":"reference/gemseo_mlearning/active_learning/#gemseo_mlearning.active_learning","title":"active_learning","text":"<p>Active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/","title":"Active learning algo","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo","title":"active_learning_algo","text":"<p>Active learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo","title":"ActiveLearningAlgo","text":"<pre><code>ActiveLearningAlgo(\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10000,\n    **criterion_arguments: Any\n)\n</code></pre> <p>An active learning algorithm using a regressor and acquisition criteria.</p> <p>Parameters:</p> <ul> <li> <code>criterion_family_name</code>               (<code>str</code>)           \u2013            <p>The name of a family of acquisition criteria, e.g. <code>\"Minimum\"</code>, <code>\"Maximum\"</code>, <code>\"LevelSet\"</code>, <code>\"Quantile\"</code> or <code>\"Exploration\"</code>.</p> </li> <li> <code>input_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The input space on which to look for the new learning point.</p> </li> <li> <code>regressor</code>               (<code>BaseRegressor | BaseRegressorDistribution</code>)           \u2013            <p>Either a regressor or a regressor distribution.</p> </li> <li> <code>criterion_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the acquisition criterion. If empty, use the default criterion of the family <code>criterion_family_name</code>.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criteria in parallel.</p> </li> <li> <code>**criterion_arguments</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The parameters of the acquisition criterion.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>When the output dimension is greater than 1.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def __init__(\n    self,\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10000,\n    **criterion_arguments: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        criterion_family_name: The name of a family of acquisition criteria,\n            *e.g.* `\"Minimum\"`, `\"Maximum\"`, `\"LevelSet\"`, `\"Quantile\"`\n            or `\"Exploration\"`.\n        input_space: The input space on which to look for the new learning point.\n        regressor: Either a regressor or a regressor distribution.\n        criterion_name: The name of the acquisition criterion.\n            If empty,\n            use the default criterion of the family `criterion_family_name`.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criteria in parallel.\n        **criterion_arguments: The parameters of the acquisition criterion.\n\n    Raises:\n        NotImplementedError: When the output dimension is greater than 1.\n    \"\"\"  # noqa: D205 D212 D415\n    # Create the regressor distribution.\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        distribution = KrigingDistribution(regressor)\n        distribution.learn()\n    elif isinstance(regressor, BaseRegressor):\n        distribution = RegressorDistribution(regressor)\n        distribution.learn()\n    else:\n        distribution = regressor\n\n    if distribution.output_dimension &gt; 1:\n        msg = \"ActiveLearningAlgo works only with scalar output.\"\n        raise NotImplementedError(msg)\n\n    # Create the acquisition problem.\n    family_factory = AcquisitionCriterionFamilyFactory()\n    self.__criterion_family_name = criterion_family_name\n    criterion_family = family_factory.get_class(criterion_family_name)\n    criterion_factory = criterion_family.ACQUISITION_CRITERION_FACTORY()\n    self.__acquisition_criterion = criterion_factory.create(\n        criterion_name,\n        distribution,\n        batch_size=batch_size,\n        mc_size=mc_size,\n        **criterion_arguments,\n    )\n    # Create the optimization space\n    # that is different from the input space\n    # when acquiring points in parallel.\n    optimization_space = DesignSpace()\n    lower_bound = tile(input_space.get_lower_bounds(), batch_size)\n    upper_bound = tile(input_space.get_upper_bounds(), batch_size)\n    input_space.initialize_missing_current_values()\n    value = tile(input_space.get_current_value(), batch_size)\n    optimization_space.add_variable(\n        \"x\",\n        size=int(len(lower_bound)),\n        lower_bound=lower_bound,\n        upper_bound=upper_bound,\n        value=value,\n    )\n    problem = self.__acquisition_problem = OptimizationProblem(optimization_space)\n    problem.objective = self.__acquisition_criterion\n    if not problem.objective.has_jac:\n        problem.differentiation_method = (\n            OptimizationProblem.DifferentiationMethod.FINITE_DIFFERENCES\n        )\n    if problem.objective.MAXIMIZE:\n        problem.minimize_objective = False\n    # Initialize acquisition algorithm.\n    optimization_factory = OptimizationLibraryFactory()\n    self.__acquisition_algo = optimization_factory.create(self.default_algo_name)\n    self.__acquisition_algo_options = self.default_opt_options\n\n    # Miscellaneous.\n    self.__database = Database()\n    self.__n_initial_samples = len(distribution.algo.learning_set)\n    self.__distribution = distribution\n    self.__input_space = input_space\n    self.__batch_size = batch_size\n\n    # Create the acquisition view.\n    if distribution.algo.input_dimension == 2 and batch_size == 1:\n        self.__acquisition_view = AcquisitionView(self)\n    else:\n        self.__acquisition_view = None\n\n    self.__n_evaluations_history = []\n    self.__qoi_history = []\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion","title":"acquisition_criterion  <code>property</code>","text":"<pre><code>acquisition_criterion: BaseAcquisitionCriterion\n</code></pre> <p>The acquisition criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion_family_name","title":"acquisition_criterion_family_name  <code>property</code>","text":"<pre><code>acquisition_criterion_family_name: str\n</code></pre> <p>The name of the acquisition criterion family.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.batch_size","title":"batch_size  <code>property</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>The number of points to be acquired in parallel.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_algo_name","title":"default_algo_name  <code>class-attribute</code>","text":"<pre><code>default_algo_name: str = 'NLOPT_COBYLA'\n</code></pre> <p>The name of the default algorithm to find the new training point(s).</p> <p>Typically a DoE or an optimizer.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_doe_options","title":"default_doe_options  <code>class-attribute</code>","text":"<pre><code>default_doe_options: dict[str, Any] = {'n_samples': 100}\n</code></pre> <p>The names and values of the default DoE options.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_opt_options","title":"default_opt_options  <code>class-attribute</code>","text":"<pre><code>default_opt_options: dict[str, Any] = {'max_iter': 100}\n</code></pre> <p>The names and values of the default optimization options.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.input_space","title":"input_space  <code>property</code>","text":"<pre><code>input_space: DesignSpace\n</code></pre> <p>The input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.n_initial_samples","title":"n_initial_samples  <code>property</code>","text":"<pre><code>n_initial_samples: int\n</code></pre> <p>The number of initial samples.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: float | None\n</code></pre> <p>The quantity of interest (QOI).</p> <p>When there is no quantity of interest associated with the acquisition criterion, this attribute is <code>None</code>.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi_history","title":"qoi_history  <code>property</code>","text":"<pre><code>qoi_history: tuple[list[int, ...], list[float, ...]]\n</code></pre> <p>The history of the quantity of interest (QOI) when it exists.</p> <p>The second term represents this history while the first one represents the history of the number of model evaluations corresponding to these QOI estimations.</p> <p>When there is no quantity of interest associated with the acquisition criterion, these lists are empty.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor","title":"regressor  <code>property</code>","text":"<pre><code>regressor: BaseRegressor\n</code></pre> <p>The regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor_distribution","title":"regressor_distribution  <code>property</code>","text":"<pre><code>regressor_distribution: BaseRegressorDistribution\n</code></pre> <p>The regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquire_new_points","title":"acquire_new_points","text":"<pre><code>acquire_new_points(\n    discipline: MDODiscipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]\n</code></pre> <p>Update the machine learning algorithm by learning new samples.</p> <p>This method acquires new learning input-output samples and trains the machine learning algorithm with the resulting enriched learning set. The effective number of points will be the largest integer multiple of batch_size and less than or equal to n_samples.</p> <p>Parameters:</p> <ul> <li> <code>discipline</code>               (<code>MDODiscipline</code>)           \u2013            <p>The discipline computing the reference output data from the input data provided by the acquisition process.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of samples to update the machine learning algorithm. It should be a multiple of batch_size.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display intermediate results Only when the input space dimension is 2.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the plots of the intermediate results. If empty, do not save them. Only when the input space dimension is 2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Database, OptimizationProblem]</code>           \u2013            <p>The concatenation of the optimization histories related to the different points and the last acquisition problem.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the input space dimension is not 2.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def acquire_new_points(\n    self,\n    discipline: MDODiscipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]:\n    \"\"\"Update the machine learning algorithm by learning new samples.\n\n    This method acquires new learning input-output samples\n    and trains the machine learning algorithm\n    with the resulting enriched learning set.\n    The effective number of points will be the largest integer multiple\n    of batch_size and less than or equal to n_samples.\n\n    Args:\n        discipline: The discipline computing the reference output data\n            from the input data provided by the acquisition process.\n        n_samples: The number of samples to update the machine learning algorithm.\n            It should be a multiple of batch_size.\n        show: Whether to display intermediate results\n            Only when the input space dimension is 2.\n        file_path: The file path to save the plots of the intermediate results.\n            If empty, do not save them.\n            Only when the input space dimension is 2.\n\n    Returns:\n        The concatenation of the optimization histories\n        related to the different points\n        and the last acquisition problem.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    plot = show or file_path\n    if plot:\n        self.__check_acquisition_view()\n\n    self.__n_evaluations_history.append(self.__n_initial_samples)\n    self.__qoi_history.append(self.__acquisition_criterion.qoi)\n    total_n_samples = self.__n_initial_samples\n    n_batches = int(n_samples / self.__batch_size)\n    LOGGER.info(\"Acquiring %s points in batches of %s\", n_samples, self.batch_size)\n    with OneLineLogging(TQDM_LOGGER):\n        for batch_id in CustomTqdmProgressBar(range(1, n_batches + 1)):\n            array_input_data = self.find_next_point()\n            if plot:\n                self.__acquisition_view.draw(\n                    discipline=discipline,\n                    new_point=array_input_data,\n                    show=show,\n                    file_path=file_path,\n                )\n\n            for inputs, outputs in self.__acquisition_problem.database.items():\n                self.__database.store(\n                    array([batch_id, *inputs.unwrap().tolist()]), outputs\n                )\n\n            for points in range(self.__batch_size):\n                input_data = self.__input_space.convert_array_to_dict(\n                    array_input_data[points, :]\n                )\n\n                discipline.execute(input_data)\n\n                extra_learning_set = IODataset()\n                distribution = self.__distribution\n                variable_names_to_n_components = distribution.algo.sizes\n                new_points = hstack(list(input_data.values()))[newaxis]\n                extra_learning_set.add_group(\n                    group_name=IODataset.INPUT_GROUP,\n                    data=new_points,\n                    variable_names=distribution.input_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                output_names = distribution.output_names\n                output_data = discipline.get_output_data()\n                extra_learning_set.add_group(\n                    group_name=IODataset.OUTPUT_GROUP,\n                    data=hstack(list(output_data.values()))[newaxis],\n                    variable_names=output_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                augmented_learning_set = concat(\n                    [distribution.algo.learning_set, extra_learning_set],\n                    ignore_index=True,\n                )\n\n                self.__distribution.change_learning_set(augmented_learning_set)\n\n            self.update_problem()\n            self.__qoi_history.append(self.__acquisition_criterion.qoi)\n            total_n_samples += self.__batch_size\n            self.__n_evaluations_history.append(total_n_samples)\n\n    return self.__database, self.__acquisition_problem\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.find_next_point","title":"find_next_point","text":"<pre><code>find_next_point(as_dict: bool = False) -&gt; DataType\n</code></pre> <p>Find the next <code>batch_size</code> learning point(s).</p> <p>Parameters:</p> <ul> <li> <code>as_dict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the input data split by input names. Otherwise, return a unique array. In both cases, the arrays will be shaped as <code>(batch_size, input_dimension)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The next <code>batch_size</code> learning point(s).</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def find_next_point(\n    self,\n    as_dict: bool = False,\n) -&gt; DataType:\n    \"\"\"Find the next `batch_size` learning point(s).\n\n    Args:\n        as_dict: Whether to return the input data split by input names.\n            Otherwise, return a unique array.\n            In both cases,\n            the arrays will be shaped as ``(batch_size, input_dimension)``.\n\n    Returns:\n        The next `batch_size` learning point(s).\n    \"\"\"\n    with LoggingContext(logging.getLogger(\"gemseo\")):\n        input_data = self.__acquisition_algo.execute(\n            self.__acquisition_problem, **self.__acquisition_algo_options\n        ).x_opt\n        input_data = input_data.reshape(self.__batch_size, -1)\n    if as_dict:\n        return self.__acquisition_problem.design_space.convert_array_to_dict(\n            input_data\n        )\n\n    return input_data\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_acquisition_view","title":"plot_acquisition_view","text":"<pre><code>plot_acquisition_view(\n    discipline: MDODiscipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>discipline</code>               (<code>MDODiscipline | None</code>, default:                   <code>None</code> )           \u2013            <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> </li> <li> <code>n_test</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>The number of points per dimension.</p> </li> <li> <code>filled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The acquisition view.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When the input space dimension is not 2.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_acquisition_view(\n    self,\n    discipline: MDODiscipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    self.__check_acquisition_view()\n    return self.__acquisition_view.draw(\n        discipline=discipline,\n        filled=filled,\n        n_test=n_test,\n        show=show,\n        file_path=file_path,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_qoi_history","title":"plot_qoi_history","text":"<pre><code>plot_qoi_history(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"Quantity of interest\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Plot the history of the quantity of interest.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> <li> <code>label</code>               (<code>str</code>, default:                   <code>'Quantity of interest'</code> )           \u2013            <p>The label for the QOI.</p> </li> <li> <code>add_markers</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add markers.</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options to create the Lines object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Lines</code>           \u2013            <p>The history of the quantity of interest.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_qoi_history(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"Quantity of interest\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Plot the history of the quantity of interest.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        The history of the quantity of interest.\n    \"\"\"\n    return QOIHistoryView(self).draw(\n        show=show,\n        file_path=file_path,\n        label=label,\n        add_markers=add_markers,\n        **options,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.set_acquisition_algorithm","title":"set_acquisition_algorithm","text":"<pre><code>set_acquisition_algorithm(\n    algo_name: str, **options: Any\n) -&gt; None\n</code></pre> <p>Set sampling or optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of a DOE or optimization algorithm to find the learning point(s).</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The values of some algorithm options; use the default values for the other ones.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def set_acquisition_algorithm(self, algo_name: str, **options: Any) -&gt; None:\n    \"\"\"Set sampling or optimization algorithm.\n\n    Args:\n        algo_name: The name of a DOE or optimization algorithm\n            to find the learning point(s).\n        **options: The values of some algorithm options;\n            use the default values for the other ones.\n    \"\"\"\n    factory = DOELibraryFactory()\n    if factory.is_available(algo_name):\n        self.__acquisition_algo_options = self.default_doe_options.copy()\n    else:\n        factory = OptimizationLibraryFactory()\n        self.__acquisition_algo_options = self.default_opt_options.copy()\n\n    self.__acquisition_algo_options.update(options)\n    self.__acquisition_algo = factory.create(algo_name)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.update_problem","title":"update_problem","text":"<pre><code>update_problem() -&gt; None\n</code></pre> <p>Update the acquisition problem.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def update_problem(self) -&gt; None:\n    \"\"\"Update the acquisition problem.\"\"\"\n    self.__acquisition_problem.reset(preprocessing=False)\n    self.__acquisition_criterion.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/","title":"Acquisition criteria","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/#gemseo_mlearning.active_learning.acquisition_criteria","title":"acquisition_criteria","text":"<p>Acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/","title":"Base acquisition criterion","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion","title":"base_acquisition_criterion","text":"<p>Base class for acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion","title":"BaseAcquisitionCriterion","text":"<pre><code>BaseAcquisitionCriterion(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>Base class for acquisition criteria.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/","title":"Base acquisition criterion family","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family","title":"base_acquisition_criterion_family","text":"<p>Base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.AcquisitionCriterionFamilyFactory","title":"AcquisitionCriterionFamilyFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.BaseAcquisitionCriterionFamily","title":"BaseAcquisitionCriterionFamily","text":"<p>The base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/","title":"Base factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory","title":"base_factory","text":"<p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory","title":"BaseAcquisitionCriterionFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFamilyFactory","title":"BaseAcquisitionCriterionFamilyFactory","text":"<p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration","title":"exploration","text":"<p>Acquisition criteria for exploration.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/","title":"Base exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration","title":"base_exploration","text":"<p>Base class for the acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration","title":"BaseExploration","text":"<pre><code>BaseExploration(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class the for acquisition criteria to explore the input space.</p> <p>Optimize it to make the error of the surrogate model tends towards zero when the number of acquired points tends to infinity.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/","title":"Distance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance","title":"distance","text":"<p>Distance to the learning set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance","title":"Distance","text":"<pre><code>Distance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Distance to the learning set.</p> <p>This acquisition criterion computes the minimum distance between a new point and the point of the learning dataset, scaled by the minimum distance between two distinct learning points.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration","title":"exploration","text":"<p>Family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration.Exploration","title":"Exploration","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory","title":"factory","text":"<p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory","title":"ExplorationFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/","title":"Standard deviation","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation","title":"standard_deviation","text":"<p>Output standard deviation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation","title":"StandardDeviation","text":"<pre><code>StandardDeviation(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output standard deviation.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma[x] = \\sqrt{\\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/","title":"Variance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance","title":"variance","text":"<p>Output variance.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance","title":"Variance","text":"<pre><code>Variance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output variance.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma[x] = \\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set","title":"level_set","text":"<p>Acquisition criteria for level set estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/","title":"Base ei ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef","title":"base_ei_ef","text":"<p>Base class for EI and EF criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF","title":"BaseEIEF","text":"<pre><code>BaseEIEF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The base class for EI and EF criteria to approximate a level set.</p> <p>EI and EF stands for expected improvement and expected feasibility respectively. More information in the subclasses EI and EF.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/","title":"Base level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set","title":"base_level_set","text":"<p>Base class for acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet","title":"BaseLevelSet","text":"<pre><code>BaseLevelSet(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a level set estimation.</p> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float</code>)           \u2013            <p>The model output value characterizing the level set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> <p>$$ EF[x] =  \\mathbb{E}\\left[\\max(\\kappa\\mathbb{S}[Y(x)] - |y - Y(x)|,0)\\right] $$ where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian regressor, it has an analytic expression:</p> \\[ EF[x] = \\mathbb{S}[Y(x)]\\times ( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) ) \\] <p>where \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\), \\(y\\) is the model output value characterizing the level set and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to \\(\\(EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max(\\kappa\\mathbb{S}[Y(x_1)] - |y - Y(x_1)|,0),\\dots, \\max(\\kappa\\mathbb{S}[Y(x_q)]  -|y - Y(x_q)|,0) \\right)\\right]\\)\\)</p> <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> <p>$$ EI[x] =  \\mathbb{E}\\left[\\max((\\kappa\\mathbb{S}[Y(x)])^2 - (y - Y(x))^2,0)\\right] $$ where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian regressor, it has an analytic expression:</p> \\[ EI[x] = \\mathbb{V}[Y(x)]\\times ( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) ) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\), \\(y\\) is the model output value characterizing the level set and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max((\\kappa\\mathbb{S}[Y(x_1)])^2 - (y - Y(x_1))^2,0),\\dots, \\max((\\kappa\\mathbb{S}[Y(x_q)])^2 - (y - Y(x_q))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory","title":"LevelSetFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set","title":"level_set","text":"<p>Family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set.LevelSet","title":"LevelSet","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\mathbb{E}\\left[\\left(\\frac{y-Y(x)}{\\mathbb{S}[Y(x)]} \\right)^2\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\). It has an analytic expression:</p> \\[U[x] = \\left(\\frac{y-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\right)^2\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y\\) is the model output value characterizing the level set. For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min\\left( \\left(\\frac{y-Y(x_1)}{\\mathbb{S}[Y(x_1)]}\\right)^2,\\dots, \\left(\\frac{y-Y(x_q)}{\\mathbb{S}[Y(x_q)]} \\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float</code>)           \u2013            <p>The model output value characterizing the level set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>output_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum","title":"maximum","text":"<p>Acquisition criteria for maximum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/","title":"Base maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum","title":"base_maximum","text":"<p>Base class for acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum","title":"BaseMaximum","text":"<pre><code>BaseMaximum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a maximum.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMaximum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-y_{\\text{max}},0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{max}}\\) is the maximum output values in the learning set.</p> <p>In the case of a Gaussian regressor, it has an analytic expression:</p> \\[ EI[x] = (\\mathbb{E}[Y(x)] - y_{\\text{max}})\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{\\mathbb{E}[Y(x)] - y_{\\text{max}}}{\\mathbb{S}[Y(x)]}\\) and \\(y_{\\text{max}}\\) is the maximum output value in the learning set.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max(Y(x_1)-y_{\\text{max}},0),\\dots, \\max(Y(x_q)-y_{\\text{max}},0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory","title":"MaximumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum","title":"maximum","text":"<p>Family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum.Maximum","title":"Maximum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/","title":"Output","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output","title":"output","text":"<p>Output-based criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output","title":"Output","text":"<pre><code>Output(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Output</code>, <code>BaseMaximum</code></p> <p>Output-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/output/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.output.Output.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/","title":"Ucb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb","title":"ucb","text":"<p>Upper confidence bound (UCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB","title":"UCB","text":"<pre><code>UCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMaximum</code></p> <p>The upper confidence bound (UCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] + \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A factor associated with the standard deviation to increase the mean value.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_confidence_bound.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        kappa: A factor associated with the standard deviation\n            to increase the mean value.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self.__kappa = kappa\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum","title":"minimum","text":"<p>Acquisition criteria for minimum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/","title":"Base minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum","title":"base_minimum","text":"<p>Base class for acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum","title":"BaseMinimum","text":"<pre><code>BaseMinimum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a minimum.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMinimum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(y_{\\text{min}}-Y(x),0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{min}}\\) is the minimum output value in the learning set.</p> <p>In the case of a Gaussian regressor, it has an analytic expression:</p> \\[ EI[x] = (y_{\\text{min}}-\\mathbb{E}[Y(x)])\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\text{min}}-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max(y_{\\text{min}}-Y(x_1),0),\\dots, \\max(y_{\\text{min}}-Y(x_q),0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory","title":"MinimumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/","title":"Lcb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb","title":"lcb","text":"<p>Lower confidence bound (LCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB","title":"LCB","text":"<pre><code>LCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMinimum</code></p> <p>The lower confidence bound (LCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] - \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>kappa</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>A factor associated with the standard deviation to increase the mean value.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:  # noqa: D102\n    super().__init__(\n        regressor_distribution,\n        kappa=-abs(kappa),\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum","title":"minimum","text":"<p>Family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum.Minimum","title":"Minimum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/","title":"Output","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output","title":"output","text":"<p>The output-based criterion to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output","title":"Output","text":"<pre><code>Output(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Output</code>, <code>BaseMinimum</code></p> <p>Output-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>Parameters:</p> <ul> <li> <code>regressor_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the regressor.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> </li> <li> <code>mc_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The sample size to estimate the acquisition criterion in parallel.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/output/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.output.Output.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile","title":"quantile","text":"<p>Acquisition criteria for quantile estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/","title":"Base quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile","title":"base_quantile","text":"<p>Base class for acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile","title":"BaseQuantile","text":"<pre><code>BaseQuantile(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a quantile.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    self.__uncertain_space = uncertain_space.__class__()\n    self.__uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = self.__uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> \\[ EF[x] = \\mathbb{S}[Y(x)]\\times ( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) ) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\), \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to \\(\\(EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max(\\kappa\\mathbb{S}[Y(x_1)] - |y_{\\alpha} - Y(x_1)|,0),\\dots, \\max(\\kappa\\mathbb{S}[Y(x_q)] - |y_{\\alpha} - Y(x_q)|,0) \\right)\\right]\\)\\)</p> <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    self.__uncertain_space = uncertain_space.__class__()\n    self.__uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = self.__uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[ EI[x] = \\mathbb{V}[Y(x)]\\times ( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) ) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\), \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max\\left( \\max((\\kappa\\mathbb{S}[Y(x_1)])^2 - (y_{\\alpha} - Y(x_1))^2,0),\\dots, \\max((\\kappa\\mathbb{S}[Y(x_q)])^2 - (y_{\\alpha} - Y(x_q))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    self.__uncertain_space = uncertain_space.__class__()\n    self.__uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = self.__uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate quantiles.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory","title":"QuantileFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate level sets.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile","title":"quantile","text":"<p>Family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile.Quantile","title":"Quantile","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\frac{|y_{\\alpha}-\\mathbb{E}[Y(x)]|}{\\mathbb{S}[Y(x)])}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output. For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min\\left( \\left(\\frac{y_{\\alpha}-Y(x_1)}{\\mathbb{S}[Y(x_1)]}\\right)^2,\\dots, \\left(\\frac{y_{\\alpha}-Y(x_q)}{\\mathbb{S}[Y(x_q)]} \\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    self.__uncertain_space = uncertain_space.__class__()\n    self.__uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = self.__uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/","title":"Distributions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions","title":"distributions","text":"<p>Regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions.get_regressor_distribution","title":"get_regressor_distribution","text":"<pre><code>get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution\n</code></pre> <p>Return the distribution of a regressor.</p> <p>Parameters:</p> <ul> <li> <code>regressor</code>               (<code>BaseRegressor</code>)           \u2013            <p>The regression algorithm.</p> </li> <li> <code>use_bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bootstrap for resampling. If <code>False</code>, use cross-validation.</p> </li> <li> <code>use_loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use leave-one-out resampling when <code>use_bootstrap</code> is <code>False</code>. If <code>False</code>, use parameterized cross-validation.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseRegressorDistribution</code>           \u2013            <p>The distribution of the regression algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/__init__.py</code> <pre><code>def get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution:\n    \"\"\"Return the distribution of a regressor.\n\n    Args:\n        regressor: The regression algorithm.\n        use_bootstrap: Whether to use bootstrap for resampling.\n            If `False`, use cross-validation.\n        use_loo: Whether to use leave-one-out resampling when\n            `use_bootstrap` is `False`.\n            If `False`, use parameterized cross-validation.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n\n    Returns:\n        The distribution of the regression algorithm.\n    \"\"\"\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        return KrigingDistribution(regressor)\n\n    return RegressorDistribution(regressor, use_bootstrap, use_loo, size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/","title":"Base regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution","title":"base_regressor_distribution","text":"<p>Base class for regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution","title":"BaseRegressorDistribution","text":"<pre><code>BaseRegressorDistribution(regressor: BaseRegressor)\n</code></pre> <p>The distribution of a regressor.</p> <p>Parameters:</p> <ul> <li> <code>regressor</code>               (<code>BaseRegressor</code>)           \u2013            <p>A regression model.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def __init__(self, regressor: BaseRegressor) -&gt; None:\n    \"\"\"\n    Args:\n        regressor: A regression model.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo = regressor\n    self._samples = []\n    self._transform_input_group = self.algo._transform_input_group\n    self._transform_output_group = self.algo._transform_output_group\n    self._input_variables_to_transform = self.algo._input_variables_to_transform\n    self._output_variables_to_transform = self.algo._output_variables_to_transform\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval  <code>abstractmethod</code>","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_confidence_interval(\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    \"\"\"Compute the lower bounds and upper bounds of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n        level: A quantile level.\n\n    Returns:\n        The lower and upper bound values of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_mean","title":"compute_mean  <code>abstractmethod</code>","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_mean(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output mean of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output mean of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_samples","title":"compute_samples  <code>abstractmethod</code>","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_samples(\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    \"\"\"Generate samples from the random process.\n\n    Args:\n        input_data: The $N$ input points of dimension $d$\n            at which to observe the random process;\n            shaped as `(N, d)`.\n        n_samples: The number of samples `M`.\n\n    Returns:\n        The output samples per output dimension shaped as `(N, M, p)`\n        where `p` is the output dimension.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_variance","title":"compute_variance  <code>abstractmethod</code>","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_variance(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output variance of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output variance of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/","title":"Kriging distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution","title":"kriging_distribution","text":"<p>Kriging-like regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution","title":"KrigingDistribution","text":"<pre><code>KrigingDistribution(algo: BaseRandomProcessRegressor)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Kriging-like regressor distribution.</p> <p>Parameters:</p> <ul> <li> <code>regressor</code>               (<code>BaseRegressor</code>)           \u2013            <p>A regression model.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def __init__(  # noqa: D107\n    self, algo: BaseRandomProcessRegressor\n) -&gt; None:\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRandomProcessRegressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    mean = self.compute_mean(input_data)\n    std = self.compute_standard_deviation(input_data)\n    quantile = norm.ppf(level)\n    if isinstance(mean, Mapping):\n        lower = {name: mean[name] - quantile * std[name] for name in mean}\n        upper = {name: mean[name] + quantile * std[name] for name in mean}\n    else:\n        lower = mean - quantile * std\n        upper = mean + quantile * std\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.algo.compute_samples(input_data, n_samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict_std(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.compute_standard_deviation(input_data) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/","title":"Regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution","title":"regressor_distribution","text":"<p>Universal regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution","title":"RegressorDistribution","text":"<pre><code>RegressorDistribution(\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Universal regressor distribution.</p> <p>Parameters:</p> <ul> <li> <code>bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>The resampling method. If <code>True</code>, use bootstrap resampling. Otherwise, use cross-validation resampling.</p> </li> <li> <code>loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The leave-One-Out sub-method, when bootstrap is <code>False</code>. If <code>False</code>, use parameterized cross-validation, Otherwise use leave-one-out.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def __init__(\n    self,\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        bootstrap: The resampling method.\n            If `True`, use bootstrap resampling.\n            Otherwise, use cross-validation resampling.\n        loo: The leave-One-Out sub-method, when bootstrap is `False`.\n            If `False`, use parameterized cross-validation,\n            Otherwise use leave-one-out.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n    \"\"\"  # noqa: D205 D212 D415\n    if bootstrap:\n        self.method = self.BOOTSTRAP\n        self.size = size or self.N_BOOTSTRAP\n    else:\n        if loo:\n            self.method = self.LOO\n            self.size = len(algo.learning_set)\n        else:\n            self.method = self.CROSS_VALIDATION\n            self.size = size or self.N_FOLDS\n    self.algos = [\n        algo.__class__(\n            data=algo.learning_set, transformer=algo.transformer, **algo.parameters\n        )\n        for _ in range(self.size)\n    ]\n    self.weights = []\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP","title":"N_BOOTSTRAP  <code>class-attribute</code>","text":"<pre><code>N_BOOTSTRAP: int = 100\n</code></pre> <p>The default number of replicates for the bootstrap method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS","title":"N_FOLDS  <code>class-attribute</code>","text":"<pre><code>N_FOLDS: int = 5\n</code></pre> <p>The default number of folds for the cross-validation method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>The resampling method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre> <p>The size of the resampling set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: list[Callable[[NumberArray], float]] = []\n</code></pre> <p>The weight functions related to the sub-algorithms.</p> <p>A weight function computes a weight from an input data array.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:  # noqa: D102\n    for algo in self.algos:\n        algo.learning_set = learning_set\n    super().change_learning_set(learning_set)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    level = (1.0 - level) / 2.0\n    predictions = self.predict_members(input_data)\n    if isinstance(predictions, Mapping):\n        lower = {\n            name: quantile(value, level, axis=0)\n            for name, value in predictions.items()\n        }\n        upper = {\n            name: quantile(value, 1 - level, axis=0)\n            for name, value in predictions.items()\n        }\n    else:\n        lower = quantile(predictions, level, axis=0)\n        upper = quantile(predictions, 1 - level, axis=0)\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output mean of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    return self.__average(weights, predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>The number of samples <code>M</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.predict_members(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output standard deviation  of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output variance of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    term1 = self.__average(weights, predictions**2)\n    term2 = self.__average(weights, predictions) ** 2\n    return term1 - term2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.evaluate_weights","title":"evaluate_weights","text":"<pre><code>evaluate_weights(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Evaluate weights.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>NumberArray</code>)           \u2013            <p>The input data with shape (size, n_input_data)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NumberArray</code>           \u2013            <p>The weights.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def evaluate_weights(self, input_data: NumberArray) -&gt; NumberArray:\n    \"\"\"Evaluate weights.\n\n    Args:\n        input_data: The input data with shape (size, n_input_data)\n\n    Returns:\n        The weights.\n    \"\"\"\n    weights = array([func(input_data) for func in self.weights])\n    return weights / npsum(weights, 0)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def learn(  # noqa: D102\n    self,\n    samples: list[int] | None = None,\n) -&gt; None:\n    self.weights = []\n    super().learn(samples)\n    if self.method in [self.CROSS_VALIDATION, self.LOO]:\n        n_folds = self.size\n        folds = array_split(self._samples, n_folds)\n    for index, algo in enumerate(self.algos):\n        if self.method == self.BOOTSTRAP:\n            new_samples = unique(\n                default_rng(1).choice(self._samples, len(self._samples))\n            )\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n        else:\n            fold = folds[index]\n            new_samples = npdelete(self._samples, fold)\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n\n        algo.learn(new_samples.tolist())\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output value(s) of the regressor.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict_members","title":"predict_members","text":"<pre><code>predict_members(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output value with the different machine learning algorithms.</p> <p>After prediction, the method stacks the results.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data, specified as either as a NumPy array or as dictionary of NumPy arrays indexed by inputs names. The NumPy array can be either a <code>(d,)</code> array representing a sample in dimension <code>d</code>, or a <code>(M, d)</code> array representing <code>M</code> samples in dimension <code>d</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output data (dimension <code>p</code>) of <code>N</code> machine learning algorithms.     If <code>input_data.shape == (d,)</code>, then <code>output_data.shape == (N, p)</code>.     If <code>input_data.shape == (M,d)</code>, then <code>output_data.shape == (N,M,p)</code>.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def predict_members(self, input_data: DataType) -&gt; DataType:\n    \"\"\"Predict the output value with the different machine learning algorithms.\n\n    After prediction, the method stacks the results.\n\n    Args:\n        input_data: The input data,\n            specified as either as a NumPy array or as dictionary of NumPy arrays\n            indexed by inputs names.\n            The NumPy array can be either a `(d,)` array\n            representing a sample in dimension `d`,\n            or a `(M, d)` array representing `M` samples in dimension `d`.\n\n    Returns:\n        The output data (dimension `p`) of `N` machine learning algorithms.\n            If `input_data.shape == (d,)`, then `output_data.shape == (N, p)`.\n            If `input_data.shape == (M,d)`, then `output_data.shape == (N,M,p)`.\n    \"\"\"\n    predictions = [algo.predict(input_data) for algo in self.algos]\n    if isinstance(input_data, Mapping):\n        return {\n            name: stack([prediction[name] for prediction in predictions])\n            for name in predictions[0]\n        }\n    return stack(predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/","title":"Visualization","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/#gemseo_mlearning.active_learning.visualization","title":"visualization","text":"<p>Visualization tools for active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/","title":"Acquisition view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view","title":"acquisition_view","text":"<p>An acquisition plot.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView","title":"AcquisitionView","text":"<pre><code>AcquisitionView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of points acquired during active learning.</p> <p>This visualization tool only works with a 2-dimensional input space.</p> <p>Parameters:</p> <ul> <li> <code>active_learning_algo</code>               (<code>ActiveLearningAlgo</code>)           \u2013            <p>The active learning algorithm using sequential acquisition.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>When the active learning algorithm uses parallel acquisition.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm\n            using sequential acquisition.\n\n    Raises:\n        NotImplementedError: When the active learning algorithm uses parallel\n            acquisition.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.batch_size &gt; 1:\n        msg = (\n            \"AcquisitionView does not support active learning algorithm using \"\n            \"parallel acquisition.\"\n        )\n        raise NotImplementedError(msg)\n\n    self.__algo = active_learning_algo\n    self.__input_dimension = (\n        active_learning_algo.regressor_distribution.algo.input_dimension\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView.draw","title":"draw","text":"<pre><code>draw(\n    new_point: RealArray | None = None,\n    discipline: MDODiscipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>new_point</code>               (<code>RealArray | None</code>, default:                   <code>None</code> )           \u2013            <p>The new point to be acquired.</p> </li> <li> <code>discipline</code>               (<code>MDODiscipline | None</code>, default:                   <code>None</code> )           \u2013            <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> </li> <li> <code>n_test</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>The number of points per dimension.</p> </li> <li> <code>filled</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The acquisition view.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def draw(\n    self,\n    new_point: RealArray | None = None,\n    discipline: MDODiscipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        new_point: The new point to be acquired.\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n    \"\"\"\n    # Create grid.\n    input_space = self.__algo.input_space\n    lower_bounds = input_space.get_lower_bounds()\n    upper_bounds = input_space.get_upper_bounds()\n    test_x1 = linspace(lower_bounds[0], upper_bounds[0], n_test)\n    test_x2 = linspace(lower_bounds[1], upper_bounds[1], n_test)\n    grid = array(meshgrid(test_x1, test_x2)).T.reshape(-1, 2)\n\n    # Generate data.\n    distribution = self.__algo.regressor_distribution\n    final_dataset = distribution.algo.learning_set\n    #    The learning input samples.\n    points = final_dataset.input_dataset.to_numpy()\n    points_x = points[:, 0]\n    points_y = points[:, 1]\n    #    The predictions, the standard deviations and the criterion values.\n    predictions = distribution.predict(grid).reshape((n_test, n_test)).T\n    std = distribution.compute_standard_deviation(grid).reshape((n_test, n_test)).T\n    acquisition_criterion = self.__algo.acquisition_criterion.original.func\n    criterion_values = acquisition_criterion(grid).reshape((n_test, n_test)).T\n    x_name, y_name = final_dataset.input_names\n    output_name = final_dataset.output_names[0]\n    #    The observations if the discipline is available.\n    observations = None\n    if discipline is not None:\n        observations = zeros((n_test, n_test))\n        for i in range(n_test):\n            for j in range(n_test):\n                xij = array([test_x1[j], test_x2[i]])\n                input_data = {x_name: array([xij[0]]), y_name: array([xij[1]])}\n                observations[i, j] = discipline.execute(input_data)[output_name][0]\n\n    # Create figure and sub-figures.\n    fig, axes = plt.subplots(2, 2)\n    titles = [\n        [\"Discipline\", self.__algo.acquisition_criterion.__class__.__name__],\n        [\"Surrogate\", \"Standard deviation\"],\n    ]\n    data = [[observations, criterion_values], [predictions, std]]\n    cf = []\n    color = \"white\" if filled else \"black\"\n    n_initial_samples = self.__algo.n_initial_samples\n    contour_method = \"contourf\" if filled else \"contour\"\n    for i in range(2):\n        for j in range(2):\n            if (i, j) == (0, 0) and discipline is None:\n                continue\n\n            ax = axes[i, j]\n            cf.append(getattr(ax, contour_method)(test_x1, test_x2, data[i][j]))\n            for x, y in zip(\n                points_x[:n_initial_samples],\n                points_y[:n_initial_samples],\n            ):\n                ax.plot(x, y, \".\", ms=2, color=color)\n\n            for index, (x, y) in enumerate(\n                zip(\n                    points_x[n_initial_samples:],\n                    points_y[n_initial_samples:],\n                )\n            ):\n                ax.plot(x, y, \"+\", color=color)\n                ax.annotate(\n                    str(n_initial_samples + 1 + index),\n                    (0.05 + x, 0.05 + y),\n                    color=color,\n                )\n\n            if new_point is not None:\n                ax.plot(*new_point, \"o\", color=color)\n\n            ax.set_title(titles[i][j])\n\n    if discipline is None:\n        fig.delaxes(axes.flatten()[0])\n        fig.colorbar(cf[0], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[1, 0])\n        fig.colorbar(cf[2], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n    else:\n        fig.colorbar(cf[0], ax=axes[:, 0])\n        axes[0, 0].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        axes[0, 1].set_yticks([])\n        fig.colorbar(cf[3], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n\n    save_show_figure(fig, show, file_path)\n    return fig\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/","title":"Qoi history view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view","title":"qoi_history_view","text":"<p>History view.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView","title":"QOIHistoryView","text":"<pre><code>QOIHistoryView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of the history of the quantity of interest (QOI).</p> <p>Parameters:</p> <ul> <li> <code>active_learning_algo</code>               (<code>ActiveLearningAlgo</code>)           \u2013            <p>The active learning algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When there is no quantity of interest associated with the acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm.\n\n    Raises:\n        ValueError: When there is no quantity of interest\n            associated with the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.acquisition_criterion.qoi is None:\n        msg = (\n            \"There is no quantity of interest \"\n            \"associated with the acquisition criterion \"\n            f\"{active_learning_algo.acquisition_criterion.__class__.__name__}.\"\n        )\n        raise ValueError(msg)\n\n    self.__algo = active_learning_algo\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView.draw","title":"draw","text":"<pre><code>draw(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Draw the QOI history.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the view.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the view. If empty, do not save it.</p> </li> <li> <code>label</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The label for the QOI. If empty, use the name of the acquisition criterion family.</p> </li> <li> <code>add_markers</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add markers.</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options to create the Lines object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Lines</code>           \u2013            <p>A view of the QOI history.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def draw(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Draw the QOI history.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n            If empty, use the name of the acquisition criterion family.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        A view of the QOI history.\n    \"\"\"\n    if not label:\n        label = self.__algo.acquisition_criterion_family_name\n\n    x_label = \"Number of evaluations\"\n    n_evaluations_history, qoi_history = self.__algo.qoi_history\n    qoi_history = [qoi[0] for qoi in qoi_history]\n    dataset = IODataset()\n    dataset.add_variable(x_label, array(n_evaluations_history)[:, newaxis])\n    dataset.add_variable(label, array(qoi_history)[:, newaxis])\n    lines = Lines(\n        dataset,\n        variables=[label],\n        abscissa_variable=x_label,\n        add_markers=add_markers,\n        **options,\n    )\n    lines.marker = \".\"\n    lines.execute(show=show, save=file_path != \"\", file_path=file_path)\n    return lines\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/","title":"Algos","text":""},{"location":"reference/gemseo_mlearning/algos/#gemseo_mlearning.algos","title":"algos","text":"<p>Wrappers for algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/#gemseo_mlearning.algos.opt","title":"opt","text":"<p>Wrappers for optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/","title":"Surrogate based optimization","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization","title":"surrogate_based_optimization","text":"<p>A library for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedAlgorithmDescription","title":"SurrogateBasedAlgorithmDescription  <code>dataclass</code>","text":"<pre><code>SurrogateBasedAlgorithmDescription(\n    library_name: str = \"gemseo-mlearning\",\n)\n</code></pre> <p>               Bases: <code>OptimizationAlgorithmDescription</code></p> <p>The description of a surrogate-based optimization algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedOptimization","title":"SurrogateBasedOptimization","text":"<p>               Bases: <code>BaseOptimizationLibrary</code></p> <p>A wrapper for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/","title":"Core","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/#gemseo_mlearning.algos.opt.core","title":"core","text":"<p>Core functionalities for the optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/","title":"Surrogate based optimizer","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer","title":"surrogate_based_optimizer","text":"<p>A class for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer","title":"SurrogateBasedOptimizer","text":"<pre><code>SurrogateBasedOptimizer(\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_options: Mapping[\n        str, DriverLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: (\n        str | BaseRegressor\n    ) = GaussianProcessRegressor.__name__,\n    regression_options: Mapping[\n        str, MLAlgoParameterType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    acquisition_options: Mapping[\n        str, OptimizationLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n)\n</code></pre> <p>An optimizer based on surrogate models.</p> <p>Parameters:</p> <ul> <li> <code>acquisition_algorithm</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm to optimize the data acquisition criterion. N.B. this algorithm must handle integers if some of the optimization variables are integers.</p> </li> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>doe_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Either the size of the initial DOE or 0 if the size is inferred from doe_options. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_algorithm</code>               (<code>str</code>, default:                   <code>'OT_OPT_LHS'</code> )           \u2013            <p>The name of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_algorithm</code>               (<code>str | BaseRegressor</code>, default:                   <code>__name__</code> )           \u2013            <p>Either the name of the regression algorithm approximating the objective function over the design space or the regression algorithm itself.</p> </li> <li> <code>regression_options</code>               (<code>Mapping[str, MLAlgoParameterType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the regression algorithm. If transformer is missing, use :attr:<code>.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER</code>. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the file to save the regression model. If empty, do not save the regression model.</p> </li> <li> <code>acquisition_options</code>               (<code>Mapping[str, OptimizationLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm to optimize the data acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def __init__(\n    self,\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_options: Mapping[str, DriverLibraryOptionType] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: (str | BaseRegressor) = GaussianProcessRegressor.__name__,\n    regression_options: Mapping[str, MLAlgoParameterType] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    acquisition_options: Mapping[\n        str, OptimizationLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n) -&gt; None:\n    \"\"\"\n    Args:\n        acquisition_algorithm: The name of the algorithm to optimize the data\n            acquisition criterion.\n            N.B. this algorithm must handle integers if some of the optimization\n            variables are integers.\n        problem: The optimization problem.\n        doe_size: Either the size of the initial DOE\n            or 0 if the size is inferred from doe_options.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_algorithm: The name of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_options: The options of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_algorithm: Either the name of the regression algorithm\n            approximating the objective function over the design space\n            or the regression algorithm itself.\n        regression_options: The options of the regression algorithm.\n            If transformer is missing,\n            use :attr:`.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER`.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_file_path: The path to the file to save the regression model.\n            If empty, do not save the regression model.\n        acquisition_options: The options of the algorithm to optimize\n            the data acquisition criterion.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__problem = problem\n    if isinstance(regression_algorithm, BaseRegressor):\n        self.__dataset = regression_algorithm.learning_set\n    else:\n        # Store max_iter as it will be overwritten by DOELibrary\n        max_iter = problem.evaluation_counter.maximum\n        options = dict(doe_options)\n        if doe_size &gt; 0 and \"n_samples\" not in options:\n            options[\"n_samples\"] = doe_size\n\n        # Store the listeners as they will be cleared by DOELibrary.\n        new_iter_listeners, store_listeners = problem.database.clear_listeners()\n        with LoggingContext(logging.getLogger(\"gemseo\")):\n            DOELibraryFactory().execute(problem, doe_algorithm, **options)\n\n        database = self.__problem.database\n        for listener in new_iter_listeners:\n            database.add_new_iter_listener(listener)\n\n        for listener in store_listeners:\n            database.add_store_listener(listener)\n\n        self.__dataset = problem.to_dataset(opt_naming=False)\n        _regression_options = {\"transformer\": BaseRegressor.DEFAULT_TRANSFORMER}\n        _regression_options.update(dict(regression_options))\n        regression_algorithm = RegressorFactory().create(\n            regression_algorithm,\n            data=self.__dataset,\n            **_regression_options,\n        )\n        # Add the first iteration to the current_iter reset by DOELibrary.\n        problem.evaluation_counter.current += 1\n        # And restore max_iter.\n        problem.evaluation_counter.maximum = max_iter\n\n    self.__distribution = get_regressor_distribution(regression_algorithm)\n    self.__active_learning_algo = ActiveLearningAlgo(\n        Minimum.__name__,\n        problem.design_space,\n        self.__distribution,\n    )\n    self.__active_learning_algo.set_acquisition_algorithm(\n        acquisition_algorithm, **acquisition_options\n    )\n    self.__regression_file_path = regression_file_path\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer.execute","title":"execute","text":"<pre><code>execute(number_of_acquisitions: int) -&gt; str\n</code></pre> <p>Execute the surrogate-based optimization.</p> <p>Parameters:</p> <ul> <li> <code>number_of_acquisitions</code>               (<code>int</code>)           \u2013            <p>The number of learning points to be acquired.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The termination message.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def execute(self, number_of_acquisitions: int) -&gt; str:\n    \"\"\"Execute the surrogate-based optimization.\n\n    Args:\n        number_of_acquisitions: The number of learning points to be acquired.\n\n    Returns:\n        The termination message.\n    \"\"\"\n    self.__distribution.learn()\n    message = self.__STOP_BECAUSE_MAX_ACQUISITIONS\n    for _ in range(number_of_acquisitions):\n        input_data = self.__active_learning_algo.find_next_point()[0]\n        if input_data in self.__problem.database:\n            message = self.__STOP_BECAUSE_ALREADY_KNOWN\n            break\n\n        output_data = self.__problem.evaluate_functions(\n            design_vector=input_data, design_vector_is_normalized=False\n        )[0]\n        extra_learning_set = IODataset()\n        distribution = self.__distribution\n        variable_names_to_n_components = distribution.algo.sizes\n        extra_learning_set.add_group(\n            group_name=IODataset.INPUT_GROUP,\n            data=input_data[newaxis],\n            variable_names=distribution.input_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        output_names = distribution.output_names\n        extra_learning_set.add_group(\n            group_name=IODataset.OUTPUT_GROUP,\n            data=hstack([output_data[output_name] for output_name in output_names])[\n                newaxis\n            ],\n            variable_names=output_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        self.__dataset = concat(\n            [distribution.algo.learning_set, extra_learning_set],\n            ignore_index=True,\n        )\n        self.__dataset = self.__dataset.map(lambda x: x.real)\n        distribution.change_learning_set(self.__dataset)\n        self.__active_learning_algo.update_problem()\n\n        if self.__regression_file_path:\n            with Path(self.__regression_file_path).open(\"wb\") as file:\n                pickle.dump(distribution.algo, file)\n\n    return message\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/","title":"Problems","text":""},{"location":"reference/gemseo_mlearning/problems/#gemseo_mlearning.problems","title":"problems","text":"<p>Use cases to benchmark and illustrate active learning algorithms.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/","title":"Branin","text":""},{"location":"reference/gemseo_mlearning/problems/branin/#gemseo_mlearning.problems.branin","title":"branin","text":"<p>The Branin use case to benchmark and illustrate active learning algorithms.</p> <p>The Branin function \\(\\(f(x_1,x_2) = \\left(15x_2 - \\frac{5.1}{4\\pi^2}(15x_1-5)^2 + \\frac{5}{\\pi}(15x_1-5)-6\\right)^2 + \\left(10- \\frac{10}{8\\pi}\\right)\\cos(15x_1-5) +10\\)\\) is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([0,1]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/","title":"Branin discipline","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline","title":"branin_discipline","text":"<p>The Branin function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline.BraninDiscipline","title":"BraninDiscipline","text":"<pre><code>BraninDiscipline()\n</code></pre> <p>               Bases: <code>MDODiscipline</code></p> <p>The Branin function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_inputs.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/","title":"Branin function","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function","title":"branin_function","text":"<p>The Branin function.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function.BraninFunction","title":"BraninFunction","text":"<pre><code>BraninFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Branin function.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Branin\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/","title":"Branin problem","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem","title":"branin_problem","text":"<p>A problem connecting the Branin function with its uncertain space.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem.BraninProblem","title":"BraninProblem","text":"<pre><code>BraninProblem()\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Branin function with its uncertain space.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_problem.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__(BraninSpace())\n    self.objective = BraninFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/","title":"Branin space","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space","title":"branin_space","text":"<p>The uncertain space used in the Branin use case.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space.BraninSpace","title":"BraninSpace","text":"<pre><code>BraninSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Branin use case.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(f\"x{index + 1}\", \"OTUniformDistribution\")\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions","title":"functions","text":"<p>The Branin function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Branin function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The value of the gradient of the Branin function.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The value of the gradient of the Branin function.\n    \"\"\"\n    return array([\n        -15\n        * (\n            __C * sin(15 * x[0] - 5)\n            + 2\n            * (15 * x[1] - __A * (15 * x[0] - 5) ** 2 + __B * (15 * x[0] - 5) - 6)\n            * (2 * __A * (15 * x[0] - 5) - __B)\n        ),\n        30 * (15 * x[1] - __A * (15 * x[0] - 5) ** 2 + __B * (15 * x[0] - 5) - 6),\n    ])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Branin function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The output value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The output value.\n    \"\"\"\n    return (\n        (15 * x[1] - __A * (15 * x[0] - 5) ** 2 + __B * (15 * x[0] - 5) - 6) ** 2\n        + __C * cos(15 * x[0] - 5)\n        + 10\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/","title":"Rosenbrock","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/#gemseo_mlearning.problems.rosenbrock","title":"rosenbrock","text":"<p>The Rosenbrock use case to benchmark and illustrate active learning algorithms.</p> <p>The Rosenbrock function $$f(x_1,x_2) = 100(x_2-x_1^2)^2 + (1-x_1)^2 $$ is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([-2,2]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions","title":"functions","text":"<p>The Rosenbrock function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Rosenbrock function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The value of the gradient of the Rosenbrock function.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The value of the gradient of the Rosenbrock function.\n    \"\"\"\n    return array([\n        4 * __A * (x[0] ** 3 - x[0] * x[1]) + 2 * (x[0] - 1),\n        2 * __A * (x[1] - x[0] ** 2),\n    ])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Rosenbrock function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>The input value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The output value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The output value.\n    \"\"\"\n    return (1 - x[0]) ** 2 + __A * (x[1] - x[0] ** 2) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/","title":"Rosenbrock discipline","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline","title":"rosenbrock_discipline","text":"<p>The Rosenbrock function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline.RosenbrockDiscipline","title":"RosenbrockDiscipline","text":"<pre><code>RosenbrockDiscipline()\n</code></pre> <p>               Bases: <code>MDODiscipline</code></p> <p>The Rosenbrock function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_inputs.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/","title":"Rosenbrock function","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function","title":"rosenbrock_function","text":"<p>The Rosenbrock function.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function.RosenbrockFunction","title":"RosenbrockFunction","text":"<pre><code>RosenbrockFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Rosenbrock function.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Rosenbrock\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/","title":"Rosenbrock problem","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem","title":"rosenbrock_problem","text":"<p>A problem connecting the Rosenbrock function with its uncertain space.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem.RosenbrockProblem","title":"RosenbrockProblem","text":"<pre><code>RosenbrockProblem()\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Rosenbrock function with its uncertain space.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__(RosenbrockSpace())\n    self.objective = RosenbrockFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/","title":"Rosenbrock space","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space","title":"rosenbrock_space","text":"<p>The uncertain space used in the Rosenbrock use case.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space.RosenbrockSpace","title":"RosenbrockSpace","text":"<pre><code>RosenbrockSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Rosenbrock use case.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(\n            f\"x{index + 1}\", \"OTUniformDistribution\", minimum=-2, maximum=2\n        )\n</code></pre>"},{"location":"reference/gemseo_mlearning/regression/","title":"Regression","text":""},{"location":"reference/gemseo_mlearning/regression/#gemseo_mlearning.regression","title":"regression","text":"<p>The regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/","title":"Smt regressor","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor","title":"smt_regressor","text":"<p>A regression model from SMT.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTRegressor","title":"SMTRegressor","text":"<pre><code>SMTRegressor(\n    data: IODataset,\n    model_class_name: SMTSurrogateModel,\n    transformer: TransformerType = BaseRegressor.IDENTITY,\n    input_names: Iterable[str] = (),\n    output_names: Iterable[str] = (),\n    **model_options: Any\n)\n</code></pre> <p>               Bases: <code>BaseRegressor</code></p> <p>A regression model from SMT.</p> <p>Note</p> <p>SMT is an open-source Python package consisting of libraries of surrogate modeling methods, sampling methods, and benchmarking problems. Read this page for the list of surrogate models and options.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>IODataset</code>)           \u2013            <p>The learning dataset.</p> </li> <li> <code>model_class_name</code>               (<code>SMTSurrogateModel</code>)           \u2013            <p>The class name of a surrogate model available in SMT, i.e. a subclass of <code>smt.surrogate_models.surrogate_model.SurrogateModel</code>.</p> </li> <li> <code>transformer</code>               (<code>TransformerType</code>, default:                   <code>IDENTITY</code> )           \u2013            <p>The strategies to transform the variables. The values are instances of :class:<code>.BaseTransformer</code> while the keys are the names of either the variables or the groups of variables, e.g. <code>\"inputs\"</code> or <code>\"outputs\"</code> in the case of the regression algorithms. If a group is specified, the :class:<code>.BaseTransformer</code> will be applied to all the variables of this group. If :attr:<code>.IDENTITY</code>, do not transform the variables.</p> </li> <li> <code>input_names</code>               (<code>Iterable[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names of the input variables. If empty, consider all the input variables of the learning dataset.</p> </li> <li> <code>output_names</code>               (<code>Iterable[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names of the output variables. If empty, consider all the output variables of the learning dataset.</p> </li> <li> <code>**model_options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options of the surrogate model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When both the variable and the group it belongs to have a transformer.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/regression/smt_regressor.py</code> <pre><code>def __init__(\n    self,\n    data: IODataset,\n    model_class_name: SMTSurrogateModel,\n    transformer: TransformerType = BaseRegressor.IDENTITY,\n    input_names: Iterable[str] = (),\n    output_names: Iterable[str] = (),\n    **model_options: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        model_class_name: The class name of a surrogate model available in SMT,\n            i.e. a subclass of\n            `smt.surrogate_models.surrogate_model.SurrogateModel`.\n        **model_options: The options of the surrogate model.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        data,\n        transformer=transformer,\n        input_names=input_names,\n        output_names=output_names,\n        function=RBFRegressor.Function.THIN_PLATE,\n        **model_options,\n    )\n    _model_options = {\"print_global\": False}\n    _model_options.update(model_options)\n    self.algo = _NAMES_TO_CLASSES[model_class_name](**_model_options)\n</code></pre>"},{"location":"user_guide/","title":"Introduction","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":""},{"location":"user_guide/active_learning/active_learning_algo/","title":"Active learning algorithm","text":""},{"location":"user_guide/active_learning/active_learning_algo/#active-learning-algorithm","title":"Active learning algorithm","text":"<p>The ActiveLearningAlgo class defines an active learning algorithm.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#in-a-nutshell","title":"In a nutshell","text":"<p>Given a coarse regressor \\(\\hat{f}\\) (a.k.a. surrogate model) of a model \\(f\\) (a.k.a. substituted model), this algorithm updates this regressor sequentially from input-output points maximizing (or minimizing) an acquisition criterion (a.k.a. infill criterion) chosen for a specific purpose.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-an-acquisition-criterion","title":"Choosing an acquisition criterion","text":"<p><code>gemseo-mlearning</code> includes five families of acquisition criteria corresponding to as many purposes:</p> <ul> <li>the Minimum family   aims to make the minimum of the surrogate model   tends towards the minimum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Maximum family   aims to make the maximum of the surrogate model   tends towards the maximum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the LevelSet family   aims to make a level set of the surrogate model   tends towards the corresponding level set of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Quantile family   aims to make a quantile of the surrogate model   tends towards the corresponding quantile of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Exploration family   aims to make the error of the surrogate model tends towards zero   when the number of acquired points tends to infinity.</li> </ul> <p>Minimum, Maximum and Quantile provides an estimation of the quantity of interest, namely the minimum, the maximum and a quantile respectively. Whatever the family, the surrogate model can be used for prediction as any surrogate model. However, outside the Exploration family, it is important to bear in mind that its learning dataset has been designed to estimate a particular quantity of interest, and should therefore be used with caution. In other words, a surrogate model built to find the minimum can be bad at predicting the high output values. In such cases, the predicted standard-deviation that characterizes the uncertainty of the prediction can help to judge the accuracy of the surrogate prediction.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-a-surrogate-model","title":"Choosing a surrogate model","text":"<p>Many of these acquisition criteria consider that the surrogate model is a Gaussian process (GP) regressor \\(\\hat{f}\\) and are expressed from realizations or statistics of this GP \\(\\hat{F}\\), e.g. its mean \\(m_n(x)\\) and its variance \\(c_n(x,x)\\) at \\(x\\) where \\(c_n(\\cdot,\\cdot)\\) is its covariance function. These statistics and realizations can be obtained using the KrigingDistribution class which can be built from any regressor deriving from BaseRandomProcessRegressor, such as GaussianProcessRegressor based on scikit-learn and OTGaussianProcessRegressor based on OpenTURNS. By distribution we mean the probability distribution of a random function of which \\(f\\) is an instance. For non-GP regressors, this distribution is not a random process from the literature but an empirical distribution based on resampling techniques and is qualified as universal by its authors: RegressorDistribution.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#how-to-use-this-algorithm","title":"How to use this algorithm","text":"<p>A basic use of this class is</p> <ol> <li>instantiate    the ActiveLearningAlgo    from an input space of type DesignSpace,    a BaseRegressor    and the name of a family of acquisition criteria    (a default acquisition criterion will be set accordingly),</li> <li>update the regressor with the method <code>acquire_new_points</code>,</li> <li>get the updated regressor with the attribute <code>regressor</code>.</li> </ol> <p>For more advanced use, it is possible to change the acquisition algorithm, i.e. the optimization algorithm to minimize or maximize the acquisition criterion, as well as the acquisition criterion among the selected family.</p> <p>Lastly, the visualization subpackage offers plotting capabilities to draw the evolution of both the surrogate model and the acquisition criterion.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/","title":"What active learning is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#active-learning","title":"Active learning","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#what-active-learning-is","title":"What active learning is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#introduction","title":"Introduction","text":"<p>Active learning techniques are iterative methods that aim to sequentially estimate various quantities of interest: optimas, contours, failure probabilities, quantiles, expected values, etc. These methods generally start by constructing a rough surrogate model (an approximation of a costly exact physical model much faster to run). Then, they iteratively look for new simulations to run, and update the surrogate model and the estimated target quantities until a budget or accuracy criterion is reached. This search is guided by an acquisition criterion, also called infill criterion, to be minimized or maximized. This approach is inspired by the efficient global optimization (EGO) algorithm <sup>1</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#surrogate-modeling","title":"Surrogate modeling","text":"<p>Surrogate models<sup>2</sup> are approximations of a costly exact model \\(f:\\mathbf{x} \\in \\mathbb{X}\\subset\\mathbb{R}^d\\mapsto y \\in \\mathbb{R}\\) much faster to run. There are many types of surrogates, but one of the most popular, in particular in the active learning literature, is the Kriging model, a.k.a. Gaussian processes (GP) regressor <sup>3</sup>. One reason is that GP models provide a characterization of the prediction uncertainty. The exact simulator \\(f\\) is considered as a realization of a GP, \\(F\\), with trend \\(m : \\mathbb{X} \\rightarrow \\mathbb{R}\\) and covariance kernel \\(c : \\mathbb{X}\\times \\mathbb{X} \\rightarrow \\mathbb{R}\\). Conditioned by \\(n\\) observations \\(\\mathcal{A}_n = \\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n)\\}\\), \\(F_n  = F|\\mathcal{A}_n\\) is also a GP, with trend \\(m_n(\\cdot)\\) and covariance function \\(c_n(\\cdot,\\cdot)\\) that express differently depending on whether the trend function is known or not. The variance of the output prediction is denoted by \\(s_n^2(\\cdot) = c_n(\\cdot,\\cdot)\\).</p> <p>Note that the implementation of active learning strategies is not limited to the use of GP surrogates. In this case, a critical step of the method is the characterization of the uncertainty of the surrogate prediction, which can be achieved using bootstrap-based techniques<sup>4</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#the-rosenbrock-function","title":"The Rosenbrock function","text":"<p>The Rosenbrock function<sup>5</sup>, which will be used for illustrative purposes, expresses as :</p> \\[ f(\\mathbf{x}) = \\sum_{i = 1}^{d-1} [100(x_{i+1} - x_i^2)^2 +(1-x_i)^2], \\] <p>with \\(\\mathbf{x}\\) a \\(d\\)-dimensional input, \\(d \\geqslant 2\\). An illustration of the function with a two-dimensional input is depicted below:</p>  Bidimensional Rosenbrock function  <p>In the case of quantile estimation, \\(x_1\\) and \\(x_2\\) are replaced by the independent random variables \\(X_1\\) and \\(X_2\\) uniformly distributed over \\([-2,2]\\).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#acquisition-criteria","title":"Acquisition criteria","text":"<p>One of the key step of the active learning methodology is the choice of the acquisition criterion that guides the enrichment of the surrogate model. It is noted \\(J_n(\\cdot)\\) in the following. Several criteria have been developed for different purposes <sup>6</sup>. Typical applications of this methodology include the improvement of the surrogate model, the minimization of black-box functions and contour estimation, among others. Some popular criteria are detailed next for the objectives listed above.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#improving-the-quality-of-the-surrogate","title":"Improving the quality of the surrogate","text":"<p>A straightforward method to improve a surrogate model consists in choosing as enrichment points those with the highest predicted uncertainty, giving for the infill criterion \\(J_n(\\cdot)\\) :</p> \\[ J_n(\\mathbf{x}) = s_n^2(\\mathbf{x}),\\]  Level plot of the predicted uncertainty. The learning points are denoted with a red star.  <p>Note in this figure that, the predicted uncertainty (in fact the infill criterion) is the highest in the areas where the amount of learning points for the training of the surrogate is the smallest.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#finding-optimas","title":"Finding optimas","text":"<p>One of the most popular acquisition criterion to minimize (alternatively to maximize) black-box functions is the expected improvement <sup>1</sup> defined as:</p> \\[ J_n(\\mathbf{x}) =  \\text{EI}_n(\\mathbf{x})  = \\mathbb{E}[\\max(\\mathbf{y}_n - F_n(\\mathbf{x}),0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). In the previous equation, \\(\\mathbf{y}_n\\) corresponds to the output values of the database \\(\\mathcal{A}_n\\) and stands for \\(\\mathbf{y}_n = (y_i)_{i \\in [1, n]}\\).</p>  Level plot of the expected improvement. The learning points are denoted with a red star.  The global minimum is set in green.  <p>In this figure, some learning points are already close to the global minimum, so the infill criterion rather seeks here to improve the GP overall prediction.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#estimating-contour-sets","title":"Estimating contour sets","text":"<p>A usual criterion to estimate the level set<sup>7</sup> associated to value \\(a\\) is :</p> \\[ J_n(\\mathbf{x}) =  \\mathbb{E}[\\max((2 s_n^2(\\mathbf{x})) -\\vert{a - F_n(\\mathbf{x})}\\vert^2,0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). With an appropriate choice of \\(a\\), this criterion can be used to estimate quantiles or failure probabilities. In the following, it is implemented to estimate the \\(35\\%\\) quantile of the Rosenbrock function.</p>  Level plot of the acquisition criterion for level set estimation. The learning points are denoted with a red star. The level set associated to the quantile to estimate is drawn in black.  <p>In this figure, the areas with the highest acquisition criterion values refer to those that are located around the level set corresponding to the target quantile (which is here the objective study) and far from the learning points (where knowledge is already available).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#parallelization","title":"Parallelization","text":"<p>Most of the time, a single observation is added at the different steps of the active learning algorithm. However, to reduce the number of iterations of the active learning strategy, a batch of several points can be added <sup>8</sup> instead of considering a single one. This implies several changes, among which the expression of the infill criterion and its maximization. Let emphasize that parallelizing the active learning methodology significantly increases the computational burden which can still be alleviated by considering approximated heuristics <sup>8</sup>.</p> <p>A key challenge of this method is defining the size of the batch of points added at each iteration. In theory, it should be as high as possible, because for a fixed number of black-box evaluations, the higher it is, the lower the number of iterations is. However, this is synonymous with an increase of the numerical complexity of the method, and a balance must be therefore struck between the computational costs and the efficiency of the active learning strategy.</p> <p>In the next section, we will present the active learning algorithm available in gemseo-mlearning.</p> <ol> <li> <p>D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Forrester, A. Sobester, and A. Keane. Engineering design via surrogate modelling: a practical guide. John Wiley &amp; Sons, 2008.\u00a0\u21a9</p> </li> <li> <p>C. Williams and C. Rasmussen. Gaussian processes for machine learning. Volume 2. MIT press Cambridge, MA, 2006.\u00a0\u21a9</p> </li> <li> <p>M. Ben Salem, O. Roustant, F. Gamboa, and L. Tomaso. Universal prediction distribution for surrogate models. SIAM/ASA Journal on Uncertainty Quantification, 5:, 12 2015. doi:10.1137/15M1053529.\u00a0\u21a9</p> </li> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> <li> <p>F. Viana, C. Gogu, and T. Goel. Surrogate modeling: tricks that endured the test of time and some recent developments. Structural and Multidisciplinary Optimization, pages 1\u201328, 2021.\u00a0\u21a9</p> </li> <li> <p>P. Ranjan, D. Bingham, and G. Michailidis. Sequential experiment design for contour estimation from complex computer codes. Technometrics, 50(4):527\u2013541, 2008.\u00a0\u21a9</p> </li> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/regression/smt/","title":"SMT","text":""},{"location":"user_guide/regression/smt/#smts-surrogate-models","title":"SMT's surrogate models","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling with a focus on derivatives <sup>1</sup><sup>2</sup>.</p> <p><code>gemseo-mlearning</code> proposes the SMTRegressor to easily use any SMT's surrogate model in your GEMSEO processes.</p>"},{"location":"user_guide/regression/smt/#basic-usage","title":"Basic usage","text":"<p>You only have to instantiate this SMTRegressor class from:</p> <ul> <li>the IODataset including your input and output samples,</li> <li>the name of an SMT's surrogate model (e.g. <code>\"KRG\"</code> for Kriging or <code>\"RBF\"</code> for radial basis function),</li> <li>the options of this surrogate model,</li> <li>and the usual options of a BaseRegressor,   namely   <code>transformer</code> to transform the input and output data,   <code>input_names</code> to use a subset of input variables and   <code>output_names</code> to use a subset of output variables.</li> </ul> <p>Here's how to build an SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>model = SMTRegressor(training_dataset, \"RBF\", d0=2.0)\nmodel.learn()\n</code></pre>"},{"location":"user_guide/regression/smt/#options","title":"Options","text":"<p>Regarding the options of the SMT's surrogate models, you will find more information in the SMT's user guide by looking at the tables at the bottom of the pages.</p>"},{"location":"user_guide/regression/smt/#derivatives","title":"Derivatives","text":""},{"location":"user_guide/regression/smt/#differentiation","title":"Differentiation","text":"<p>You can use the <code>predict_jacobian</code> method of any SMTRegressor as long as the corresponding SMT's surrogate model can be derived with respect to its inputs.</p> <p>This feature is particularly useful in the case of gradient-based optimization.</p>"},{"location":"user_guide/regression/smt/#gradient-enhanced","title":"Gradient-enhanced","text":"<p>You can wrap the gradient-enhanced surrogate models available in SMT, such as <code>\"GEKPLS\"</code> and <code>\"GENN\"</code>, as long as your training dataset includes both output and Jacobian samples (the Jacobian samples have to be stored in the <code>\"gradients\"</code> group of the IODataset).</p> <p>This feature can improve the quality of the surrogate model without increasing the size of the training dataset.</p> <ol> <li> <p>B. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, M. Morlier, and J. R. R. A. Martins. A python surrogate modeling framework with derivatives. Advances in Engineering Software, pages 102662, 2019. doi:https://doi.org/10.1016/j.advengsoft.2019.03.005.\u00a0\u21a9</p> </li> <li> <p>P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, and J. R. R. A. Martins. SMT 2.0: A surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes. Advances in Engineering Sofware, 188:103571, 2024. doi:https://doi.org/10.1016/j.advengsoft.2023.103571.\u00a0\u21a9</p> </li> </ol>"}]}