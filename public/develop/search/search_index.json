{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-mlearning","title":"gemseo-mlearning","text":"<p><code>gemseo-mlearning</code> is a plugin of the library GEMSEO, dedicated to machine learning. This package is open-source, under the LGPL v3 license.</p>"},{"location":"#overview","title":"Overview","text":"<p>This package adds new regression models and quality measures.</p> <p>A package for active learning is also available, deeply based on the core GEMSEO objects for optimization, as well as a SurrogateBasedOptimization library built on its top. An effort is being made to improve both content and performance in future versions.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest stable version with <code>pip install gemseo-mlearning</code>.</p> <p>Install the development version with <code>pip install gemseo-mlearning@git+https://gitlab.com/gemseo/dev/gemseo-mlearning.git@develop</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Antoine Dechaume</li> <li>Beno\u00eet Pauwels</li> <li>Cl\u00e9ment Laboulfie</li> <li>Matthias De Lozzo</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#develop","title":"Develop","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>SMTRegressionModel   can be any surrogate model available in the Python package SMT.</li> <li>SurrogateBasedOptimization   can use an existing BaseMLRegressionAlgo   and save the BaseMLRegressionAlgo that it enriches   using the <code>regression_file_path</code> option.</li> <li>Given a sequence of input points,   OTGaussianProcessRegressor   can generate samples of the conditioned Gaussian process   with its method compute_samples.</li> <li>The <code>multi_start_n_samples</code>, <code>multi_start_algo_name</code> and <code>multi_start_algo_options</code> arguments of   OTGaussianProcessRegressor   allows to use multi-start optimization for the covariance model parameters;   by default, the number of starting points <code>multi_start_n_samples</code> is set to 10.</li> <li>The <code>optimization_space</code> argument of   OTGaussianProcessRegressor   allows to set the lower and upper bounds of the covariance model parameters   by means of a DesignSpace.</li> <li>The <code>covariance_model</code> argument of   OTGaussianProcessRegressor   allows to use any covariance model proposed by OpenTURNS   and facilitates the use of some of them through the enumeration   CovarianceModel;   its default value is Mat\u00e9rn 5/2.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>BREAKING CHANGE: The argument <code>trend_type</code> and the attribute <code>TrendType</code> of   OTGaussianProcessRegressor   renamed to <code>trend</code> and <code>Trend</code> respectively.</li> <li>BREAKING CHANGE: the acquisition criterion <code>LimitState</code> renamed to   LevelSet</li> <li>BREAKING CHANGE: each acquisition criterion class has a specific module   in gemseo_mlearning.active_learning.acquisition_criteria   whose name is the snake-case version of it class name, i.e. <code>nice_criterion.py</code> contains <code>NiceCriterion</code>.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.adaptive.distribution.MLRegressorDistribution</code> renamed to   gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.lib_surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.surrogate_based_optimization.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.core.surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.</li> <li>BREAKING CHANGE: <code>MLDataAcquisition</code> renamed to   ActiveLearningAlgo.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterion</code> renamed to   BaseAcquisitionCriterion   and moved to</li> <li>gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterionFactory</code> renamed to   AcquisitionCriterionFactory,   moved to   gemseo_mlearning.active_learning.acquisition_criteria   and without the property <code>available_criteria</code> (use <code>AcquisitionCriterionFactory.class_names</code>).</li> <li>BREAKING CHANGE: <code>gemseo.adaptive</code> renamed to gemseo_mlearning.active_learning.</li> <li>BREAKING CHANGE: <code>gemseo.adaptive.criteria</code> renamed to   gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE:   The module <code>gemseo_mlearning.api</code> no longer exists;   the functions   sample_discipline   and sample_disciplines   must be imported from gemseo_mlearning.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The data transformer can be set with the <code>\"transformer\"</code> key of the <code>regression_options</code> dictionary   passed to SurrogateBasedOptimization.</li> <li>The Quantile   estimates the quantile by Monte Carlo sampling   by means of the probability distributions of the input variables;   this distributions are defined with its new argument <code>uncertain_space</code>.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>sample_discipline</code>; use sample_disciplines from <code>gemseo</code> instead.</li> <li><code>sample_disciplines</code>; move to <code>gemseo</code>: sample_disciplines.</li> <li><code>MAEMeasure</code>; moved to <code>gemseo</code>: MAEMeasure.</li> <li><code>MEMeasure</code>; moved to <code>gemseo</code>: MEMeasure.</li> <li><code>GradientBoostingRegressor</code>; moved to <code>gemseo</code>: GradientBoostingRegressor.</li> <li><code>MLPRegressor</code>; moved to <code>gemseo</code>: MLPRegressor.</li> <li><code>OTGaussianProcessRegressor</code>; moved to <code>gemseo</code>: OTGaussianProcessRegressor.</li> <li><code>RegressorChain</code>; moved to <code>gemseo</code>: RegressorChain.</li> <li><code>SVMRegressor</code>; moved to <code>gemseo</code>: SVMRegressor.</li> <li><code>TPSRegressor</code>; moved to <code>gemseo</code>: TPSRegressor.</li> </ul>"},{"location":"changelog/#version-112-december-2023","title":"Version 1.1.2 (December 2023)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> <li>OTGaussianProcessRegressor   has a new optional argument <code>optimizer</code>   to select the OpenTURNS optimizer for the covariance model parameters.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-111-september-2023","title":"Version 1.1.1 (September 2023)","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>OTGaussianProcessRegressor.predict_std   no longer returns the variance of the output but its standard deviation.</li> </ul>"},{"location":"changelog/#version-110-june-2023","title":"Version 1.1.0 (June 2023)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>An argument <code>trend_type</code> of type   OTGaussianProcessRegressor.TrendType   to OTGaussianProcessRegressor;   the trend type of the Gaussian process regressor can be either constant,   linear or quadratic.</li> <li>A new optimization library   SurrogateBasedOptimization   to perform EGO-like surrogate-based optimization on unconstrained problems.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>The output of an MLDataAcquisitionCriterion   based on a regressor built from constant output values is no longer <code>nan</code>.</li> </ul>"},{"location":"changelog/#version-101-february-2022","title":"Version 1.0.1 (February 2022)","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>BaseRegressorDistribution   can now use a regression algorithm instantiated with transformers.</li> </ul>"},{"location":"changelog/#version-100-july-2022","title":"Version 1.0.0 (July 2022)","text":"<p>First release.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>The developers thank all the open source libraries making <code>gemseo-mlearning</code> possible.</p>"},{"location":"credits/#exec-1--external-dependencies","title":"External Dependencies","text":"<p><code>gemseo-mlearning</code> depends on software with compatible licenses that are listed below.</p> Project License <code>Python</code> Python Software License <code>gemseo</code> GNU Lesser General Public License v3 <code>numpy</code> BSD License <code>openturns</code> LGPL <code>scikit-learn</code> new BSD <code>scipy</code> BSD License <code>smt</code> BSD-3"},{"location":"credits/#exec-1--external-applications","title":"External applications","text":"<p>Some external applications are used by <code>gemseo-mlearning</code>, but not linked with the application, for testing, documentation generation, training or example purposes.</p> Project License <code>black</code> MIT <code>commitizen</code> MIT License <code>covdefaults</code> MIT License <code>docformatter</code> MIT License <code>griffe-inherited-docstrings</code> ISC <code>insert-license</code> MIT <code>markdown-exec</code> ISC <code>mike</code> BSD-3-Clause <code>mkdocs-bibtex</code> BSD-3-Clause-LBNL <code>mkdocs-gallery</code> BSD 3-Clause <code>mkdocs-gen-files</code> MIT License <code>mkdocs-include-markdown-plugin</code> Apache Software License <code>mkdocs-literate-nav</code> MIT License <code>mkdocs-material</code> MIT License <code>mkdocs-section-index</code> MIT License <code>mkdocstrings</code> ISC <code>pre-commit</code> MIT License <code>pygrep-hooks</code> MIT <code>pytest</code> MIT License <code>pytest-cov</code> MIT License <code>pytest-xdist</code> MIT License <code>ruff</code> MIT License <code>setuptools</code> MIT License <code>setuptools-scm</code> MIT License"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-mlearning</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-mlearning</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-mlearning</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"generated/examples/active_learning/","title":"Active learning","text":""},{"location":"generated/examples/active_learning/#active-learning","title":"Active learning","text":"<p> Expected improvement based on bootstrap. </p> <p> Expected improvement based on bootstrap. </p> <p> Expected improvement based on cross-validation. </p> <p> Limit state based on resampling. </p> <p> EGO based on resampling. </p> <p> Cross-validation vs bootstrap. </p> <p> Download all examples in Python source code: active_learning_python.zip</p> <p> Download all examples in Jupyter notebooks: active_learning_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_boot/","title":"Expected improvement based on bootstrap.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_boot/#expected-improvement-based-on-bootstrap","title":"Expected improvement based on bootstrap.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma import MeanSigma\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>algo = RBFRegressor(dataset)\nalgo.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#create-mlalgosampler","title":"Create MLAlgoSampler","text":"<pre><code>distribution = RegressorDistribution(algo)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#filling-objectives","title":"Filling objectives","text":"<pre><code>ego = ExpectedImprovement(distribution)\nlower = MeanSigma(distribution, -2.0)\nupper = MeanSigma(distribution, 2.0)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#find-next-training-point","title":"Find next training point","text":"<pre><code>space = DesignSpace()\nspace.add_variable(\"x\", l_b=x_l, u_b=x_u, value=1.5)\n\nacquisition = ActiveLearningAlgo(\"ExpectedImprovement\", space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\")\nopt = acquisition.compute_next_input_data()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#evaluation-of-discipline-surrogate-model-and-expected-improvement","title":"Evaluation of discipline, surrogate model and expected improvement","text":"<pre><code>x_test = linspace(x_l, x_u, n_test)\nego_data = []\nsurr_data = []\nlower_data = []\nupper_data = []\ny_test = f(x_test)\nfor x_i in x_test:\n    surr_data.append(algo.predict(array([x_i]))[0])\n    ego_data.append(ego(array([x_i]))[0])\n    lower_data.append(lower(array([x_i]))[0] * lower.output_range)\n    upper_data.append(upper(array([x_i]))[0] * upper.output_range)\nego_data = array(ego_data)\nlower_data = array(lower_data)\nupper_data = array(upper_data)\n\ndisc_data = IODataset()\ndisc_data.add_input_variable(\"x\", x_test)\ndisc_data.add_output_variable(\"y\", y_test)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_boot/#plotting","title":"Plotting","text":"<pre><code>fig, ax = plt.subplots(2, 1)\nfor algo_b in distribution.algos:\n    algo_data = [algo_b.predict(array([x_i])) for x_i in x_test]\n    ax[0].plot(x_test, algo_data, \"gray\", alpha=0.2)\nax[0].plot(\n    x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n)\nax[0].plot(\n    x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n)\nax[0].plot(x_test, surr_data, \"b\", label=\"surrogate\")\nax[0].fill_between(\n    x_test, lower_data, upper_data, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax[0].legend(loc=\"upper right\")\nax[0].axvline(x=opt[0])\nax[1].plot(x_test, ego_data, \"r\", label=\"EGO\")\nax[1].axvline(x=opt[0])\nax[1].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_boot.py</p> <p> Download Jupyter notebook: ego_boot.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_cv/","title":"Expected improvement based on cross-validation.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_cv/#expected-improvement-based-on-cross-validation","title":"Expected improvement based on cross-validation.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma import MeanSigma\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>algo = RBFRegressor(dataset)\nalgo.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#create-mlalgosampler","title":"Create MLAlgoSampler","text":"<pre><code>distribution = RegressorDistribution(algo, bootstrap=False, loo=True)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#filling-objectives","title":"Filling objectives","text":"<pre><code>ego = ExpectedImprovement(distribution)\nlower = MeanSigma(distribution, -2.0)\nupper = MeanSigma(distribution, 2.0)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#find-next-training-point","title":"Find next training point","text":"<pre><code>space = DesignSpace()\nspace.add_variable(\"x\", l_b=x_l, u_b=x_u, value=1.5)\n\nacquisition = ActiveLearningAlgo(\"ExpectedImprovement\", space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\")\nopt = acquisition.compute_next_input_data()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#evaluation-of-discipline-surrogate-model-and-expected-improvement","title":"Evaluation of discipline, surrogate model and expected improvement","text":"<pre><code>x_test = linspace(x_l, x_u, n_test)\nego_data = []\nsurr_data = []\nlower_data = []\nupper_data = []\ny_test = f(x_test)\nfor x_i in x_test:\n    surr_data.append(algo.predict(array([x_i]))[0])\n    ego_data.append(ego(array([x_i]))[0])\n    lower_data.append(lower(array([x_i]))[0] * lower.output_range)\n    upper_data.append(upper(array([x_i]))[0] * upper.output_range)\nego_data = array(ego_data)\nlower_data = array(lower_data)\nupper_data = array(upper_data)\n\ndisc_data = IODataset()\ndisc_data.add_input_variable(\"x\", x_test)\ndisc_data.add_output_variable(\"y\", y_test)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv/#plotting","title":"Plotting","text":"<pre><code>fig, ax = plt.subplots(2, 1)\nfor algo_b in distribution.algos:\n    algo_data = [algo_b.predict(array([x_i])) for x_i in x_test]\n    ax[0].plot(x_test, algo_data, \"gray\", alpha=0.2)\nax[0].plot(\n    x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n)\nax[0].plot(\n    x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n)\nax[0].plot(x_test, surr_data, \"b\", label=\"surrogate\")\nax[0].fill_between(\n    x_test, lower_data, upper_data, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax[0].legend(loc=\"upper right\")\nax[0].axvline(x=opt[0])\nax[1].plot(x_test, ego_data, \"r\", label=\"EGO\")\nax[1].axvline(x=opt[0])\nax[1].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_cv.py</p> <p> Download Jupyter notebook: ego_cv.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_cv_boot/","title":"Cross-validation vs bootstrap.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_cv_boot/#cross-validation-vs-bootstrap","title":"Cross-validation vs bootstrap.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma import MeanSigma\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv_boot/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n\nax = [[None, None], [None, None]]\nax[0][0] = plt.subplot(221)\nax[0][1] = plt.subplot(222, sharey=ax[0][0])\nax[1][0] = plt.subplot(223)\nax[1][1] = plt.subplot(224, sharey=ax[1][0])\n</code></pre>"},{"location":"generated/examples/active_learning/ego_cv_boot/#active-learning","title":"Active learning","text":"<p>We compare the bootstrap and leave-one-out methods in search of a new point to learn in order to estimate the minimum of the discipline</p> <pre><code>for index, bootstrap in enumerate([False, True]):\n    # Train a RBF regression model\n    algo = RBFRegressor(dataset)\n    algo.learn()\n\n    # Build a regressor distribution\n    distribution = RegressorDistribution(algo, bootstrap=bootstrap, loo=not bootstrap)\n    distribution.learn()\n\n    # Define the expected improvement measure\n    ego = ExpectedImprovement(distribution)\n\n    # Define confidence bounds, equal to mean +/- 2*sigma\n    lower = MeanSigma(distribution, -2.0)\n    upper = MeanSigma(distribution, 2.0)\n\n    # Define the input_space\n    input_space = DesignSpace()\n    input_space.add_variable(\"x\", l_b=x_l, u_b=x_u, value=1.5)\n\n    # Define the data acquisition process\n    acquisition = ActiveLearningAlgo(\"ExpectedImprovement\", input_space, distribution)\n    acquisition.set_acquisition_algorithm(\"fullfact\")\n\n    # Compute the next input data\n    opt = acquisition.compute_next_input_data()\n\n    # Plot the results\n    x_test = linspace(x_l, x_u, n_test)\n    ego_data = []\n    surr_data = []\n    lower_data = []\n    upper_data = []\n    y_test = f(x_test)\n    for x_i in x_test:\n        surr_data.append(algo.predict(array([x_i]))[0])\n        ego_data.append(ego(array([x_i]))[0])\n        lower_data.append(lower(array([x_i]))[0] * lower.output_range)\n        upper_data.append(upper(array([x_i]))[0] * upper.output_range)\n\n    disc_data = IODataset()\n    disc_data.add_input_variable(\"x\", x_test)\n    disc_data.add_output_variable(\"y\", y_test)\n\n    for algo in distribution.algos:\n        algo_data = [algo.predict(array([x_i])) for x_i in x_test]\n        ax[0][index].plot(x_test, algo_data, \"gray\", alpha=0.2)\n\n    ax[0][index].plot(\n        x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n    )\n    ax[0][index].plot(\n        x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n    )\n    ax[0][index].plot(x_test, surr_data, \"b\", label=\"surrogate\")\n    ax[0][index].fill_between(\n        x_test,\n        array(lower_data),\n        array(upper_data),\n        color=\"b\",\n        alpha=0.1,\n        label=\"CI(95%)\",\n    )\n    ax[0][index].legend(loc=\"upper right\")\n    ax[0][index].axvline(x=opt[0])\n    ax[1][index].plot(x_test, array(ego_data), \"r\", label=\"EGO\")\n    ax[1][index].axvline(x=opt[0])\n    ax[1][index].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_cv_boot.py</p> <p> Download Jupyter notebook: ego_cv_boot.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_krig/","title":"Ego krig","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_krig/#expected-improvement-based-on-bootstrap","title":"Expected improvement based on bootstrap.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma import MeanSigma\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.kriging_distribution import (\n    KrigingDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#initial-learning-dataset","title":"Initial learning dataset","text":"<pre><code>def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>algo = GaussianProcessRegressor(dataset)\nalgo.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#create-mlalgosampler","title":"Create MLAlgoSampler","text":"<pre><code>distribution = KrigingDistribution(algo)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#filling-objectives","title":"Filling objectives","text":"<pre><code>ego = ExpectedImprovement(distribution)\nlower = MeanSigma(distribution, -2.0)\nupper = MeanSigma(distribution, 2.0)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#find-next-training-point","title":"Find next training point","text":"<pre><code>space = DesignSpace()\nspace.add_variable(\"x\", l_b=x_l, u_b=x_u, value=1.5)\n\nacquisition = ActiveLearningAlgo(\"ExpectedImprovement\", space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\")\nopt = acquisition.compute_next_input_data()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#evaluation-of-discipline-surrogate-model-and-expected-improvement","title":"Evaluation of discipline, surrogate model and expected improvement","text":"<pre><code>x_test = linspace(x_l, x_u, n_test)\nego_data = []\nsurr_data = []\nlower_data = []\nupper_data = []\ny_test = f(x_test)\nfor x_i in x_test:\n    surr_data.append(algo.predict(array([x_i]))[0])\n    ego_data.append(ego(array([x_i]))[0])\n    lower_data.append(lower(array([x_i]))[0] * lower.output_range)\n    upper_data.append(upper(array([x_i]))[0] * upper.output_range)\nego_data = array(ego_data)\nlower_data = array(lower_data)\nupper_data = array(upper_data)\n\ndisc_data = IODataset()\ndisc_data.add_input_variable(\"x\", x_test)\ndisc_data.add_output_variable(\"y\", y_test)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_krig/#plotting","title":"Plotting","text":"<pre><code>fig, ax = plt.subplots(2, 1)\nax[0].plot(\n    x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n)\nax[0].plot(\n    x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n)\nax[0].plot(x_test, surr_data, \"b\", label=\"surrogate\")\nax[0].fill_between(\n    x_test, lower_data, upper_data, color=\"b\", alpha=0.1, label=\"CI(95%)\"\n)\nax[0].legend(loc=\"upper right\")\nax[0].axvline(x=opt[0])\nax[1].plot(x_test, ego_data, \"r\", label=\"EGO\")\nax[1].axvline(x=opt[0])\nax[1].legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_krig.py</p> <p> Download Jupyter notebook: ego_krig.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/ego_rosenbrock/","title":"EGO based on resampling.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#ego-based-on-resampling","title":"EGO based on resampling.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.disciplines.analytic import AnalyticDiscipline\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import linspace\nfrom numpy import zeros\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nconfigure_logger()\n\nn_test = 20\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#definition-of-the-discipline","title":"Definition of the discipline","text":"<pre><code>discipline = AnalyticDiscipline({\"z\": \"(1-x)**2+100*(y-x**2)**2\"}, name=\"Rosenbrock\")\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#definition-of-the-input-space","title":"Definition of the input space","text":"<pre><code>input_space = DesignSpace()\ninput_space.add_variable(\"x\", l_b=-2, u_b=2, value=1.0)\ninput_space.add_variable(\"y\", l_b=-2, u_b=2, value=1.0)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>learning_dataset = sample_disciplines([discipline], input_space, \"z\", 30, \"OT_OPT_LHS\")\nalgo = RBFRegressor(learning_dataset)\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#universal-distribution-for-this-surrogate-model","title":"Universal distribution for this surrogate model","text":"<pre><code>distribution = RegressorDistribution(algo, bootstrap=False)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#data-acquisition-to-improve-the-surrogate-model","title":"Data acquisition to improve the surrogate model","text":"<pre><code>acquisition = ActiveLearningAlgo(\"ExpectedImprovement\", input_space, distribution)\nacquisition.set_acquisition_algorithm(\"fullfact\", n_samples=1000)\nacquisition.update_algo(discipline, 20)\n\nopt = distribution.algo.learning_set[[\"x\", \"y\"]]\nopt_x = opt[\"x\"]\nopt_y = opt[\"y\"]\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#evaluation-of-discipline-and-expected-improvement","title":"Evaluation of discipline and expected improvement","text":"<pre><code>crit = ExpectedImprovement(distribution)\nx_test = linspace(-2, 2, n_test)\ndisc_data = zeros((n_test, n_test))\ncrit_data = zeros((n_test, n_test))\nsurr_data = zeros((n_test, n_test))\nfor i in range(n_test):\n    for j in range(n_test):\n        xij = array([x_test[j], x_test[i]])\n        input_data = {\"x\": array([xij[0]]), \"y\": array([xij[1]])}\n        disc_data[i, j] = discipline.execute(input_data)[\"z\"][0]\n        crit_data[i, j] = crit(xij)\n        surr_data[i, j] = algo.predict(xij)[0]\n</code></pre>"},{"location":"generated/examples/active_learning/ego_rosenbrock/#plotting","title":"Plotting","text":"<pre><code>train = learning_dataset.get_view(variable_names=[\"x\", \"y\"]).to_numpy()\nx_train = train[\"x\"]\ny_train = train[\"y\"]\nfig = plt.figure(constrained_layout=True)\nspec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\naxes = [[None, None], [None, None]]\ntitles = [[\"Discipline\", \"Infill criterion\"], [\"Surrogate model\", None]]\ndata = [[disc_data, crit_data], [surr_data, None]]\nfor i in range(2):\n    for j in range(2):\n        if [i, j] != [1, 1]:\n            axes[i][j] = fig.add_subplot(spec[i, j])\n            axes[i][j].contourf(x_test, x_test, data[i][j])\n            axes[i][j].axhline(1, color=\"white\", alpha=0.5)\n            axes[i][j].axvline(1, color=\"white\", alpha=0.5)\n            axes[i][j].plot(x_train[:, 0], y_train[:, 0], \"w+\", ms=1)\n            for index, _ in enumerate(opt_x):\n                axes[i][j].plot(opt_x[index, 0], opt_y[index, 0], \"wo\", ms=1)\n                axes[i][j].annotate(\n                    index + 1,\n                    (opt_x[index, 0] + 0.05, opt_y[index, 0] + 0.05),\n                    color=\"white\",\n                )\n            axes[i][j].set_title(titles[i][j])\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: ego_rosenbrock.py</p> <p> Download Jupyter notebook: ego_rosenbrock.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/limit_state/","title":"Limit state based on resampling.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/limit_state/#limit-state-based-on-resampling","title":"Limit state based on resampling.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.disciplines.analytic import AnalyticDiscipline\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import linspace\nfrom numpy import zeros\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.quantile import Quantile\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nl_b = -2\nu_b = 2\n\nn_test = 20\nlevel = 0.8\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#definition-of-the-discipline","title":"Definition of the discipline","text":"<pre><code>discipline = AnalyticDiscipline({\"z\": \"(1-x)**2+100*(y-x**2)**2\"}, name=\"Rosenbrock\")\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#definition-of-the-input-space","title":"Definition of the input space","text":"<pre><code>input_space = DesignSpace()\ninput_space.add_variable(\"x\", l_b=-2, u_b=2, value=1.0)\ninput_space.add_variable(\"y\", l_b=-2, u_b=2, value=1.0)\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#initial-surrogate-model","title":"Initial surrogate model","text":"<pre><code>learning_dataset = sample_disciplines([discipline], input_space, \"z\", 30, \"OT_OPT_LHS\")\nalgo = RBFRegressor(learning_dataset)\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#universal-distribution-for-this-surrogate-model","title":"Universal distribution for this surrogate model","text":"<pre><code>distribution = RegressorDistribution(algo, bootstrap=False)\ndistribution.learn()\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#data-acquisition-to-improve-the-surrogate-model","title":"Data acquisition to improve the surrogate model","text":"<pre><code>acquisition = ActiveLearningAlgo(\"Quantile\", input_space, distribution, level=level)\nacquisition.set_acquisition_algorithm(\"fullfact\", n_samples=1000)\nacquisition.update_algo(discipline, 20)\n\npoints = distribution.algo.learning_set[[\"x\", \"y\"]]\npoints_x = points[\"x\"]\npoints_y = points[\"y\"]\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#evaluation-of-discipline-and-expected-improvement","title":"Evaluation of discipline and expected improvement","text":"<pre><code>crit = Quantile(distribution, level)\nx_test = linspace(l_b, u_b, n_test)\ndisc_data = zeros((n_test, n_test))\ncrit_data = zeros((n_test, n_test))\nsurr_data = zeros((n_test, n_test))\nfor i in range(n_test):\n    for j in range(n_test):\n        xij = array([x_test[j], x_test[i]])\n        input_data = {\"x\": array([xij[0]]), \"y\": array([xij[1]])}\n        disc_data[i, j] = discipline.execute(input_data)[\"z\"][0]\n        crit_data[i, j] = crit(xij)\n        surr_data[i, j] = algo.predict(xij)[0]\n</code></pre>"},{"location":"generated/examples/active_learning/limit_state/#plotting","title":"Plotting","text":"<pre><code>train = learning_dataset.get_view(variable_names=[\"x\", \"y\"]).to_numpy()\nx_train = train[\"x\"]\ny_train = train[\"y\"]\nfig = plt.figure(constrained_layout=True)\nspec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\naxes = [[None, None], [None, None]]\ntitles = [[\"Discipline\", \"Infill criterion\"], [\"Surrogate model\", None]]\ndata = [[disc_data, crit_data], [surr_data, None]]\nfor i in range(2):\n    for j in range(2):\n        if [i, j] != [1, 1]:\n            axes[i][j] = fig.add_subplot(spec[i, j])\n            axes[i][j].contourf(x_test, x_test, data[i][j])\n            axes[i][j].plot(x_train[:, 0], y_train[:, 0], \"w+\", ms=1)\n            for index, _ in enumerate(points_x):\n                axes[i][j].plot(points_x[index, 0], points_y[index, 0], \"wo\", ms=1)\n                axes[i][j].annotate(\n                    index + 1,\n                    (points_x[index, 0] + 0.05, points_y[index, 0] + 0.05),\n                    color=\"white\",\n                )\n            axes[i][j].set_title(titles[i][j])\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: limit_state.py</p> <p> Download Jupyter notebook: limit_state.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/","title":"Optimization","text":""},{"location":"generated/examples/optimization/#surrogate-based-optimization","title":"Surrogate-based optimization","text":"<p> Surrogate-based optimization of Rastrigin's function. </p> <p> Download all examples in Python source code: optimization_python.zip</p> <p> Download all examples in Jupyter notebooks: optimization_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/sbo_rastrigin/","title":"Surrogate-based optimization of Rastrigin's function.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/sbo_rastrigin/#surrogate-based-optimization-of-rastrigins-function","title":"Surrogate-based optimization of Rastrigin's function.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configure_logger\nfrom gemseo import execute_algo\nfrom gemseo import execute_post\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\n\nconfigure_logger()\n\nproblem = Rastrigin()\n\nexecute_algo(\n    problem,\n    algo_name=\"SBO\",\n    acquisition_options={\n        \"max_iter\": 1000,\n        \"popsize\": 50,\n        \"stop_crit_n_x\": 10000,\n    },\n    max_iter=20,\n)\n\nexecute_post(problem, \"OptHistoryView\", save=False, show=True)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: sbo_rastrigin.py</p> <p> Download Jupyter notebook: sbo_rastrigin.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_mlearning<ul> <li>active_learning<ul> <li>acquisition_criteria<ul> <li>acquisition_criterion_factory</li> <li>base_acquisition_criterion</li> <li>expectation</li> <li>expected_improvement</li> <li>level_set</li> <li>max_expected_improvement</li> <li>mean_sigma</li> <li>min_expected_improvement</li> <li>minimum_distance</li> <li>quantile</li> <li>standard_deviation</li> <li>variance</li> </ul> </li> <li>active_learning_algo</li> <li>distributions<ul> <li>base_regressor_distribution</li> <li>kriging_distribution</li> <li>regressor_distribution</li> </ul> </li> </ul> </li> <li>algos<ul> <li>opt<ul> <li>core<ul> <li>surrogate_based_optimizer</li> </ul> </li> <li>surrogate_based_optimization</li> </ul> </li> </ul> </li> <li>regression<ul> <li>smt_regression_model</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/gemseo_mlearning/","title":"API documentation","text":""},{"location":"reference/gemseo_mlearning/#gemseo_mlearning","title":"gemseo_mlearning","text":"<p>A GEMSEO extension for advanced machine learning.</p> <p>GEMSEO includes the main machine learning capabilities.</p>"},{"location":"reference/gemseo_mlearning/active_learning/","title":"Active learning","text":""},{"location":"reference/gemseo_mlearning/active_learning/#gemseo_mlearning.active_learning","title":"active_learning","text":"<p>An algorithm with criteria to find new points to learn.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/","title":"Active learning algo","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo","title":"active_learning_algo","text":"<p>Active learning algorithm using regression algorithms and acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo","title":"ActiveLearningAlgo","text":"<pre><code>ActiveLearningAlgo(\n    criterion: str,\n    input_space: DesignSpace,\n    distribution: BaseRegressorDistribution,\n    **criterion_options: Any\n)\n</code></pre> <p>An active learning algorithm.</p> <p>Parameters:</p> <ul> <li> <code>criterion</code>               (<code>str</code>)           \u2013            <p>The name of a data acquisition criterion selecting new point(s) to reach a particular goal (name of a class inheriting from BaseAcquisitionCriterion ).</p> </li> <li> <code>input_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The input space on which to look for the new learning point.</p> </li> <li> <code>distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of the machine learning algorithm.</p> </li> <li> <code>**criterion_options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options of the data acquisition criterion.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>When the output dimension is greater than 1.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def __init__(\n    self,\n    criterion: str,\n    input_space: DesignSpace,\n    distribution: BaseRegressorDistribution,\n    **criterion_options: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        criterion: The name of a data acquisition criterion\n            selecting new point(s) to reach a particular goal\n            (name of a class inheriting from\n            [BaseAcquisitionCriterion][gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion]\n            ).\n        input_space: The input space on which to look for the new learning point.\n        distribution: The distribution of the machine learning algorithm.\n        **criterion_options: The options of the data acquisition criterion.\n\n    Raises:\n        NotImplementedError: When the output dimension is greater than 1.\n    \"\"\"  # noqa: D205 D212 D415\n    if distribution.output_dimension &gt; 1:\n        msg = \"ActiveLearningAlgo works only with scalar output.\"\n        raise NotImplementedError(msg)\n    self.__acquisition_criterion = criterion\n    self.__acquisition_criterion_options = criterion_options.copy()\n    self.__distribution = distribution\n    self.__input_space = input_space\n    self.__acquisition_problem = self.__create_acquisition_problem()\n    self.__acquisition_algo = OptimizationLibraryFactory().create(\n        self.default_algo_name\n    )\n    self.__acquisition_algo_options = self.default_opt_options\n    self.__database = Database()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_algo_name","title":"default_algo_name  <code>class-attribute</code>","text":"<pre><code>default_algo_name: str = 'NLOPT_COBYLA'\n</code></pre> <p>The name of the default algorithm to find the new training point(s).</p> <p>Typically a DoE or an optimizer.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_doe_options","title":"default_doe_options  <code>class-attribute</code>","text":"<pre><code>default_doe_options: dict[str, Any] = {'n_samples': 100}\n</code></pre> <p>The names and values of the default DoE options.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.default_opt_options","title":"default_opt_options  <code>class-attribute</code>","text":"<pre><code>default_opt_options: dict[str, Any] = {'max_iter': 100}\n</code></pre> <p>The names and values of the default optimization options.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.compute_next_input_data","title":"compute_next_input_data","text":"<pre><code>compute_next_input_data(as_dict: bool = False) -&gt; DataType\n</code></pre> <p>Find the next learning point.</p> <p>Parameters:</p> <ul> <li> <code>as_dict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the input data split by input names. Otherwise, return a unique array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The next learning point.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def compute_next_input_data(\n    self,\n    as_dict: bool = False,\n) -&gt; DataType:\n    \"\"\"Find the next learning point.\n\n    Args:\n        as_dict: Whether to return the input data split by input names.\n            Otherwise, return a unique array.\n\n    Returns:\n        The next learning point.\n    \"\"\"\n    with LoggingContext(logging.getLogger(\"gemseo\")):\n        input_data = self.__acquisition_algo.execute(\n            self.__acquisition_problem, **self.__acquisition_algo_options\n        ).x_opt\n\n    if as_dict:\n        return self.__acquisition_problem.design_space.array_to_dict(input_data)\n\n    return input_data\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.set_acquisition_algorithm","title":"set_acquisition_algorithm","text":"<pre><code>set_acquisition_algorithm(\n    algo_name: str, **options: Any\n) -&gt; None\n</code></pre> <p>Set sampling or optimization algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm to find the learning point(s). Typically a DoE or an optimizer.</p> </li> <li> <code>**options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The values of some algorithm options; use the default values for the other ones.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def set_acquisition_algorithm(self, algo_name: str, **options: Any) -&gt; None:\n    \"\"\"Set sampling or optimization algorithm.\n\n    Args:\n        algo_name: The name of the algorithm to find the learning point(s).\n            Typically a DoE or an optimizer.\n        **options: The values of some algorithm options;\n            use the default values for the other ones.\n    \"\"\"\n    factory = DOELibraryFactory()\n    if factory.is_available(algo_name):\n        self.__acquisition_algo_options = self.default_doe_options.copy()\n    else:\n        factory = OptimizationLibraryFactory()\n        self.__acquisition_algo_options = self.default_opt_options.copy()\n\n    self.__acquisition_algo_options.update(options)\n    self.__acquisition_algo = factory.create(algo_name)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.update_algo","title":"update_algo","text":"<pre><code>update_algo(\n    discipline: MDODiscipline, n_samples: int = 1\n) -&gt; tuple[Database, OptimizationProblem]\n</code></pre> <p>Update the machine learning algorithm by learning new samples.</p> <p>This method acquires new learning input-output samples and trains the machine learning algorithm with the resulting enriched learning set.</p> <p>Parameters:</p> <ul> <li> <code>discipline</code>               (<code>MDODiscipline</code>)           \u2013            <p>The discipline computing the reference output data from the input data provided by the acquisition process.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of samples to update the machine learning algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Database, OptimizationProblem]</code>           \u2013            <p>The concatenation of the optimization histories related to the different points and the last acquisition problem.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def update_algo(\n    self, discipline: MDODiscipline, n_samples: int = 1\n) -&gt; tuple[Database, OptimizationProblem]:\n    \"\"\"Update the machine learning algorithm by learning new samples.\n\n    This method acquires new learning input-output samples\n    and trains the machine learning algorithm\n    with the resulting enriched learning set.\n\n    Args:\n        discipline: The discipline computing the reference output data\n            from the input data provided by the acquisition process.\n        n_samples: The number of samples to update the machine learning algorithm.\n\n    Returns:\n        The concatenation of the optimization histories\n        related to the different points\n        and the last acquisition problem.\n    \"\"\"\n    LOGGER.info(\"Update machine learning algorithm with %s points\", n_samples)\n    for sample_id in CustomTqdmProgressBar(range(1, n_samples + 1)):\n        input_data = self.compute_next_input_data(as_dict=True)\n        for inputs, outputs in self.__acquisition_problem.database.items():\n            self.__database.store(\n                array([sample_id, *inputs.unwrap().tolist()]), outputs\n            )\n\n        discipline.execute(input_data)\n\n        extra_learning_set = IODataset()\n        distribution = self.__distribution\n        variable_names_to_n_components = distribution.algo.sizes\n        extra_learning_set.add_group(\n            group_name=IODataset.INPUT_GROUP,\n            data=hstack(list(input_data.values()))[newaxis],\n            variable_names=distribution.input_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        output_names = distribution.output_names\n        extra_learning_set.add_group(\n            group_name=IODataset.OUTPUT_GROUP,\n            data=hstack([\n                discipline.local_data[output_name] for output_name in output_names\n            ])[newaxis],\n            variable_names=output_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        augmented_learning_set = concat(\n            [distribution.algo.learning_set, extra_learning_set],\n            ignore_index=True,\n        )\n\n        self.__distribution.change_learning_set(augmented_learning_set)\n        self.update_problem()\n\n    return self.__database, self.__acquisition_problem\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.update_problem","title":"update_problem","text":"<pre><code>update_problem() -&gt; None\n</code></pre> <p>Update the acquisition problem.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def update_problem(self) -&gt; None:\n    \"\"\"Update the acquisition problem.\"\"\"\n    self.__acquisition_problem = self.__create_acquisition_problem()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/","title":"Acquisition criteria","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/#gemseo_mlearning.active_learning.acquisition_criteria","title":"acquisition_criteria","text":"<p>The acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/acquisition_criterion_factory/","title":"Acquisition criterion factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/acquisition_criterion_factory/#gemseo_mlearning.active_learning.acquisition_criteria.acquisition_criterion_factory","title":"acquisition_criterion_factory","text":"<p>A factory of acquisition criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/acquisition_criterion_factory/#gemseo_mlearning.active_learning.acquisition_criteria.acquisition_criterion_factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/acquisition_criterion_factory/#gemseo_mlearning.active_learning.acquisition_criteria.acquisition_criterion_factory.AcquisitionCriterionFactory","title":"AcquisitionCriterionFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>A factory of <code>BaseAcquisitionCriterion</code>.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/","title":"Base acquisition criterion","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion","title":"base_acquisition_criterion","text":"<p>Acquisition criterion for which the optimum would improve the regression model.</p> <p>An acquisition criterion (also called infill criterion) is a function taking a model input value and returning a value of interest to maximize (default option) or minimize according to the meaning of the acquisition criterion.</p> <p>Then, the input value optimizing this criterion can be used to enrich the dataset used by a machine learning algorithm in its training stage. This is the purpose of active learning.</p> <p>This notion of acquisition criterion is implemented through the AcquisitionCriterion class which is built from a BaseRegressor and inherits from MDOFunction.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion","title":"BaseAcquisitionCriterion","text":"<pre><code>BaseAcquisitionCriterion(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>A base acquisition criterion.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/","title":"Expectation","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation","title":"expectation","text":"<p>Criterion to be maximized for estimating a maximum, without exploration.</p> <p>Statistic:</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>Bootstrap estimator:</p> \\[\\widehat{E}[x] = \\frac{1}{B}\\sum_{b=1}^B Y_b(x)\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation.Expectation","title":"Expectation","text":"<pre><code>Expectation(algo_distribution: BaseRegressorDistribution)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for estimating a maximum, without exploration.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation.Expectation-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation.Expectation.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation.Expectation.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expectation/#gemseo_mlearning.active_learning.acquisition_criteria.expectation.Expectation.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expected_improvement/","title":"Expected improvement","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement","title":"expected_improvement","text":"<p>Criterion to be maximized for estimating a minimum, with exploration.</p> <p>This is the same as the expected improvement of the regression model for the minimum.</p> <p>Statistic:</p> \\[EI[x] = \\mathbb{E}[\\max(y_{\\text{min}}-Y(x),0)]\\] <p>where \\(y_{\\text{min}}=\\min_{1\\leq i \\leq n}~y^{(i)}\\).</p> <p>Bootstrap estimator:</p> \\[\\widehat{EI}[x] = \\frac{1}{B}\\sum_{b=1}^B \\max(y_{\\text{min}}-Y_b(x),0)\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement.ExpectedImprovement","title":"ExpectedImprovement","text":"<pre><code>ExpectedImprovement(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>MinExpectedImprovement</code></p> <p>Criterion to be maximized for estimating a minimum, with exploration.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set","title":"level_set","text":"<p>Criterion to be minimized for estimating a level set, with exploration.</p> <p>Statistics:</p> \\[EI[x] = \\mathbb{E}[|q-Y(x)|]\\] <p>where \\(q\\) is a value provided by the user.</p> <p>Bootstrap estimator:</p> \\[\\widehat{EI}[x] = \\frac{1}{B}\\sum_{b=1}^B |q-Y_b(x)|\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet","title":"LevelSet","text":"<pre><code>LevelSet(\n    algo_distribution: BaseRegressorDistribution,\n    value: float,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be minimized for estimating a level set, with exploration.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> <li> <code>value</code>               (<code>float</code>)           \u2013            <p>A value of interest.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set.py</code> <pre><code>def __init__(\n    self, algo_distribution: BaseRegressorDistribution, value: float\n) -&gt; None:\n    \"\"\"\n    Args:\n        value: A value of interest.\n    \"\"\"  # noqa: D205 D212 D415\n    self.value = value\n    super().__init__(algo_distribution)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.LevelSet.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: float = value\n</code></pre> <p>The value of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/","title":"Max expected improvement","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement","title":"max_expected_improvement","text":"<p>Criterion to be maximized for estimating a maximum, with exploration.</p> <p>Statistic:</p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-y_{\\text{max}},0)]\\] <p>where \\(y_{\\text{max}}=\\max_{1\\leq i \\leq n}~y^{(i)}\\).</p> <p>Bootstrap estimator:</p> \\[\\widehat{EI}[x] = \\frac{1}{B}\\sum_{b=1}^B \\max(Y_b(x)-f_{\\text{max}},0)\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement.MaxExpectedImprovement","title":"MaxExpectedImprovement","text":"<pre><code>MaxExpectedImprovement(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for estimating a maximum, with exploration.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement.MaxExpectedImprovement-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement.MaxExpectedImprovement.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement.MaxExpectedImprovement.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/max_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.max_expected_improvement.MaxExpectedImprovement.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/","title":"Mean sigma","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma","title":"mean_sigma","text":"<p>Criterion to be maximized for estimating a maximum, without exploration.</p> <p>Statistic:</p> \\[M[x;\\kappa] = \\mathbb{E}[x] + \\kappa \\times \\mathbb{S}[x]\\] <p>Estimator:</p> \\[\\widehat{M}[x;\\kappa] = \\widehat{E}[x] + \\kappa \\times \\widehat{sigma}[x]\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma","title":"MeanSigma","text":"<pre><code>MeanSigma(\n    algo_distribution: BaseRegressorDistribution,\n    kappa: float,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for estimating a maximum, without exploration.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> <li> <code>kappa</code>               (<code>float</code>)           \u2013            <p>A factor associated with the standard deviation to increase or decrease the mean value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma.py</code> <pre><code>def __init__(\n    self, algo_distribution: BaseRegressorDistribution, kappa: float\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A factor associated with the standard deviation\n            to increase or decrease the mean value.\n    \"\"\"  # noqa: D205 D212 D415\n    self.kappa = kappa\n    super().__init__(algo_distribution)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma.kappa","title":"kappa  <code>instance-attribute</code>","text":"<pre><code>kappa: float = kappa\n</code></pre> <p>The factor associated with the standard deviation.</p> <p>It is used to increase or decrease the mean value.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/mean_sigma/#gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma.MeanSigma.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/","title":"Min expected improvement","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement","title":"min_expected_improvement","text":"<p>Criterion to be maximized for estimating a minimum, with exploration.</p> <p>Statistic:</p> \\[EI[x] = \\mathbb{E}[\\max(y_{\\text{min}}-Y(x),0)]\\] <p>where \\(y_{\\text{min}}=\\min_{1\\leq i \\leq n}~y^{(i)}\\).</p> <p>Bootstrap estimator:</p> \\[\\widehat{EI}[x] = \\frac{1}{B}\\sum_{b=1}^B \\max(f_{\\text{min}}-Y_b(x),0)\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement.MinExpectedImprovement","title":"MinExpectedImprovement","text":"<pre><code>MinExpectedImprovement(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for estimating a minimum, with exploration.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement.MinExpectedImprovement-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement.MinExpectedImprovement.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement.MinExpectedImprovement.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/min_expected_improvement/#gemseo_mlearning.active_learning.acquisition_criteria.min_expected_improvement.MinExpectedImprovement.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/","title":"Minimum distance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance","title":"minimum_distance","text":"<p>Criterion to be maximized for adding points away from the learning dataset.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance.MinimumDistance","title":"MinimumDistance","text":"<pre><code>MinimumDistance(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for adding points away from the learning dataset.</p> <p>This infill criterion computes the minimum distance between a new point and the point of the learning dataset, scaled by the maximum distance between two learning points.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance.MinimumDistance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance.MinimumDistance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance.MinimumDistance.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum_distance/#gemseo_mlearning.active_learning.acquisition_criteria.minimum_distance.MinimumDistance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile","title":"quantile","text":"<p>Criterion to be minimized for estimating a quantile, with exploration.</p> <p>Statistic:</p> \\[EI[x] = \\mathbb{E}[|q(\\alpha)-Y(x)|]\\] <p>where \\(q\\) is a quantile with level \\(\\alpha\\).</p> <p>Bootstrap estimator:</p> \\[\\widehat{EI}[x] = \\frac{1}{B}\\sum_{b=1}^B |q-Y_b(x)|\\]"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile","title":"Quantile","text":"<pre><code>Quantile(\n    algo_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10000,\n)\n</code></pre> <p>               Bases: <code>LevelSet</code></p> <p>Criterion to be minimized for estimating a quantile, with exploration.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> <li> <code>level</code>               (<code>float</code>)           \u2013            <p>The quantile level.</p> </li> <li> <code>uncertain_space</code>               (<code>ParameterSpace</code>)           \u2013            <p>The uncertain variable space.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>The number of samples to estimate the quantile of the regression model by Monte Carlo.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile.py</code> <pre><code>def __init__(\n    self,\n    algo_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regression model by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = algo_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = uncertain_space.__class__()\n    for name in input_names:\n        new_uncertain_space[name] = uncertain_space[name]\n\n    data = algo_distribution.predict(new_uncertain_space.compute_samples(n_samples))\n    super().__init__(algo_distribution, quantile(data, level))\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.Quantile.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: float = value\n</code></pre> <p>The value of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/","title":"Standard deviation","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation","title":"standard_deviation","text":"<p>Criterion to be maximized for improving the quality of the regression model.</p> <p>Statistic:</p> \\[\\sigma[x] = \\sqrt{\\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]}\\] <p>Bootstrap estimator:</p> \\[\\hat{\\sigma}[x] = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B (Y_b(x)-\\widehat{E}[x])^2}\\] <p>where \\(\\widehat{E}[x]= \\frac{1}{B}\\sum_{b=1}^B Y_b(x)\\).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation.StandardDeviation","title":"StandardDeviation","text":"<pre><code>StandardDeviation(\n    algo_distribution: BaseRegressorDistribution,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for improving the quality of the regression model.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation.StandardDeviation-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation.StandardDeviation.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation.StandardDeviation.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.standard_deviation.StandardDeviation.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/","title":"Variance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance","title":"variance","text":"<p>Criterion to be maximized for improving the quality of the regression model.</p> <p>Statistic:</p> \\[V[x] = \\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]\\] <p>Bootstrap estimator:</p> \\[\\widehat{V}[x] = \\frac{1}{B-1}\\sum_{b=1}^B (Y_b(x)-\\widehat{E}[x])^2\\] <p>where \\(\\widehat{E}[x]= \\frac{1}{B}\\sum_{b=1}^B Y_b(x)\\).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance.Variance","title":"Variance","text":"<pre><code>Variance(algo_distribution: BaseRegressorDistribution)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>Criterion to be maximized for improving the quality of the regression model.</p> <p>This criterion is scaled by the output range.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> <p>Parameters:</p> <ul> <li> <code>algo_distribution</code>               (<code>BaseRegressorDistribution</code>)           \u2013            <p>The distribution of a machine learning algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(self, algo_distribution: BaseRegressorDistribution) -&gt; None:\n    \"\"\"\n    Args:\n        algo_distribution: The distribution of a machine learning algorithm.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo_distribution = algo_distribution\n    dataset = self.algo_distribution.learning_set\n    data = dataset.get_view(group_names=dataset.OUTPUT_GROUP).to_numpy()\n    self.output_range = data.max() - data.min()\n    try:\n        jac = self._compute_jacobian(ones(algo_distribution.algo.input_dimension))\n    except NotImplementedError:\n        jac = None\n    super().__init__(\n        self._compute_output,\n        self._compute_output.__name__,\n        jac=jac,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance.Variance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance.Variance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance.Variance.algo_distribution","title":"algo_distribution  <code>instance-attribute</code>","text":"<pre><code>algo_distribution: BaseRegressorDistribution = (\n    algo_distribution\n)\n</code></pre> <p>The distribution of a machine learning algorithm assessor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/variance/#gemseo_mlearning.active_learning.acquisition_criteria.variance.Variance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/","title":"Distributions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions","title":"distributions","text":"<p>The distributions of machine learning algorithms.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions.get_regressor_distribution","title":"get_regressor_distribution","text":"<pre><code>get_regressor_distribution(\n    regression_algorithm: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution\n</code></pre> <p>Return the distribution of a regression algorithm.</p> <p>Parameters:</p> <ul> <li> <code>regression_algorithm</code>               (<code>BaseRegressor</code>)           \u2013            <p>The regression algorithm.</p> </li> <li> <code>use_bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bootstrap for resampling. If <code>False</code>, use cross-validation.</p> </li> <li> <code>use_loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use leave-one-out resampling when <code>use_bootstrap</code> is <code>False</code>. If <code>False</code>, use parameterized cross-validation.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseRegressorDistribution</code>           \u2013            <p>The distribution of the regression algorithm.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/__init__.py</code> <pre><code>def get_regressor_distribution(\n    regression_algorithm: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution:\n    \"\"\"Return the distribution of a regression algorithm.\n\n    Args:\n        regression_algorithm: The regression algorithm.\n        use_bootstrap: Whether to use bootstrap for resampling.\n            If `False`, use cross-validation.\n        use_loo: Whether to use leave-one-out resampling when\n            `use_bootstrap` is `False`.\n            If `False`, use parameterized cross-validation.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n\n    Returns:\n        The distribution of the regression algorithm.\n    \"\"\"\n    if hasattr(regression_algorithm, \"predict_std\"):\n        return KrigingDistribution(regression_algorithm)\n\n    return RegressorDistribution(regression_algorithm, use_bootstrap, use_loo, size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/","title":"Base regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution","title":"base_regressor_distribution","text":"<p>This module defines the notion of distribution of a machine learning algorithm.</p> <p>Once a BaseMLAlgo has been trained, assessing its quality is important before using it.</p> <p>One can not only measure its global quality (e.g. from a BaseMLAlgoQuality ) but also its local one.</p> <p>The BaseRegressorDistribution class addresses the latter case, by quantifying the robustness of a machine learning algorithm to a learning point. The more robust it is, the less variability it has around this point.</p> Note <p>For now, one does not consider any BaseMLAlgo but instances of class which is built from a MLSupervisedAlgo.</p> <p>The BaseRegressorDistribution can be particularly useful to:</p> <ul> <li>study the robustness of an BaseMLAlgo   w.r.t. learning dataset elements,</li> <li>evaluate acquisition criteria for active learning purposes   (see   BaseAcquisitionCriterion   ),</li> <li>etc.</li> </ul> <p>The abstract BaseRegressorDistribution class is derived into two classes:</p> <ul> <li>KrigingDistribution   :   the   BaseRegressor   is a Kriging model   and this assessor takes advantage of the underlying Gaussian stochastic process,</li> <li>RegressorDistribution:   this class is based on sampling methods,   such as bootstrap,   cross-validation   or leave-one-out.</li> </ul>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution","title":"BaseRegressorDistribution","text":"<pre><code>BaseRegressorDistribution(algo: BaseRegressor)\n</code></pre> <p>Distribution related to a regression model.</p> <p>Parameters:</p> <ul> <li> <code>algo</code>               (<code>BaseRegressor</code>)           \u2013            <p>A regression model.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def __init__(self, algo: BaseRegressor) -&gt; None:\n    \"\"\"\n    Args:\n        algo: A regression model.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo = algo\n    self._samples = []\n    self._transform_input_group = self.algo._transform_input_group\n    self._transform_output_group = self.algo._transform_output_group\n    self._input_variables_to_transform = self.algo._input_variables_to_transform\n    self._output_variables_to_transform = self.algo._output_variables_to_transform\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = algo\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm inputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the original machine learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The dimension of the machine learning output space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm outputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the machine learning algorithm relying on the initial learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the machine learning algorithm relying on the initial learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval  <code>abstractmethod</code>","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Predict the lower bounds and upper bounds from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_confidence_interval(\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    \"\"\"Predict the lower bounds and upper bounds from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n        level: A quantile level.\n\n    Returns:\n        The lower and upper bound values.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_expected_improvement","title":"compute_expected_improvement  <code>abstractmethod</code>","text":"<pre><code>compute_expected_improvement(\n    input_data: DataType,\n    fopt: float,\n    maximize: bool = False,\n) -&gt; DataType\n</code></pre> <p>Compute the expected improvement from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>fopt</code>               (<code>float</code>)           \u2013            <p>The current optimum value.</p> </li> <li> <code>maximize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The type of optimum to seek.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The expected improvement value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_expected_improvement(\n    self, input_data: DataType, fopt: float, maximize: bool = False\n) -&gt; DataType:\n    \"\"\"Compute the expected improvement from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n        fopt: The current optimum value.\n        maximize: The type of optimum to seek.\n\n    Returns:\n        The expected improvement value.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_mean","title":"compute_mean  <code>abstractmethod</code>","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the mean from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The mean value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_mean(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the mean from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The mean value.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the standard deviation from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The standard deviation value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the standard deviation from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The standard deviation value.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_variance","title":"compute_variance  <code>abstractmethod</code>","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the variance from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The variance value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_variance(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the variance from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The variance value.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the machine learning algorithm from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the machine learning algorithm from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output of the original machine learning algorithm.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The predicted output data.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Predict the output of the original machine learning algorithm.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The predicted output data.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/","title":"Kriging distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution","title":"kriging_distribution","text":"<p>Distribution related to a Kriging-like regression model.</p> <p>A Kriging-like regression model predicts both output mean and standard deviation while a standard regression model predicts only the output value.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution","title":"KrigingDistribution","text":"<pre><code>KrigingDistribution(algo: BaseRandomProcessRegressor)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Distribution related to a Kriging-like regression model.</p> <p>The regression model must be a Kriging-like regression model computing both mean and standard deviation.</p> <p>Parameters:</p> <ul> <li> <code>algo</code>               (<code>BaseRandomProcessRegressor</code>)           \u2013            <p>A regression model.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def __init__(  # noqa: D107\n    self, algo: BaseRandomProcessRegressor\n) -&gt; None:\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = algo\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm inputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the original machine learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The dimension of the machine learning output space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm outputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the machine learning algorithm relying on the initial learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the machine learning algorithm relying on the initial learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Predict the lower bounds and upper bounds from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    mean = self.compute_mean(input_data)\n    std = self.compute_standard_deviation(input_data)\n    quantile = norm.ppf(level)\n    if isinstance(mean, Mapping):\n        lower = {name: mean[name] - quantile * std[name] for name in mean}\n        upper = {name: mean[name] + quantile * std[name] for name in mean}\n    else:\n        lower = mean - quantile * std\n        upper = mean + quantile * std\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_expected_improvement","title":"compute_expected_improvement","text":"<pre><code>compute_expected_improvement(\n    input_data: DataType,\n    f_opt: float,\n    maximize: bool = False,\n) -&gt; DataType\n</code></pre> <p>Compute the expected improvement from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>fopt</code>               (<code>float</code>)           \u2013            <p>The current optimum value.</p> </li> <li> <code>maximize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The type of optimum to seek.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The expected improvement value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_expected_improvement(  # noqa: D102\n    self,\n    input_data: DataType,\n    f_opt: float,\n    maximize: bool = False,\n) -&gt; DataType:\n    mean = self.compute_mean(input_data)\n    improvement = mean - f_opt if maximize else f_opt - mean\n\n    std = self.compute_standard_deviation(input_data)\n    value = improvement / std\n    return improvement * norm.cdf(value) + std * norm.pdf(value)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the mean from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The mean value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_mean(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the standard deviation from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The standard deviation value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_standard_deviation(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict_std(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the variance from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The variance value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_variance(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.compute_standard_deviation(input_data) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the machine learning algorithm from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the machine learning algorithm from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output of the original machine learning algorithm.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The predicted output data.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Predict the output of the original machine learning algorithm.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The predicted output data.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/","title":"Regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution","title":"regressor_distribution","text":"<p>Universal distribution for regression models.</p> <p>A RegressorDistribution samples an BaseRegressor, by learning new versions of the latter from subsets of the original learning dataset.</p> <p>These new BaseMLAlgo instances are based on sampling methods, such as bootstrap, cross-validation or leave-one-out.</p> <p>Sampling a BaseMLAlgo can be particularly useful to:</p> <ul> <li>study the robustness of a BaseMLAlgo   w.r.t. learning dataset elements,</li> <li>estimate infill criteria for active learning purposes,</li> <li>etc.</li> </ul>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution","title":"RegressorDistribution","text":"<pre><code>RegressorDistribution(\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Distribution related to a regression algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algo</code>               (<code>BaseRegressor</code>)           \u2013            <p>A regression model.</p> </li> <li> <code>bootstrap</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>The resampling method. If <code>True</code>, use bootstrap resampling. Otherwise, use cross-validation resampling.</p> </li> <li> <code>loo</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The leave-One-Out sub-method, when bootstrap is <code>False</code>. If <code>False</code>, use parameterized cross-validation, Otherwise use leave-one-out.</p> </li> <li> <code>size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def __init__(\n    self,\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        bootstrap: The resampling method.\n            If `True`, use bootstrap resampling.\n            Otherwise, use cross-validation resampling.\n        loo: The leave-One-Out sub-method, when bootstrap is `False`.\n            If `False`, use parameterized cross-validation,\n            Otherwise use leave-one-out.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n    \"\"\"  # noqa: D205 D212 D415\n    if bootstrap:\n        self.method = self.BOOTSTRAP\n        self.size = size or self.N_BOOTSTRAP\n    else:\n        if loo:\n            self.method = self.LOO\n            self.size = len(algo.learning_set)\n        else:\n            self.method = self.CROSS_VALIDATION\n            self.size = size or self.N_FOLDS\n    self.algos = [\n        algo.__class__(\n            data=algo.learning_set, transformer=algo.transformer, **algo.parameters\n        )\n        for _ in range(self.size)\n    ]\n    self.weights = []\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP","title":"N_BOOTSTRAP  <code>class-attribute</code>","text":"<pre><code>N_BOOTSTRAP: int = 100\n</code></pre> <p>The default number of replicates for the bootstrap method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS","title":"N_FOLDS  <code>class-attribute</code>","text":"<pre><code>N_FOLDS: int = 5\n</code></pre> <p>The default number of folds for the cross-validation method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = algo\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm inputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the original machine learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>The resampling method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The dimension of the machine learning output space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The names of the original machine learning algorithm outputs.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre> <p>The size of the resampling set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: list[Callable[[NumberArray], float]] = []\n</code></pre> <p>The weight functions related to the sub-algorithms.</p> <p>A weight function computes a weight from an input data array.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the machine learning algorithm relying on the initial learning set.</p> <p>Parameters:</p> <ul> <li> <code>learning_set</code>               (<code>Dataset</code>)           \u2013            <p>The new learning set.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:  # noqa: D102\n    for algo in self.algos:\n        algo.learning_set = learning_set\n    super().change_learning_set(learning_set)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Predict the lower bounds and upper bounds from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>level</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>A quantile level.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code>           \u2013            <p>The lower and upper bound values.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    level = (1.0 - level) / 2.0\n    predictions = self.predict_members(input_data)\n    if isinstance(predictions, Mapping):\n        lower = {\n            name: quantile(value, level, axis=0)\n            for name, value in predictions.items()\n        }\n        upper = {\n            name: quantile(value, 1 - level, axis=0)\n            for name, value in predictions.items()\n        }\n    else:\n        lower = quantile(predictions, level, axis=0)\n        upper = quantile(predictions, 1 - level, axis=0)\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_expected_improvement","title":"compute_expected_improvement","text":"<pre><code>compute_expected_improvement(\n    input_data: DataType,\n    fopt: float,\n    maximize: bool = False,\n) -&gt; DataType\n</code></pre> <p>Compute the expected improvement from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> <li> <code>fopt</code>               (<code>float</code>)           \u2013            <p>The current optimum value.</p> </li> <li> <code>maximize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>The type of optimum to seek.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The expected improvement value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_expected_improvement(  # noqa: D102\n    self,\n    input_data: DataType,\n    fopt: float,\n    maximize: bool = False,\n) -&gt; DataType:\n    predictions = self.predict_members(input_data)\n    weights = self._evaluate_weights(input_data)\n    if maximize:\n        expected_improvement = maximum(predictions - fopt, 0.0)\n    else:\n        expected_improvement = maximum(fopt - predictions, 0.0)\n    return self.__average(weights, expected_improvement)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the mean from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The mean value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_mean(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self._evaluate_weights(input_data)\n    return self.__average(weights, predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the standard deviation from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The standard deviation value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the standard deviation from input data.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The standard deviation value.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the variance from input data.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The variance value.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples\ndef compute_variance(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self._evaluate_weights(input_data)\n    term1 = self.__average(weights, predictions**2)\n    term2 = self.__average(weights, predictions) ** 2\n    return term1 - term2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the machine learning algorithm from the learning dataset.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def learn(  # noqa: D102\n    self,\n    samples: list[int] | None = None,\n) -&gt; None:\n    self.weights = []\n    super().learn(samples)\n    if self.method in [self.CROSS_VALIDATION, self.LOO]:\n        n_folds = self.size\n        folds = array_split(self._samples, n_folds)\n    for index, algo in enumerate(self.algos):\n        if self.method == self.BOOTSTRAP:\n            new_samples = unique(\n                default_rng(1).choice(self._samples, len(self._samples))\n            )\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n        else:\n            fold = folds[index]\n            new_samples = npdelete(self._samples, fold)\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n\n        algo.learn(new_samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output of the original machine learning algorithm.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The predicted output data.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Predict the output of the original machine learning algorithm.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The predicted output data.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict_members","title":"predict_members","text":"<pre><code>predict_members(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output value with the different machine learning algorithms.</p> <p>After prediction, the method stacks the results.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>               (<code>DataType</code>)           \u2013            <p>The input data, specified as either as a numpy.array or as dictionary of numpy.array indexed by inputs names. The numpy.array can be either a (d,) array representing a sample in dimension d, or a (M, d) array representing M samples in dimension d.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>The output data (dimension p) of N machine learning algorithms.     If input_data.shape == (d,), then output_data.shape == (N, p).     If input_data.shape == (M,d), then output_data;shape == (N,M,p).</p> </li> </ul> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def predict_members(self, input_data: DataType) -&gt; DataType:\n    \"\"\"Predict the output value with the different machine learning algorithms.\n\n    After prediction, the method stacks the results.\n\n    Args:\n        input_data: The input data,\n            specified as either as a numpy.array or as dictionary of numpy.array\n            indexed by inputs names.\n            The numpy.array can be either a (d,) array\n            representing a sample in dimension d,\n            or a (M, d) array representing M samples in dimension d.\n\n    Returns:\n        The output data (dimension p) of N machine learning algorithms.\n            If input_data.shape == (d,), then output_data.shape == (N, p).\n            If input_data.shape == (M,d), then output_data;shape == (N,M,p).\n    \"\"\"\n    predictions = [algo.predict(input_data) for algo in self.algos]\n    if isinstance(input_data, Mapping):\n        return {\n            name: stack([prediction[name] for prediction in predictions])\n            for name in predictions[0]\n        }\n    return stack(predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/","title":"Algos","text":""},{"location":"reference/gemseo_mlearning/algos/#gemseo_mlearning.algos","title":"algos","text":"<p>Wrappers for algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/#gemseo_mlearning.algos.opt","title":"opt","text":"<p>Wrappers for optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/","title":"Surrogate based optimization","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization","title":"surrogate_based_optimization","text":"<p>A library for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedAlgorithmDescription","title":"SurrogateBasedAlgorithmDescription  <code>dataclass</code>","text":"<pre><code>SurrogateBasedAlgorithmDescription(\n    library_name: str = \"gemseo-mlearning\",\n)\n</code></pre> <p>               Bases: <code>OptimizationAlgorithmDescription</code></p> <p>The description of a surrogate-based optimization algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedOptimization","title":"SurrogateBasedOptimization","text":"<pre><code>SurrogateBasedOptimization()\n</code></pre> <p>               Bases: <code>OptimizationLibrary</code></p> <p>A wrapper for surrogate-based optimization.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p> Source code in <code>src/gemseo_mlearning/algos/opt/surrogate_based_optimization.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.descriptions = {\n        self.__SBO: SurrogateBasedAlgorithmDescription(\n            algorithm_name=self.__SBO,\n            description=\"GEMSEO in-house surrogate-based optimizer.\",\n            handle_equality_constraints=False,\n            handle_inequality_constraints=False,\n            handle_integer_variables=True,  # provided acquisition handles integers\n            internal_algorithm_name=self.__SBO,\n        )\n    }\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/","title":"Core","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/#gemseo_mlearning.algos.opt.core","title":"core","text":"<p>Core functionalities for the optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/","title":"Surrogate based optimizer","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer","title":"surrogate_based_optimizer","text":"<p>A class for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer","title":"SurrogateBasedOptimizer","text":"<pre><code>SurrogateBasedOptimizer(\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = OpenTURNS.OT_LHSO,\n    doe_options: Mapping[\n        str, DriverLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: (\n        str | BaseRegressor\n    ) = GaussianProcessRegressor.__name__,\n    regression_options: Mapping[\n        str, MLAlgoParameterType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    acquisition_options: Mapping[\n        str, OptimizationLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n)\n</code></pre> <p>An optimizer based on surrogate models.</p> <p>Parameters:</p> <ul> <li> <code>acquisition_algorithm</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm to optimize the data acquisition criterion. N.B. this algorithm must handle integers if some of the optimization variables are integers.</p> </li> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>doe_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Either the size of the initial DOE or 0 if the size is inferred from doe_options. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_algorithm</code>               (<code>str</code>, default:                   <code>OT_LHSO</code> )           \u2013            <p>The name of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_algorithm</code>               (<code>str | BaseRegressor</code>, default:                   <code>__name__</code> )           \u2013            <p>Either the name of the regression algorithm approximating the objective function over the design space or the regression algorithm itself.</p> </li> <li> <code>regression_options</code>               (<code>Mapping[str, MLAlgoParameterType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the regression algorithm. If transformer is missing, use :attr:<code>.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER</code>. This argument is ignored when regression_algorithm is a BaseRegressor.</p> </li> <li> <code>regression_file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the file to save the regression model. If empty, do not save the regression model.</p> </li> <li> <code>acquisition_options</code>               (<code>Mapping[str, OptimizationLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm to optimize the data acquisition criterion.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def __init__(\n    self,\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = OpenTURNS.OT_LHSO,\n    doe_options: Mapping[str, DriverLibraryOptionType] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: (str | BaseRegressor) = GaussianProcessRegressor.__name__,\n    regression_options: Mapping[str, MLAlgoParameterType] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    acquisition_options: Mapping[\n        str, OptimizationLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n) -&gt; None:\n    \"\"\"\n    Args:\n        acquisition_algorithm: The name of the algorithm to optimize the data\n            acquisition criterion.\n            N.B. this algorithm must handle integers if some of the optimization\n            variables are integers.\n        problem: The optimization problem.\n        doe_size: Either the size of the initial DOE\n            or 0 if the size is inferred from doe_options.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_algorithm: The name of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_options: The options of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_algorithm: Either the name of the regression algorithm\n            approximating the objective function over the design space\n            or the regression algorithm itself.\n        regression_options: The options of the regression algorithm.\n            If transformer is missing,\n            use :attr:`.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER`.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_file_path: The path to the file to save the regression model.\n            If empty, do not save the regression model.\n        acquisition_options: The options of the algorithm to optimize\n            the data acquisition criterion.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__problem = problem\n    if isinstance(regression_algorithm, BaseRegressor):\n        self.__dataset = regression_algorithm.learning_set\n    else:\n        # Store max_iter as it will be overwritten by DOELibrary\n        max_iter = problem.max_iter\n        options = dict(doe_options)\n        if doe_size &gt; 0 and DOELibrary.N_SAMPLES not in options:\n            options[DOELibrary.N_SAMPLES] = doe_size\n\n        # Store the listeners as they will be cleared by DOELibrary.\n        new_iter_listeners, store_listeners = problem.database.clear_listeners()\n        with LoggingContext(logging.getLogger(\"gemseo\")):\n            DOELibraryFactory().execute(problem, doe_algorithm, **options)\n\n        database = self.__problem.database\n        for listener in new_iter_listeners:\n            database.add_new_iter_listener(listener)\n\n        for listener in store_listeners:\n            database.add_store_listener(listener)\n\n        self.__dataset = problem.to_dataset(opt_naming=False)\n        _regression_options = {\"transformer\": BaseRegressor.DEFAULT_TRANSFORMER}\n        _regression_options.update(dict(regression_options))\n        regression_algorithm = RegressorFactory().create(\n            regression_algorithm,\n            data=self.__dataset,\n            **_regression_options,\n        )\n        # Add the first iteration to the current_iter reset by DOELibrary.\n        problem.current_iter += 1\n        # And restore max_iter.\n        problem.max_iter = max_iter\n\n    self.__distribution = get_regressor_distribution(regression_algorithm)\n    self.__active_learning_algo = ActiveLearningAlgo(\n        ExpectedImprovement.__name__, problem.design_space, self.__distribution\n    )\n    self.__active_learning_algo.set_acquisition_algorithm(\n        acquisition_algorithm, **acquisition_options\n    )\n    self.__regression_file_path = regression_file_path\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer.execute","title":"execute","text":"<pre><code>execute(number_of_acquisitions: int) -&gt; str\n</code></pre> <p>Execute the surrogate-based optimization.</p> <p>Parameters:</p> <ul> <li> <code>number_of_acquisitions</code>               (<code>int</code>)           \u2013            <p>The number of learning points to be acquired.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The termination message.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def execute(self, number_of_acquisitions: int) -&gt; str:\n    \"\"\"Execute the surrogate-based optimization.\n\n    Args:\n        number_of_acquisitions: The number of learning points to be acquired.\n\n    Returns:\n        The termination message.\n    \"\"\"\n    self.__distribution.learn()\n    message = self.__STOP_BECAUSE_MAX_ACQUISITIONS\n    for _ in range(number_of_acquisitions):\n        input_data = self.__active_learning_algo.compute_next_input_data()\n        if input_data in self.__problem.database:\n            message = self.__STOP_BECAUSE_ALREADY_KNOWN\n            break\n\n        output_data = self.__problem.evaluate_functions(\n            x_vect=input_data, normalize=False\n        )[0]\n        extra_learning_set = IODataset()\n        distribution = self.__distribution\n        variable_names_to_n_components = distribution.algo.sizes\n        extra_learning_set.add_group(\n            group_name=IODataset.INPUT_GROUP,\n            data=input_data[newaxis],\n            variable_names=distribution.input_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        output_names = distribution.output_names\n        extra_learning_set.add_group(\n            group_name=IODataset.OUTPUT_GROUP,\n            data=hstack([output_data[output_name] for output_name in output_names])[\n                newaxis\n            ],\n            variable_names=output_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        self.__dataset = concat(\n            [distribution.algo.learning_set, extra_learning_set],\n            ignore_index=True,\n        )\n        self.__dataset = self.__dataset.map(lambda x: x.real)\n        distribution.change_learning_set(self.__dataset)\n        self.__active_learning_algo.update_problem()\n\n        if self.__regression_file_path:\n            with Path(self.__regression_file_path).open(\"wb\") as file:\n                pickle.dump(distribution.algo, file)\n\n    return message\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/regression/","title":"Regression","text":""},{"location":"reference/gemseo_mlearning/regression/#gemseo_mlearning.regression","title":"regression","text":"<p>The regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/","title":"Smt regression model","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/#gemseo_mlearning.regression.smt_regression_model","title":"smt_regression_model","text":"<p>A regression model from SMT.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/#gemseo_mlearning.regression.smt_regression_model-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/#gemseo_mlearning.regression.smt_regression_model.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/#gemseo_mlearning.regression.smt_regression_model-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regression_model/#gemseo_mlearning.regression.smt_regression_model.SMTRegressionModel","title":"SMTRegressionModel","text":"<pre><code>SMTRegressionModel(\n    data: IODataset,\n    model_class_name: SMTSurrogateModel,\n    transformer: TransformerType = BaseRegressor.IDENTITY,\n    input_names: Iterable[str] = (),\n    output_names: Iterable[str] = (),\n    **model_options: Any\n)\n</code></pre> <p>               Bases: <code>BaseRegressor</code></p> <p>A regression model from SMT.</p> <p>Note</p> <p>SMT is an open-source Python package consisting of libraries of surrogate modeling methods, sampling methods, and benchmarking problems. Read this page for the list of surrogate models and options.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>IODataset</code>)           \u2013            <p>The learning dataset.</p> </li> <li> <code>model_class_name</code>               (<code>SMTSurrogateModel</code>)           \u2013            <p>The class name of a surrogate model available in SMT, i.e. a subclass of <code>smt.surrogate_models.surrogate_model.SurrogateModel</code>.</p> </li> <li> <code>transformer</code>               (<code>TransformerType</code>, default:                   <code>IDENTITY</code> )           \u2013            <p>The strategies to transform the variables. The values are instances of :class:<code>.BaseTransformer</code> while the keys are the names of either the variables or the groups of variables, e.g. <code>\"inputs\"</code> or <code>\"outputs\"</code> in the case of the regression algorithms. If a group is specified, the :class:<code>.BaseTransformer</code> will be applied to all the variables of this group. If :attr:<code>.IDENTITY</code>, do not transform the variables.</p> </li> <li> <code>input_names</code>               (<code>Iterable[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names of the input variables. If <code>None</code>, consider all the input variables of the learning dataset.</p> </li> <li> <code>output_names</code>               (<code>Iterable[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names of the output variables. If <code>None</code>, consider all the output variables of the learning dataset.</p> </li> <li> <code>**model_options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options of the surrogate model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When both the variable and the group it belongs to have a transformer.</p> </li> </ul> Source code in <code>src/gemseo_mlearning/regression/smt_regression_model.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    data: IODataset,\n    model_class_name: SMTSurrogateModel,\n    transformer: TransformerType = BaseRegressor.IDENTITY,\n    input_names: Iterable[str] = (),\n    output_names: Iterable[str] = (),\n    **model_options: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        model_class_name: The class name of a surrogate model available in SMT,\n            i.e. a subclass of\n            ``smt.surrogate_models.surrogate_model.SurrogateModel``.\n        **model_options: The options of the surrogate model.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        data,\n        transformer=transformer,\n        input_names=input_names,\n        output_names=output_names,\n        function=RBFRegressor.Function.THIN_PLATE,\n        **model_options,\n    )\n    _model_options = {\"print_global\": False}\n    _model_options.update(model_options)\n    self.__smt_model = _NAMES_TO_CLASSES[model_class_name](**_model_options)\n</code></pre>"},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":""}]}