{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Surrogate-based optimization using in-house features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom gemseo import configure_logger\nfrom gemseo import execute_algo\nfrom gemseo.post.dataset.zvsxy import ZvsXY\nfrom gemseo.problems.dataset.rosenbrock import create_rosenbrock_dataset\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nconfigure_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example,\nwe seek to minimize the Rosenbrock function $f(x,y)=(1-x)^2+100(y-x^2)^2$\nover the design space $[-2,2]^2$.\nFirst,\nwe instantiate the problem with $(0, 0)$ as initial guess:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = Rosenbrock()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe minimize the Rosenbrock function using:\n\n- the `\"SBO\"` algorithm,\n- a maximum number of 40 evaluations,\n  including the initial one at the center of the design space\n  (this first point is common to all optimization algorithms)\n  and the initial training dataset,\n- its default settings,\n  namely:\n\n  - the expected improvement as acquisition criterion,\n  - 1 point acquired at a time,\n  - the `GaussianProcessRegressor` based on scikit-learn,\n  - 10 initial training points\n    based on an optimized latin hypercube sampling (LHS) technique,\n  - a multi-start local optimization of the acquisition criterion\n    from 50 start points with a limit of 20 iterations per local optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "execute_algo(problem, \"SBO\", max_iter=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see\nthat the solution is close to the theoretical one $(x^*,f^*)=((1,1),0)$.\n\nWe can also visualize all the evaluations\nand note that most of the points have been added in the valley as expected:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimization_history = problem.to_dataset()\n\ninitial_point = optimization_history[0:1]\ninitial_point.name = \"Initial point\"\n\ninitial_training_points = optimization_history[1:12]\ninitial_training_points.name = \"Initial training points\"\n\nacquired_points = optimization_history[12:]\nacquired_points.name = \"Acquired points\"\n\nvisualization = ZvsXY(\n    create_rosenbrock_dataset(900),\n    (\"x\", 0),\n    (\"x\", 1),\n    \"rosen\",\n    fill=False,\n    other_datasets=(initial_point, initial_training_points, acquired_points),\n)\nvisualization.execute(save=False, show=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly,\nwe can compare the solution to the one obtained with COBYLA,\nwhich is another popular gradient-free optimization algorithm:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "execute_algo(Rosenbrock(), \"NLOPT_COBYLA\", max_iter=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and conclude that for this problem and this initial guess,\nthe surrogate-based algorithm is better than COBYLA.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}