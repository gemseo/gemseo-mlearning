{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross-validation vs bootstrap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom numpy import array\nfrom numpy import cos\nfrom numpy import linspace\n\nfrom gemseo_mlearning.active_learning.acquisition_criteria.expected_improvement import (\n    ExpectedImprovement,\n)\nfrom gemseo_mlearning.active_learning.acquisition_criteria.mean_sigma import MeanSigma\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\n\nn_test = 200\nx_l = -3.0\nx_u = 3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initial learning dataset\n------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f(x):\n    return (10 * cos(2 * x) + 15 - 5 * x + x**2) / 50\n\n\nx_train = array([-2.4, -1.2, 0.0, 1.2, 2.4])\ny_train = f(x_train)\n\ndataset = IODataset()\ndataset.add_input_variable(\"x\", x_train)\ndataset.add_output_variable(\"y\", y_train)\n\nax = [[None, None], [None, None]]\nax[0][0] = plt.subplot(221)\nax[0][1] = plt.subplot(222, sharey=ax[0][0])\nax[1][0] = plt.subplot(223)\nax[1][1] = plt.subplot(224, sharey=ax[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Active learning\n---------------\nWe compare the bootstrap and leave-one-out methods\nin search of a new point to learn\nin order to estimate the minimum of the discipline\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, bootstrap in enumerate([False, True]):\n    # Train a RBF regression model\n    algo = RBFRegressor(dataset)\n    algo.learn()\n\n    # Build a regressor distribution\n    distribution = RegressorDistribution(algo, bootstrap=bootstrap, loo=not bootstrap)\n    distribution.learn()\n\n    # Define the expected improvement measure\n    ego = ExpectedImprovement(distribution)\n\n    # Define confidence bounds, equal to mean +/- 2*sigma\n    lower = MeanSigma(distribution, -2.0)\n    upper = MeanSigma(distribution, 2.0)\n\n    # Define the input_space\n    input_space = DesignSpace()\n    input_space.add_variable(\"x\", l_b=x_l, u_b=x_u, value=1.5)\n\n    # Define the data acquisition process\n    acquisition = ActiveLearningAlgo(\"ExpectedImprovement\", input_space, distribution)\n    acquisition.set_acquisition_algorithm(\"fullfact\")\n\n    # Compute the next input data\n    opt = acquisition.compute_next_input_data()\n\n    # Plot the results\n    x_test = linspace(x_l, x_u, n_test)\n    ego_data = []\n    surr_data = []\n    lower_data = []\n    upper_data = []\n    y_test = f(x_test)\n    for x_i in x_test:\n        surr_data.append(algo.predict(array([x_i]))[0])\n        ego_data.append(ego(array([x_i]))[0])\n        lower_data.append(lower(array([x_i]))[0] * lower.output_range)\n        upper_data.append(upper(array([x_i]))[0] * upper.output_range)\n\n    disc_data = IODataset()\n    disc_data.add_input_variable(\"x\", x_test)\n    disc_data.add_output_variable(\"y\", y_test)\n\n    for algo in distribution.algos:\n        algo_data = [algo.predict(array([x_i])) for x_i in x_test]\n        ax[0][index].plot(x_test, algo_data, \"gray\", alpha=0.2)\n\n    ax[0][index].plot(\n        x_train, dataset.get_view(variable_names=\"y\").to_numpy(), \"ro\", label=\"training\"\n    )\n    ax[0][index].plot(\n        x_test, disc_data.get_view(variable_names=\"y\").to_numpy(), \"r\", label=\"original\"\n    )\n    ax[0][index].plot(x_test, surr_data, \"b\", label=\"surrogate\")\n    ax[0][index].fill_between(\n        x_test,\n        array(lower_data),\n        array(upper_data),\n        color=\"b\",\n        alpha=0.1,\n        label=\"CI(95%)\",\n    )\n    ax[0][index].legend(loc=\"upper right\")\n    ax[0][index].axvline(x=opt[0])\n    ax[1][index].plot(x_test, array(ego_data), \"r\", label=\"EGO\")\n    ax[1][index].axvline(x=opt[0])\n    ax[1][index].legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}