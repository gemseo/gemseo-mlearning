{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch acquisition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configure\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import argmin\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script (use configure() with care)\nconfigure(False, False, True, False, False, False, False)\n\nconfigure_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The use of active learning methods\ndedicated to optimization\nis illustrated in this example.\nMore specifically,\nwe aim to test here\nthe impact of batch acquisition,\nthat is to say when several points\nare added at each method iteration,\non the active learning procedure.\nThe function to minimize is\nthe Rosenbrock function $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discipline = RosenbrockDiscipline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "with $x_1$ and $x_2$ belonging to $[-2,2]^2$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_space = RosenbrockSpace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First,\nwe create an initial training dataset using an optimal LHS including 10 samples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", \"OT_OPT_LHS\", n_samples=10\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and two identical initial\nGaussian process regressors from OpenTURNS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "regressor_1 = OTGaussianProcessRegressor(\n    learning_dataset, trend=\"quadratic\", multi_start_n_samples=20\n)\nregressor_2 = OTGaussianProcessRegressor(\n    learning_dataset, trend=\"quadratic\", multi_start_n_samples=20\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe build two active learning algorithms\nto test the impact of batch acquisition,\nthat is to say when several points\nare added at each method iteration,\non the active learning procedure.\nWe consider one algorithm with\nsamples added one by one\nalso called sequential acquisition (default),\nand in the second one,\nfour samples instead of one\nare added at each iteration (parallel acquisition),\nusing argument `batch_size` set to 4.\nNote that here,\nthis feature is only available\nwith the expected improvement criterion.\nAll other settings are put to\ntheir default values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "active_learning_1 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Minimum\", input_space, regressor_2, batch_size=4\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To study the results,\nwe extract first\nthe data associated\nto the history\nof the quantity of interest\nfor both active learning procedures\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"Sequential\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"Parallel\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Minimum\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also\ncompare the estimated optimas\nfrom the active learning procedures\nto their exact counterparts\nfor both algorithms\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally,\nfor both active learning algorithms,\nwe plot the training points,\nalongside the original model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", \"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the exact minimum and the estimated minima\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter([1], [1], marker=\"o\", label=\"Exact minimum\", color=\"red\")\nplt.scatter(\n    points_1[argmin(points_1[:, -1]), 0],\n    points_1[argmin(points_1[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with sequential acquisition\",\n    color=\"tab:blue\",\n)\nplt.scatter(\n    points_2[argmin(points_2[:, -1]), 0],\n    points_2[argmin(points_2[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with parallel acquisition\",\n    color=\"tab:orange\",\n)\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with sequential acquisition\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with parallel acquisition\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}