{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Acquisition criterion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configure\nfrom gemseo import configure_logger\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script (use configure() with care)\nconfigure(False, False, True, False, False, False, False)\n\nconfigure_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The use of active learning methods\ndedicated to exploration\nis illustrated in this example.\nMore specifically,\nwe aim to test here\nthe impact of the choice\nof the acquisition criterion\nused to enrich\nthe dataset\non the active learning procedure.\nThe function to approximate is\nthe Rosenbrock function $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discipline = RosenbrockDiscipline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "with $x_1$ and $x_2$ belonging to $[-2,2]^2$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_space = RosenbrockSpace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First,\nwe create an initial training dataset using an optimal LHS including 10 samples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and three identical initial\nGaussian process regressors from OpenTURNS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_3 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe build three active learning algorithms\nto test the impact of the choice\nof the acquisition criterion\non the active learning procedure.\nThey respectively refer to the\nvariance (default),\nstandard deviation\nand distance criteria.\nAll other settings are put to\ntheir default values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "active_learning_1 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Exploration\", input_space, regressor_2, criterion_name=\"StandardDeviation\"\n)\nactive_learning_3 = ActiveLearningAlgo(\n    \"Exploration\", input_space, regressor_3, criterion_name=\"Distance\"\n)\n\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\nactive_learning_3.acquire_new_points(discipline, n_samples=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally,\nfor the three active learning algorithms,\nwe plot the training points,\nalongside the original model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points.\nplt.figure()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\npoints_3 = active_learning_3.regressor.learning_set.to_numpy()\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"Variance\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"Standard deviation\")\nplt.scatter(points_3[:, 0], points_3[:, 1], marker=\"*\", label=\"Distance\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this implementation,\nVariance and standard deviation\ncriteria provide the same\nlearning points,\nwhich is expected because\nthey are identical up to\na square transformation.\nOn the contrary,\nlearning points from the distance criterion\nare much different\nand uniformly fills the input space.\nAll 3 methods provide a\nsurrogate with good accuracy\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_test = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=50\n)\nR2_1 = R2Measure(active_learning_1.regressor).compute_test_measure(dataset_test)\nR2_2 = R2Measure(active_learning_2.regressor).compute_test_measure(dataset_test)\nR2_3 = R2Measure(active_learning_3.regressor).compute_test_measure(dataset_test)\nR2_1[0], R2_2[0], R2_3[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}