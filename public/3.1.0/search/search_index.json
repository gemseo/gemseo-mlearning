{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-mlearning","title":"gemseo-mlearning","text":""},{"location":"#overview","title":"Overview","text":"<p><code>gemseo-mlearning</code> is a plugin of the library GEMSEO, dedicated to machine learning.</p> <p>This package adds new regression models and optimization algorithms based on SMT.</p> <p>A package for active learning is also available, deeply based on the core GEMSEO objects for optimization, as well as a SurrogateBasedOptimization library built on its top. An effort is being made to improve both content and performance in future versions.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version with <code>pip install gemseo-mlearning</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Antoine Dechaume</li> <li>Beno\u00eet Pauwels</li> <li>Cl\u00e9ment Laboulfie</li> <li>Matthias De Lozzo</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#version-310-october-2025","title":"Version 3.1.0 (October 2025)","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>UCB   and   LCB   can now acquire learning points by batch   with <code>batch_size</code> argument,   when the regressor is based on a random process   such as <code>GaussianProcessRegressor</code>   and <code>OTGaussianProcessRegressor</code>.</li> <li>Support for Python 3.13.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Support for Python 3.9.</li> </ul>"},{"location":"changelog/#version-300-august-2025","title":"Version 3.0.0 (August 2025)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>gemseo_mlearning.settings.opt: the Pydantic models to define the settings of the optimization algorithms.</li> <li>gemseo_mlearning.settings.mlearning: the Pydantic models to define the settings of the machine learning algorithms.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li><code>SBOSettings</code> renamed to <code>SBO_Settings</code>.</li> <li><code>SMTEGOSettings</code> renamed to <code>SMT_EGO_Settings</code>.</li> <li><code>SMTRegressorSettings</code> renamed to <code>SMT_Regressor_Settings</code>.</li> </ul>"},{"location":"changelog/#version-201-april-2025","title":"Version 2.0.1 (April 2025)","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>The optimization algorithm <code>SMT_EGO</code> correctly uses the option <code>normalize_design_space</code>.</li> <li>The method   ActiveLearningAlgo.acquire_new_points   works when the discipline it uses has output variables   that are not outputs of the regression model   used by ActiveLearningAlgo</li> </ul>"},{"location":"changelog/#version-200-november-2024","title":"Version 2.0.0 (November 2024)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Support GEMSEO v6.</li> <li>Support for Python 3.12.</li> <li>ActiveLearningAlgo   can acquire points by batch using the <code>batch_size</code>   and <code>mc_size</code> argument,   when the regressor is based on a random process   such as <code>GaussianProcessRegressor</code>   and <code>OTGaussianProcessRegressor</code>.   This option is only available for criteria dedicated   to level set LevelSet   (alternatively quantile estimation Quantile)   and the expected improvement for maximum/minimum estimation Maximum.</li> <li>The Branin and   Rosenbrock problems   can be used to benchmark   the efficiency of the active learning algorithms   and estimate several quantities of interest,   optimas or quantiles for instance.</li> <li>AcquisitionView   can be used to plot   both the output of the original model,   the prediction of the surrogate model,   the standard deviation of the surrogate model   and the acquisition criterion,   when the input dimension is 1 or 2.</li> <li>SMTRegressor   can be any surrogate model available in the Python package SMT.</li> <li><code>\"SMT_EGO\"</code> is the name of the expected global optimization (EGO) algorithm   wrapping the surrogate-based optimizers available in the Python package SMT;   this is the unique algorithm of the SMTEGO optimization library.</li> <li>SurrogateBasedOptimization   can use the acquisition criteria <code>CB</code> and <code>Mean</code> in addition to <code>EI</code>.</li> <li>SurrogateBasedOptimization   can use an existing BaseRegressor   and save the BaseRegressor that it enriches   using the <code>regression_file_path</code> option.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>BREAKING CHANGE: The acquisition algorithm settings have to be passed to   SurrogateBasedOptimizer   as keyword arguments, i.e. <code>SurrogateBasedOptimizer(..., key_1=value_1, key_2=value_2, ...)</code>.</li> <li>BREAKING CHANGE: The term <code>option</code> has been replaced by <code>setting</code> when it was linked to a DOE or an optimization algorithm.</li> <li>BREAKING CHANGE: The argument <code>distribution</code> of   ActiveLearningAlgo.   renamed to <code>regressor</code>;   it can be either a</li> <li>BaseRegressor   or a   BaseRegressorDistribution.</li> <li>BREAKING CHANGE: The method <code>compute_next_input_data</code> of   ActiveLearningAlgo.   renamed to   find_next_point.</li> <li>BREAKING CHANGE: The method <code>update_algo</code> of   ActiveLearningAlgo.   renamed to   acquire_new_points.</li> <li>BREAKING CHANGE: <code>MaxExpectedImprovement</code> renamed to <code>`Maximum</code>.</li> <li>BREAKING CHANGE: <code>MinExpectedImprovement</code> renamed to <code>`Minimum</code>.</li> <li>BREAKING CHANGE: <code>ExpectedImprovement</code> removed.</li> <li>BREAKING CHANGE: the acquisition criterion <code>LimitState</code> renamed to   LevelSet</li> <li>BREAKING CHANGE: each acquisition criterion class has a specific module   in gemseo_mlearning.active_learning.acquisition_criteria   whose name is the snake-case version of it class name, i.e. <code>nice_criterion.py</code> contains <code>NiceCriterion</code>.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.adaptive.distribution.MLRegressorDistribution</code> renamed to   gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.lib_surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.surrogate_based_optimization.</li> <li>BREAKING CHANGE: <code>gemseo_mlearning.algos.opt.core.surrogate_based</code> renamed to   gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.</li> <li>BREAKING CHANGE: <code>MLDataAcquisition</code> renamed to   ActiveLearningAlgo.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterion</code> renamed to   BaseAcquisitionCriterion   and moved to</li> <li>gemseo_mlearning.active_learning.acquisition_criteria.</li> <li>BREAKING CHANGE: <code>MLDataAcquisitionCriterionFactory</code> renamed to   BaseAcquisitionCriterionFactory,   moved to   gemseo_mlearning.active_learning.acquisition_criteria   and without the property <code>available_criteria</code> (use <code>BaseAcquisitionCriterionFactory.class_names</code>).</li> <li>BREAKING CHANGE: <code>gemseo.adaptive</code> renamed to gemseo_mlearning.active_learning.</li> <li>BREAKING CHANGE: <code>gemseo.adaptive.criteria</code> renamed to   gemseo_mlearning.active_learning.acquisition_criteria.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>The data transformer can be set with the <code>\"transformer\"</code> key of the <code>regression_options</code> dictionary   passed to SurrogateBasedOptimization.</li> <li>The Quantile   estimates the quantile by Monte Carlo sampling   by means of the probability distributions of the input variables;   these distributions are defined with its new argument <code>uncertain_space</code>.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li><code>AcquisitionCriterionFactory</code>; replaced by different factories for the developers.</li> <li><code>sample_discipline</code>; use sample_disciplines from <code>gemseo</code> instead.</li> <li><code>sample_disciplines</code>; move to <code>gemseo</code>: sample_disciplines.</li> <li><code>MAEMeasure</code>; moved to <code>gemseo</code>: MAEMeasure.</li> <li><code>MEMeasure</code>; moved to <code>gemseo</code>: MEMeasure.</li> <li><code>GradientBoostingRegressor</code>; moved to <code>gemseo</code>: GradientBoostingRegressor.</li> <li><code>MLPRegressor</code>; moved to <code>gemseo</code>: MLPRegressor.</li> <li><code>OTGaussianProcessRegressor</code>; moved to <code>gemseo</code>: OTGaussianProcessRegressor.</li> <li><code>RegressorChain</code>; moved to <code>gemseo</code>: RegressorChain.</li> <li><code>SVMRegressor</code>; moved to <code>gemseo</code>: SVMRegressor.</li> <li><code>TPSRegressor</code>; moved to <code>gemseo</code>: TPSRegressor.</li> </ul>"},{"location":"changelog/#version-112-december-2023","title":"Version 1.1.2 (December 2023)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> <li><code>OTGaussianProcessRegressor</code> has a new optional argument <code>optimizer</code>   to select the OpenTURNS optimizer for the covariance model parameters.</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-111-september-2023","title":"Version 1.1.1 (September 2023)","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li><code>OTGaussianProcessRegressor.predict_std</code>   no longer returns the variance of the output but its standard deviation.</li> </ul>"},{"location":"changelog/#version-110-june-2023","title":"Version 1.1.0 (June 2023)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>An argument <code>trend_type</code> of type <code>OTGaussianProcessRegressor.TrendType</code> to <code>OTGaussianProcessRegressor</code>;   the trend type of the Gaussian process regressor can be either constant,   linear or quadratic.</li> <li>A new optimization library   SurrogateBasedOptimization   to perform EGO-like surrogate-based optimization on unconstrained problems.</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>The output of an <code>MLDataAcquisitionCriterion</code>   based on a regressor built from constant output values is no longer <code>nan</code>.</li> </ul>"},{"location":"changelog/#version-101-february-2022","title":"Version 1.0.1 (February 2022)","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>BaseRegressorDistribution   can now use a regression algorithm instantiated with transformers.</li> </ul>"},{"location":"changelog/#version-100-july-2022","title":"Version 1.0.0 (July 2022)","text":"<p>First release.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>These projects were used to build gemseo-mlearning. Thank you!</p> <p>Python | uv</p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License annotated-types Reusable constraint types to use with typing.Annotated <code>&gt;=0.6.0</code> <code>0.7.0</code> MIT License attrs Classes Without Boilerplate <code>&gt;=22.2.0</code> <code>25.4.0</code> MIT certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2025.10.5</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.4</code> MIT colorama Cross-platform colored terminal text. <code>&gt;=0.4</code> <code>0.4.6</code> BSD License contourpy Python library for calculating contours of 2D quadrilateral grids <code>&gt;=1.0.1</code> <code>1.3.3</code> BSD License cycler Composable style cycles <code>&gt;=0.10</code> <code>0.12.1</code> BSD License dill serialize all of Python <code>0.4.0</code> BSD-3-Clause docstring-inheritance Avoid writing and maintaining duplicated docstrings. <code>&gt;=2.0.0, &lt;=2.2.2</code> <code>2.2.2</code> MIT egobox <code>~=0.32</code> <code>0.32.0</code> Apache Software License et_xmlfile An implementation of lxml.xmlfile for the standard library <code>2.0.0</code> MIT fastjsonschema Fastest Python implementation of JSON schema <code>&gt;=2.14.5, &lt;=2.21.2</code> <code>2.21.2</code> BSD fonttools Tools to manipulate font files <code>&gt;=4.22.0</code> <code>4.60.1</code> MIT gemseo Generic Engine for Multi-disciplinary Scenarios, Exploration and Optimization <code>&gt;=6.3, &lt;7</code> <code>6.3.1.dev1+g31a6c94b6</code> LGPL-3.0 genson GenSON is a powerful, user-friendly JSON Schema generator. <code>&gt;=1.2.2, &lt;=1.3.0</code> <code>1.3.0</code> MIT graphviz Simple Python interface for Graphviz <code>&gt;=0.19, &lt;=0.21</code> <code>0.21</code> MIT h5py Read and write HDF5 files from Python <code>&gt;=3.6.0, &lt;=3.14.0</code> <code>3.14.0</code> BSD-3-Clause idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.11</code> BSD-3-Clause jenn Jacobian-Enhanced Neural Nets (JENN) <code>1.0.8</code> MIT License Jinja2 A very fast and expressive template engine. <code>&gt;=3.0.0, &gt;=2.11.1, &lt;=3.1.6</code> <code>3.1.6</code> BSD License joblib Lightweight pipelining with Python functions <code>&gt;=1.2.0</code> <code>1.5.2</code> BSD 3-Clause jsonpointer Identify specific nodes in a JSON document (RFC 6901) <code>&gt;=2.4</code> <code>3.0.0</code> Modified BSD License jsonschema An implementation of JSON Schema validation for Python <code>&gt;=4.22</code> <code>4.25.1</code> MIT jsonschema-specifications The JSON Schema meta-schemas and vocabularies, exposed as a Registry <code>&gt;=2023.03.6</code> <code>2025.9.1</code> MIT kiwisolver A fast implementation of the Cassowary constraint solver <code>&gt;=1.3.1</code> <code>1.4.9</code> BSD License MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0, &gt;=1.1</code> <code>3.0.3</code> BSD-3-Clause matplotlib Python plotting package <code>3.10.6</code> Python Software Foundation License MiniSom Minimalistic implementation of the Self Organizing Maps (SOM) <code>&gt;=2.3.5, &lt;2.4</code> <code>2.3.5</code> MIT mpmath Python library for arbitrary-precision floating-point arithmetic <code>&gt;=1.1.0, &lt;1.4</code> <code>1.3.0</code> BSD narwhals Extremely lightweight compatibility layer between dataframe libraries <code>&gt;=1.15.1</code> <code>2.8.0</code> MIT License networkx Python package for creating and manipulating graphs and networks <code>&gt;=2.5, &lt;=3.5</code> <code>3.5</code> BSD License nlopt Library for nonlinear optimization, wrapping many algorithms for global and local, constrained or unconstrained, optimization <code>&gt;=2.7.0, &lt;=2.9.1</code> <code>2.9.1</code> MIT numpy Fundamental package for array computing in Python <code>2.3.3</code> BSD License openpyxl A Python library to read/write Excel 2010 xlsx/xlsm files <code>&gt;=3.0.10, &lt;=3.1.5</code> <code>3.1.5</code> MIT openturns Uncertainty treatment library <code>&lt;1.25</code> <code>1.24</code> LGPL orjson Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy <code>&gt;=3.9</code> <code>3.11.3</code> Apache-2.0 OR MIT packaging Core utilities for Python packages <code>&gt;=20.5</code> <code>25.0</code> Apache Software License + BSD License pandas Powerful data structures for data analysis, time series, and statistics <code>&gt;=2.1.0, &lt;=2.3.2</code> <code>2.3.2</code> BSD License pillow Python Imaging Library (Fork) <code>&gt;=8</code> <code>11.3.0</code> MIT-CMU plotly An open-source interactive data visualization library for Python <code>&gt;=5.7.0, &lt;=6.3.0</code> <code>6.3.0</code> MIT License prettytable A simple Python library for easily displaying tabular data in a visually appealing ASCII table format <code>&gt;=2.3.0, &lt;=3.16.0</code> <code>3.16.0</code> BSD-3-Clause psutil Cross-platform lib for process and system monitoring. <code>7.1.0</code> BSD-3-Clause pydantic Data validation using Python type hints <code>&gt;=2.7, &lt;=2.11.9</code> <code>2.11.9</code> MIT pydantic-settings Settings management using Pydantic <code>&gt;=2.1.0, &lt;=2.11.0</code> <code>2.11.0</code> MIT pydantic_core Core functionality for Pydantic validation and serialization <code>==2.33.2</code> <code>2.33.2</code> MIT pyDOE3 Design of experiments for Python <code>1.4.0</code> BSD-3-Clause pyparsing pyparsing - Classes and methods to define and execute parsing grammars <code>&gt;=3.0, &gt;=2.3.1</code> <code>3.2.5</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.1, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License python-dotenv Read key-value pairs from a .env file and set them as environment variables <code>&gt;=0.21.0</code> <code>1.1.1</code> BSD-3-Clause pytz World timezone definitions, modern and historical <code>&gt;=2020.1, &gt;=2015.7</code> <code>2025.2</code> MIT pyXDSM Python script to generate PDF XDSM diagrams using TikZ and LaTeX <code>&gt;=2.2.1, &lt;=2.3.1</code> <code>2.3.1</code> Apache License Version 2.0 referencing JSON Referencing + Python <code>&gt;=0.28.4</code> <code>0.37.0</code> MIT requests Python HTTP for Humans. <code>&gt;=2.8.1, &gt;2, &lt;3</code> <code>2.32.5</code> Apache-2.0 rpds-py Python bindings to Rust's persistent data structures (rpds) <code>&gt;=0.7.1</code> <code>0.27.1</code> MIT scikit-learn A set of python modules for machine learning and data mining <code>1.7.2</code> BSD-3-Clause scipy Fundamental algorithms for scientific computing in Python <code>1.15.2</code> BSD License six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT smt The Surrogate Modeling Toolbox (SMT) <code>&gt;=2.5.0, &lt;=2.9.5</code> <code>2.9.5</code> BSD-3 spgl1 SPGL1: A solver for large-scale sparse reconstruction. <code>&gt;=0.0.3, &lt;=1.0.0</code> <code>0.0.3</code> GNU Lesser General Public License v3 (LGPLv3) StrEnum An Enum that inherits from str. <code>&gt;=0.4.9, &lt;=0.4.15</code> <code>0.4.15</code> MIT License sympy Computer algebra system (CAS) in Python <code>&gt;=1.5, &lt;=1.14.0</code> <code>1.14.0</code> BSD threadpoolctl threadpoolctl <code>&gt;=3.1.0</code> <code>3.6.0</code> BSD-3-Clause tqdm Fast, Extensible Progress Meter <code>&gt;=4.50, &lt;=4.67.1</code> <code>4.67.1</code> MPL-2.0 AND MIT typing-inspection Runtime typing introspection tools <code>&gt;=0.4.0</code> <code>0.4.2</code> MIT typing_extensions Backported and Experimental Type Hints for Python 3.9+ <code>&gt;=4.0, &gt;=4, &lt;5</code> <code>4.15.0</code> PSF-2.0 tzdata Provider of IANA time zone data <code>&gt;=2022.7</code> <code>2025.2</code> Apache-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.5.0</code> MIT wcwidth Measures the displayed width of unicode strings in a terminal <code>0.2.14</code> MIT xdsmjs XDSMjs Python module <code>&gt;=1.0.0, &lt;=2.0.0</code> <code>2.0.0</code> Apache License, Version 2.0 xxhash Python binding for xxHash <code>&gt;=3.0.0, &lt;=3.5.0</code> <code>3.5.0</code> BSD"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License babel Internationalization utilities <code>~=2.10</code> <code>2.17.0</code> BSD-3-Clause backrefs A wrapper around re and regex that adds additional back references. <code>~=5.7.post1</code> <code>5.9</code> MIT black The uncompromising code formatter. <code>25.9.0</code> MIT bracex Bash style brace expander. <code>&gt;=2.1.1</code> <code>2.6</code> MIT certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2025.10.5</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.4</code> MIT click Composable command line interface toolkit <code>&gt;=7.0</code> <code>8.3.0</code> BSD-3-Clause colorama Cross-platform colored terminal text. <code>&gt;=0.4</code> <code>0.4.6</code> BSD License fieldz Utilities for providing compatibility with many dataclass-like libraries <code>&gt;=0.1.0</code> <code>0.1.3</code> BSD-3-Clause ghp-import Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License griffe Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=1.13</code> <code>1.14.0</code> ISC griffe-fieldz Griffe extension adding support for data-class like things (pydantic, attrs, etc...) <code>0.3.0</code> BSD-3-Clause griffe-inherited-docstrings Griffe extension for inheriting docstrings. <code>1.1.2</code> ISC idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.11</code> BSD-3-Clause importlib_metadata Read metadata from Python packages <code>&gt;=4.6</code> <code>8.7.0</code> Apache Software License importlib_resources Read resources from Python packages <code>6.5.2</code> Apache Software License Jinja2 A very fast and expressive template engine. <code>&gt;=3.0.0, &gt;=2.11.1, &lt;=3.1.6</code> <code>3.1.6</code> BSD License latexcodec A lexer and codec to work with LaTeX code in Python. <code>&gt;=1.0.4</code> <code>3.0.1</code> MIT Markdown Python implementation of John Gruber's Markdown. <code>&gt;=3.6</code> <code>3.9</code> BSD-3-Clause markdown-exec Utilities to execute code blocks in Markdown files. <code>1.11.0</code> ISC MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0, &gt;=1.1</code> <code>3.0.3</code> BSD-3-Clause mergedeep A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License mike Manage multiple versions of your MkDocs-powered documentation <code>2.1.3</code> BSD-3-Clause mkdocs Project documentation with Markdown. <code>&gt;=1.2</code> <code>1.6.1</code> BSD-2-Clause mkdocs-autorefs Automatically link across pages in MkDocs. <code>&gt;=1.4</code> <code>1.4.3</code> ISC mkdocs-bibtex An MkDocs plugin that enables managing citations with BibTex <code>4.4.0</code> BSD-3-Clause-LBNL mkdocs-gallery a <code>mkdocs</code> plugin to generate example galleries from python scripts, similar to <code>sphinx-gallery</code>. <code>0.10.4</code> BSD 3-Clause mkdocs-gen-files MkDocs plugin to programmatically generate documentation pages during the build <code>0.5.0</code> MIT mkdocs-get-deps MkDocs extension that lists all dependencies according to a mkdocs.yml file <code>&gt;=0.2.0</code> <code>0.2.0</code> MIT mkdocs-include-markdown-plugin Mkdocs Markdown includer plugin. <code>7.2.0</code> Apache-2.0 mkdocs-literate-nav MkDocs plugin to specify the navigation in Markdown instead of YAML <code>0.6.2</code> MIT mkdocs-material Documentation that simply works <code>9.6.21</code> MIT mkdocs-material-extensions Extension pack for Python Markdown and MkDocs Material. <code>~=1.3</code> <code>1.3.1</code> MIT mkdocs-section-index MkDocs plugin to allow clickable sections that lead to an index page <code>0.3.10</code> MIT mkdocstrings Automatic documentation from sources, for MkDocs. <code>0.30.1</code> ISC mkdocstrings-python A Python handler for mkdocstrings. <code>&gt;=1.16.2</code> <code>1.18.2</code> ISC mypy_extensions Type system extensions for programs checked with the mypy type checker. <code>&gt;=0.4.3</code> <code>1.1.0</code> MIT packaging Core utilities for Python packages <code>&gt;=20.5</code> <code>25.0</code> Apache Software License + BSD License paginate Divides large result sets into pages for easier browsing <code>~=0.5</code> <code>0.5.7</code> MIT pathspec Utility library for gitignore style pattern matching of file paths. <code>&gt;=0.11.1</code> <code>0.12.1</code> Mozilla Public License 2.0 (MPL 2.0) platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. <code>&gt;=2.2.0</code> <code>4.5.0</code> MIT pybtex A BibTeX-compatible bibliography processor in Python <code>&gt;=0.22</code> <code>0.25.1</code> MIT Pygments Pygments is a syntax highlighting package written in Python. <code>~=2.16</code> <code>2.19.2</code> BSD-2-Clause pygments-ansi-color <code>&gt;=0.3</code> <code>0.3.0</code> Apache Software License pymdown-extensions Extension pack for Python Markdown. <code>&gt;=6.3</code> <code>10.16.1</code> MIT pypandoc Thin wrapper for pandoc. <code>&gt;=1.5</code> <code>1.15</code> MIT pyparsing pyparsing - Classes and methods to define and execute parsing grammars <code>&gt;=3.0, &gt;=2.3.1</code> <code>3.2.5</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.1, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytokens A Fast, spec compliant Python 3.12+ tokenizer that runs on older Pythons. <code>&gt;=0.1.10</code> <code>0.1.10</code> MIT pytz World timezone definitions, modern and historical <code>&gt;=2020.1, &gt;=2015.7</code> <code>2025.2</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.3</code> MIT pyyaml_env_tag A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>1.1</code> MIT requests Python HTTP for Humans. <code>&gt;=2.8.1, &gt;2, &lt;3</code> <code>2.32.5</code> Apache-2.0 responses A utility library for mocking out the <code>requests</code> Python library. <code>&gt;=0.25.6</code> <code>0.25.8</code> Apache 2.0 setuptools Easily download, build, install, upgrade, and uninstall Python packages <code>&gt;=68.0.0</code> <code>80.9.0</code> MIT six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT tqdm Fast, Extensible Progress Meter <code>&gt;=4.50, &lt;=4.67.1</code> <code>4.67.1</code> MPL-2.0 AND MIT typing_extensions Backported and Experimental Type Hints for Python 3.9+ <code>&gt;=4.0, &gt;=4, &lt;5</code> <code>4.15.0</code> PSF-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.5.0</code> MIT validators Python Data Validation for Humans\u2122 <code>&gt;=0.19.0</code> <code>0.35.0</code> MIT verspec Flexible version handling <code>0.1.0</code> BSD 2-Clause or Apache-2.0 watchdog Filesystem events monitoring <code>&gt;=2.0</code> <code>6.0.0</code> Apache-2.0 wcmatch Wildcard/glob file name matcher. <code>10.1</code> MIT zipp Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.20</code> <code>3.23.0</code> MIT"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-mlearning</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-mlearning</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-mlearning</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"developer_guide/active_learning/","title":"Active learning","text":""},{"location":"developer_guide/active_learning/#active-learning","title":"Active learning","text":"<p>This section describes the design of the active_learning subpackage.</p> <p>Info</p> <p>Open the user guide for general information, e.g. concepts, API, examples, etc.</p>"},{"location":"developer_guide/active_learning/#tree-structure","title":"Tree structure","text":"<pre><code>\ud83d\udcc1 gemseo_mlearning\n\u2514\u2500\u2500 \ud83d\udcc1 active_learning # Subpackage for active learning (AL)\n    \u251c\u2500\u2500 \ud83d\udcc4 active_learning_algo.py # Class to set and solve an AL problem\n    \u251c\u2500\u2500 \ud83d\udcc1 acquisition_criteria # Acquisition criteria (ACs)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 exploration # ACs to improve the regressor\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_exploration.py # Base class for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 distance.py # AC (distance to the learning set)\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 exploration.py # Specific AC family\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 factory.py # Factory for these ACs\n    \u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 standard_deviation.py # An AC (regressor's standard deviation)\n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 variance.py # AC (regressor's variance)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 level_set/ # ACs to approximate a level set\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 maximum/ # ACs to approximate the global maximum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 minimum/ # ACs to approximate the global minimum\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1 quantile/ # ACs to approximate a quantile\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion.py # Base class for ACs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_acquisition_criterion_family.py # Base class for AC families\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 base_factory.py # Base class for AC\n    \u251c\u2500\u2500 \ud83d\udcc1 distributions # Regressor distributions (RD)\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 base_regressor_distribution.py # The base class for RDs\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 kriging_distribution.py # The RD for Kriging regressors\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 regressor_distribution.py # The RD for any regressor\n    \u2514\u2500\u2500 \ud83d\udcc1 visualization # Visualization tools\n        \u251c\u2500\u2500 \ud83d\udcc4 acquisition_view.py # Plot the acquisition process (in 2D only)\n        \u2514\u2500\u2500 \ud83d\udcc4 qoi_history_view.py # Plot the history of the quantity of interest\n</code></pre>"},{"location":"developer_guide/active_learning/#class-diagram","title":"Class diagram","text":"<pre><code>classDiagram\n\n    ActiveLearningAlgo *-- OptimizationProblem\n    ActiveLearningAlgo o-- DesignSpace\n    ActiveLearningAlgo --&gt; AcquisitionCriterionFamilyFactory: criterion_family_name\n    ActiveLearningAlgo --&gt; BaseAcquisitionCriterionFactory: criterion_name \\n criterion_options\n    ActiveLearningAlgo --&gt; BaseRegressorDistribution: regressor\n    BaseDriverLibrary --* ActiveLearningAlgo\n    BaseDOELibrary --* BaseDriverLibrary\n    BaseOptimizationLibrary --* BaseDriverLibrary\n    Database --* ActiveLearningAlgo\n    AcquisitionView --* ActiveLearningAlgo\n    QOIHistoryView --* ActiveLearningAlgo\n\n    OptimizationProblem o-- BaseAcquisitionCriterion\n\n    AcquisitionCriterionFamilyFactory --&gt; BaseAcquisitionCriterionFamily\n    BaseAcquisitionCriterionFamily --&gt; BaseAcquisitionCriterionFactory\n    BaseAcquisitionCriterionFactory --&gt; BaseAcquisitionCriterion\n\n    BaseAcquisitionCriterion --|&gt; MDOFunction\n    BaseAcquisitionCriterion o-- BaseRegressorDistribution\n\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterion\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFactory\n    &lt;&lt;abstract&gt;&gt; BaseAcquisitionCriterionFamily\n    &lt;&lt;abstract&gt;&gt; BaseRegressorDistribution\n\n    class ActiveLearningAlgo {\n        +default_algo_name\n        +default_doe_settings\n        +default_opt_settings\n        +acquisition_criterion\n        +input_space\n        +n_initial_samples\n        +qoi\n        +qoi_history\n        +regressor\n        +regressor_distribution\n        +set_acquisition_algorithm()\n        +find_nex_point()\n        +acquire_new_points()\n        +plot_acquisition_view()\n        +plot_qoi_history()\n        +update_problem()\n    }\n\n    class AcquisitionCriterionFamilyFactory {\n        +create()\n    }\n\n    class BaseAcquisitionCriterionFamily {\n        +ACQUISITION_CRITERION_FACTORY\n    }\n\n    class BaseAcquisitionCriterionFactory {\n        #DEFAULT_CLASS_NAME\n    }</code></pre>"},{"location":"developer_guide/active_learning/#how-to","title":"How to...","text":"<code>... create a new family of acquisition criteria?</code> <ol> <li>Derive an abstract class <code>BaseNewAcquisitionCriterion</code> from    BaseAcquisitionCriterion.</li> <li>Derive <code>FirstAcquisitionCriterion</code>, <code>SecondAcquisitionCriterion</code>, ... from <code>BaseNewAcquisitionCriterion</code>    in modules located in <code>root_package_name.subpackage_name.package_name</code>.</li> <li>Derive <code>NewAcquisitionCriterionFactory</code> from    BaseAcquisitionCriterionFactory    and set the class attributes:    <pre><code>_CLASS = BaseNewAcquisitionCriterion\n_DEFAULT_CLASS_NAME = \"FirstAcquisitionCriterion\"\n_MODULE_NAMES = (\"root_package.subpackage_name.package_name\",)\n</code></pre></li> <li>Derive <code>NewAcquisitionCriterionFamily</code> from    BaseAcquisitionCriterionFamily    and set the class attribute <code>ACQUISITION_CRITERION_FACTORY = NewAcquisitionCriterionFactory</code>.</li> </ol> <p>Now, the user can instantiate the ActiveLearningAlgo</p> <ul> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   to use this family of acquisition criteria,</li> <li>with <code>criterion_family_name=\"NewAcquisitionCriterionFamily\"</code>   and <code>criterion_name=\"SecondAcquisitionCriterion\"</code>   to use a specific acquisition criterion from this family.</li> </ul>"},{"location":"generated/examples/active_learning/","title":"Active learning","text":""},{"location":"generated/examples/active_learning/#active-learning","title":"Active learning","text":""},{"location":"generated/examples/active_learning/#active-learning-and-exploration","title":"Active learning and exploration","text":"<p>These examples illustrate the use of active learning methods dedicated to exploration, either by using the default settings or by modifying the functionality indicated in the title.</p> <p> Non-GP regressor. </p> <p> Default settings. </p> <p> GP regressor. </p> <p> Acquisition algorithm. </p> <p> Acquisition criterion. </p>"},{"location":"generated/examples/active_learning/#active-learning-and-level-set-estimation","title":"Active learning and level set estimation","text":"<p>These examples illustrate the use of active learning methods dedicated to level set estimation, either by using the default settings or by modifying the functionality indicated in the title.</p> <p> Default settings. </p> <p> Non-GP regressor. </p> <p> Batch acquisition. </p> <p> GP regressor. </p> <p> Acquisition algorithm. </p> <p> Acquisition criterion. </p>"},{"location":"generated/examples/active_learning/#active-learning-and-optimization","title":"Active learning and optimization","text":"<p>These examples illustrate the use of active learning methods dedicated to optimization, either by using the default settings or by modifying the functionality indicated in the title.</p> <p> Default settings. </p> <p> Non-GP regressor. </p> <p> GP regressor. </p> <p> Batch acquisition. </p> <p> Acquisition algorithm. </p> <p> Acquisition criterion. </p>"},{"location":"generated/examples/active_learning/#active-learning-and-quantile-estimation","title":"Active learning and quantile estimation","text":"<p>These examples illustrate the use of active learning methods dedicated to quantile estimation, either by using the default settings or by modifying the functionality indicated in the title.</p> <p> Default settings. </p> <p> Non-GP regressor. </p> <p> GP regressor. </p> <p> Batch acquisition. </p> <p> Acquisition algorithm. </p> <p> Acquisition criterion. </p> <p> Download all examples in Python source code: active_learning_python.zip</p> <p> Download all examples in Jupyter notebooks: active_learning_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/exploration/exploration_rbf/","title":"Non-GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/exploration/exploration_rbf/#non-gp-regressor","title":"Non-GP regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to exploration is illustrated in this example, with all default settings. The function to approximate is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and a universal regressor, namely a radial basis function network based on SciPy:</p> <pre><code>regressor = RBFRegressor(learning_dataset)\nregressor_distribution = RegressorDistribution(regressor, bootstrap=False, size=50)\nregressor_distribution.learn()\n</code></pre> <p>Then, we look for 20 points that will help us to improve the overall accuracy of the surrogate model. By default, for this purpose, the active learning algorithm looks for the point maximizing the regressor local variance with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Exploration\", input_space, regressor_distribution)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Lastly, we plot the training points, the original model, the RBF regressor and the variance after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: exploration_rbf.py</p> <p> Download Jupyter notebook: exploration_rbf.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/exploration/mg_execution_times/","title":"Computation times","text":"<p>00:54.009 total execution time for generated_examples_active_learning_exploration files:</p> <p>+--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_exploration_acquisition_algo (docs/examples/active_learning/exploration/plot_exploration_acquisition_algo.py) | 00:18.063 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_exploration_criterion (docs/examples/active_learning/exploration/plot_exploration_criterion.py)                      | 00:16.596 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_exploration_gp (docs/examples/active_learning/exploration/plot_exploration_gp.py)                                           | 00:12.629 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_exploration (docs/examples/active_learning/exploration/plot_exploration.py)                                                    | 00:06.721 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | exploration_rbf (docs/examples/active_learning/exploration/exploration_rbf.py)                                                       | 00:00.000 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration/","title":"Default settings.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration/#default-settings","title":"Default settings.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to exploration is illustrated in this example, with all default settings. The function to approximate is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and an initial Gaussian process regressor from OpenTURNS:</p> <pre><code>regressor = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we look for 20 points that will help us to improve the overall accuracy of the surrogate model. By default, for this purpose, the active learning algorithm looks for the point maximizing the regressor local variance with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Exploration\", input_space, regressor)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Out:</p> <pre><code>&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9cd117ce0&gt;, Optimization problem:\n   minimize -Variance\n   with respect to x)\n</code></pre> <p>Lastly, we plot the training points, the original model, the Gaussian process regressor and the variance after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 7 Axes&gt;\n</code></pre> <p>The infill criterion targets areas where the variance is the highest. This occurs in particular on the borders of the design space. The learning points are not uniformly distributed within the input space. Overall, the target surrogate reproduces faithfully the Rosenbrock function with a good R\u00b2</p> <pre><code>dataset_test = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=50\n)\nR2 = R2Measure(active_learning.regressor).compute_test_measure(dataset_test)\nR2\n# Caution:\n# the variance is scaled and thus\n# does not here directly corresponds\n# to the square of the standard deviation.\n</code></pre> <p>Out:</p> <pre><code>array([0.99995626])\n</code></pre> <p>Total running time of the script: ( 0 minutes  6.721 seconds)</p> <p> Download Python source code: plot_exploration.py</p> <p> Download Jupyter notebook: plot_exploration.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_acquisition_algo/","title":"Acquisition algorithm.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_acquisition_algo/#acquisition-algorithm","title":"Acquisition algorithm.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to exploration is illustrated in this example. More specifically, we aim to test here the impact of the choice of the optimization algorithm used to find the next acquisition point on the active learning procedure. The function to approximate is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of the acquisition algorithm used to optimize the acquisition criterion on the active learning procedure. One uses the SLSQP gradient-based routine in a multistart fashion (default) for the optimization of the acquisition criterion, and the second the NELDER-MEAD gradient-free algorithm, also in a multistart fashion. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_2)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.set_acquisition_algorithm(\n    algo_name=\"MultiStart\", opt_algo_name=\"NELDER-MEAD\", n_start=20\n)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>(&lt;gemseo.algos.database.Database object at 0x7fc9cc993a50&gt;, Optimization problem:\n   minimize -Variance\n   with respect to x)\n</code></pre> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points.\nplt.figure()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"SLSQP\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"NELDER-MEAD\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.suptitle(\"Learning points\")\nplt.show()\n</code></pre> <p></p> <p>In this implementation, changing the acquisition algorithm, does not seem to significantly alter the learning procedure, though some learning points acquired with the gradient-based algorithm deviate from those acquired with the gradient. This does not result into a poorer accuracy as suggests the R2 validation metric:</p> <pre><code>dataset_test = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=50\n)\nR2_1 = R2Measure(active_learning_1.regressor).compute_test_measure(dataset_test)\nR2_2 = R2Measure(active_learning_2.regressor).compute_test_measure(dataset_test)\nR2_1[0], R2_2[0]\n</code></pre> <p>Out:</p> <pre><code>(np.float64(0.9999562597980046), np.float64(0.9999749112321269))\n</code></pre> <p>Total running time of the script: ( 0 minutes  18.063 seconds)</p> <p> Download Python source code: plot_exploration_acquisition_algo.py</p> <p> Download Jupyter notebook: plot_exploration_acquisition_algo.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_criterion/","title":"Acquisition criterion.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_criterion/#acquisition-criterion","title":"Acquisition criterion.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to exploration is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function to approximate is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and three identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_3 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build three active learning algorithms to test the impact of the choice of the acquisition criterion on the active learning procedure. They respectively refer to the variance (default), standard deviation and distance criteria. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Exploration\", input_space, regressor_2, criterion_name=\"StandardDeviation\"\n)\nactive_learning_3 = ActiveLearningAlgo(\n    \"Exploration\", input_space, regressor_3, criterion_name=\"Distance\"\n)\n\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\nactive_learning_3.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>(&lt;gemseo.algos.database.Database object at 0x7fc9c7952a50&gt;, Optimization problem:\n   minimize -Distance\n   with respect to x)\n</code></pre> <p>Finally, for the three active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points.\nplt.figure()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\npoints_3 = active_learning_3.regressor.learning_set.to_numpy()\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"Variance\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"Standard deviation\")\nplt.scatter(points_3[:, 0], points_3[:, 1], marker=\"*\", label=\"Distance\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>In this implementation, Variance and standard deviation criteria provide the same learning points, which is expected because they are identical up to a square transformation. On the contrary, learning points from the distance criterion are much different and uniformly fills the input space. All 3 methods provide a surrogate with good accuracy</p> <pre><code>dataset_test = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=50\n)\nR2_1 = R2Measure(active_learning_1.regressor).compute_test_measure(dataset_test)\nR2_2 = R2Measure(active_learning_2.regressor).compute_test_measure(dataset_test)\nR2_3 = R2Measure(active_learning_3.regressor).compute_test_measure(dataset_test)\nR2_1[0], R2_2[0], R2_3[0]\n</code></pre> <p>Out:</p> <pre><code>(np.float64(0.9999562597980046), np.float64(0.9999252651278516), np.float64(0.9997194816101844))\n</code></pre> <p>Total running time of the script: ( 0 minutes  16.596 seconds)</p> <p> Download Python source code: plot_exploration_criterion.py</p> <p> Download Jupyter notebook: plot_exploration_criterion.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_gp/","title":"GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/exploration/plot_exploration_gp/#gp-regressor","title":"GP regressor.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/gemseo/mlearning/data_formatters/supervised_data_formatters.py:237: DocstringInheritanceWarning: in GaussianProcessRegressor.compute_samples: section Args: the docstring for the argument 'algo' is missing.\n  Lastly,\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/gemseo/mlearning/data_formatters/supervised_data_formatters.py:237: DocstringInheritanceWarning: in GaussianProcessRegressor.compute_samples: section Args: the docstring for the argument '*args' is missing.\n  Lastly,\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/gemseo/mlearning/data_formatters/supervised_data_formatters.py:237: DocstringInheritanceWarning: in GaussianProcessRegressor.compute_samples: section Args: the docstring for the argument '**kwargs' is missing.\n  Lastly,\n</code></pre> <p>The use of active learning methods dedicated to exploration is illustrated in this example. More specifically, we aim to test here the impact of the choice to test the impact of the choice of the GP on the active learning procedure, The function to approximate is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and one Gaussian process regressor OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>and the other from scikit-learn:</p> <pre><code>regressor_2 = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of the GP on the active learning procedure, one from the OpenTURNS library and the second from scikit-learn. Those two are notably different, in particular, GPs from scikit-learn do not include trend modeling. All other settings are put to their default value.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\"Exploration\", input_space, regressor_2)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9cca3ead0&gt;, Optimization problem:\n   minimize -Variance\n   with respect to x)\n</code></pre> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points.\nplt.figure()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"OpenTURNS GP\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"scikit-learn GP\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>In this implementation, changing the GP regressor, does not seem to significantly alter the learning procedure, as most acquisition points correspond for both surrogates. Thus, they have a similar R2 validation metric:</p> <pre><code>dataset_test = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=50\n)\nR2_1 = R2Measure(active_learning_1.regressor).compute_test_measure(dataset_test)\nR2_2 = R2Measure(active_learning_2.regressor).compute_test_measure(dataset_test)\n(R2_1[0], R2_2[0])\n</code></pre> <p>Out:</p> <pre><code>(np.float64(0.9999562597980046), np.float64(0.9920673734869645))\n</code></pre> <p>Total running time of the script: ( 0 minutes  12.629 seconds)</p> <p> Download Python source code: plot_exploration_gp.py</p> <p> Download Jupyter notebook: plot_exploration_gp.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/level_set_batch/","title":"Batch acquisition.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/level_set_batch/#batch-acquisition","title":"Batch acquisition.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.disciplines.surrogate import SurrogateDiscipline\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to level set estimation is illustrated in this example. More specifically, we aim to test here the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. We consider one algorithm with samples added one by one also called sequential acquisition (default), and in the second one, four samples instead of one are added at each iteration (parallel acquisition), using argument <code>batch_size</code> set to 4. All other settings are put to their default values.</p> <pre><code>value_level = 400\nactive_learning_1 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_1, output_value=value_level\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_2, output_value=value_level, batch_size=4\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>To study the results, for both active learning algorithms, we plot the training points, the estimated level sets alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nsurrogate_1 = SurrogateDiscipline(active_learning_1.regressor)\nsurrogate_2 = SurrogateDiscipline(active_learning_2.regressor)\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_1 = sample_disciplines(\n    [surrogate_1], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_2 = sample_disciplines(\n    [surrogate_2], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\n# and the level sets.\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nlevel_set_exact = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"red\",\n)\nlevel_set_gp_1 = plt.contour(\n    unique(observations_gp_1[:, 0]),\n    unique(observations_gp_1[:, 1]),\n    observations_gp_1[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:blue\",\n    linestyles=\"dotted\",\n)\nlevel_set_gp_2 = plt.contour(\n    unique(observations_gp_2[:, 0]),\n    unique(observations_gp_2[:, 1]),\n    observations_gp_2[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:orange\",\n    linestyles=\"dotted\",\n)\nplt.clabel(level_set_exact, levels=[value_level], fontsize=10, colors=\"red\")\nplt.annotate(\"Target level set\", (-0.2, 0.75), color=\"red\")\nplt.annotate(\n    \"Level set estimated with sequential acquisition\", (-1.5, 0.5), color=\"tab:blue\"\n)\nplt.annotate(\n    \"Level set estimated with parallel acquisition\",\n    (-1.5, 0.25),\n    color=\"tab:orange\",\n)\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"Sequential\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"Parallel\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n# The estimated level set\n# estimated with both sequential and batch\n# acquisition provide\n# a good approximation of the target.\n# It can be noticed that\n# with parallel acquisition,\n# a larger share of learning points\n# are used for exploration\n# contrary to sequential acquisition.\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: level_set_batch.py</p> <p> Download Jupyter notebook: level_set_batch.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/level_set_rbf/","title":"Non-GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/level_set_rbf/#non-gp-regressor","title":"Non-GP regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to level set estimation is illustrated in this example, with all default settings. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and a universal regressor, namely a radial basis function network based on SciPy:</p> <pre><code>regressor = RBFRegressor(learning_dataset)\nregressor_distribution = RegressorDistribution(regressor, bootstrap=False)\nregressor_distribution.learn()\n</code></pre> <p>Then, we look for 20 points that will help us to approximate the level-set associated to the 35% quantile. By default, for this purpose, the active learning algorithm looks for the point minimizing the U-function with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>level_value = 300.0\nactive_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    regressor_distribution,\n    output_value=level_value,\n)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Lastly, we plot the training points, the original model, the RBF regressor and the U-function after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n# It can be seen\n# that the learning points\n# are distributed around\n# the target level set,\n# thus approximating it properly.\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: level_set_rbf.py</p> <p> Download Jupyter notebook: level_set_rbf.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/mg_execution_times/","title":"Computation times","text":"<p>00:41.593 total execution time for generated_examples_active_learning_level_set files:</p> <p>+------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_level_set_criterion (docs/examples/active_learning/level_set/plot_level_set_criterion.py)                      | 00:16.644 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_level_set_gp (docs/examples/active_learning/level_set/plot_level_set_gp.py)                                           | 00:10.947 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_level_set_acquisition_algo (docs/examples/active_learning/level_set/plot_level_set_acquisition_algo.py) | 00:08.274 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_level_set (docs/examples/active_learning/level_set/plot_level_set.py)                                                    | 00:05.728 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | level_set_batch (docs/examples/active_learning/level_set/level_set_batch.py)                                                 | 00:00.000 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | level_set_rbf (docs/examples/active_learning/level_set/level_set_rbf.py)                                                       | 00:00.000 | 0.0 MB | +------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set/","title":"Default settings.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set/#default-settings","title":"Default settings.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to level set estimation is illustrated in this example, with all default settings. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and an initial Gaussian process regressor from OpenTURNS:</p> <pre><code>regressor = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we look for 20 points that will help us to approximate the level-set to value 300.0. By default, for this purpose, the active learning algorithm looks for the point minimizing the U-function with the help of the SLSQP algorithm applied in a multistart framework.</p> <pre><code>level_value = 300.0\nactive_learning = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor, output_value=level_value\n)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c6bee6d0&gt;, Optimization problem:\n   minimize U\n   with respect to x)\n</code></pre> <p>Lastly, we plot the training points, the original model, the Gaussian process regressor and the U-function after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n# It can be seen\n# that the learning points\n# are distributed around\n# the target level set,\n# thus approximating it properly.\n# After the 25th points,\n# the target level set\n# is known well enough\n# to allow to spend\n# computational budget on exploration.\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 7 Axes&gt;\n</code></pre> <p>Total running time of the script: ( 0 minutes  5.728 seconds)</p> <p> Download Python source code: plot_level_set.py</p> <p> Download Jupyter notebook: plot_level_set.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_acquisition_algo/","title":"Acquisition algorithm.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_acquisition_algo/#acquisition-algorithm","title":"Acquisition algorithm.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.disciplines.surrogate import SurrogateDiscipline\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example. More specifically, we aim to test here the impact of the choice of the optimization algorithm used to find the next acquisition point on the active learning procedure. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(\n    learning_dataset,\n    trend=\"quadratic\",\n)\nregressor_2 = OTGaussianProcessRegressor(\n    learning_dataset,\n    trend=\"quadratic\",\n)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of acquisition algorithm used to optimize the acquisition criterion on the active learning procedure. One uses the SLSQP gradient-based routine in a multistart fashion (default) for the optimization of the acquisition criterion, and the second the NELDER-MEAD gradient-free algorithm, also in a multistart fashion. All other settings are put to their default values.</p> <pre><code>value_level = 400\nactive_learning_1 = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    regressor_1,\n    output_value=value_level,\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    regressor_2,\n    output_value=value_level,\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.set_acquisition_algorithm(algo_name=\"NELDER-MEAD\")\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c63776d0&gt;, Optimization problem:\n   minimize U\n   with respect to x)\n</code></pre> <p>To study the results, for both active learning algorithms, we plot the training points, the estimated level sets alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nsurrogate_1 = SurrogateDiscipline(active_learning_1.regressor)\nsurrogate_2 = SurrogateDiscipline(active_learning_2.regressor)\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_1 = sample_disciplines(\n    [surrogate_1], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_2 = sample_disciplines(\n    [surrogate_2], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\n# and the level sets.\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nlevel_set_exact = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"red\",\n)\nlevel_set_gp_1 = plt.contour(\n    unique(observations_gp_1[:, 0]),\n    unique(observations_gp_1[:, 1]),\n    observations_gp_1[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:blue\",\n    linestyles=\"dotted\",\n)\nlevel_set_gp_2 = plt.contour(\n    unique(observations_gp_2[:, 0]),\n    unique(observations_gp_2[:, 1]),\n    observations_gp_2[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:orange\",\n    linestyles=\"dotted\",\n)\nplt.clabel(level_set_exact, levels=[value_level], fontsize=10, colors=\"red\")\nplt.annotate(\"Target level set\", (-0.2, 0.75), color=\"red\")\nplt.annotate(\n    \"Level set estimated with the multistart SLSQP\", (-1.5, 0.5), color=\"tab:blue\"\n)\nplt.annotate(\n    \"Level set estimated with the multistart NELDER-MEAD\",\n    (-1.5, 0.25),\n    color=\"tab:orange\",\n)\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with multistart SLSQP\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with NELDER-MEAD\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n# The estimated level set\n# estimated with both acquisition algorithm\n# provide a good approximation of the target.\n# Furthermore,\n# though the learning points are not strictly\n# speaking the same,\n# several are located close the other\n# if not the same.\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  8.274 seconds)</p> <p> Download Python source code: plot_level_set_acquisition_algo.py</p> <p> Download Jupyter notebook: plot_level_set_acquisition_algo.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_criterion/","title":"Acquisition criterion.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_criterion/#acquisition-criterion","title":"Acquisition criterion.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.disciplines.surrogate import SurrogateDiscipline\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to level set estimation is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and three identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_3 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build three active learning algorithms to test the impact of the choice of the acquisition criterion on the active learning procedure. They respectively refer to the U-function (default), EI and EF criteria. All other settings are put to their default values.</p> <pre><code>value_level = 400\nactive_learning_1 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_1, output_value=value_level\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_2, output_value=value_level, criterion_name=\"EI\"\n)\nactive_learning_3 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_3, output_value=value_level, criterion_name=\"EF\"\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\nactive_learning_3.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c542bad0&gt;, Optimization problem:\n   minimize -EF\n   with respect to x)\n</code></pre> <p>To study the results, for the three active learning algorithms, we plot the training points, the estimated level sets alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nsurrogate_1 = SurrogateDiscipline(active_learning_1.regressor)\nsurrogate_2 = SurrogateDiscipline(active_learning_2.regressor)\nsurrogate_3 = SurrogateDiscipline(active_learning_3.regressor)\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_1 = sample_disciplines(\n    [surrogate_1], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_2 = sample_disciplines(\n    [surrogate_2], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_3 = sample_disciplines(\n    [surrogate_3], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\n# and the level sets.\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\npoints_3 = active_learning_3.regressor.learning_set.to_numpy()\n\nlevel_set_exact = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"red\",\n)\nlevel_set_gp_1 = plt.contour(\n    unique(observations_gp_1[:, 0]),\n    unique(observations_gp_1[:, 1]),\n    observations_gp_1[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:blue\",\n    linestyles=\"dotted\",\n)\nlevel_set_gp_2 = plt.contour(\n    unique(observations_gp_2[:, 0]),\n    unique(observations_gp_2[:, 1]),\n    observations_gp_2[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:orange\",\n    linestyles=\"dotted\",\n)\nlevel_set_gp_3 = plt.contour(\n    unique(observations_gp_3[:, 0]),\n    unique(observations_gp_3[:, 1]),\n    observations_gp_3[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:purple\",\n    linestyles=\"dotted\",\n)\nplt.clabel(level_set_exact, levels=[value_level], fontsize=10, colors=\"red\")\nplt.annotate(\"Target level set\", (-0.2, 0.75), color=\"red\")\nplt.annotate(\"Level set estimated with U criterion\", (-1.5, 0.5), color=\"tab:blue\")\nplt.annotate(\n    \"Level set estimated with EI criterion\",\n    (-1.5, 0.25),\n    color=\"tab:orange\",\n)\nplt.annotate(\n    \"Level set estimated with EF criterion\",\n    (-1.5, 0),\n    color=\"tab:purple\",\n)\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"U\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"EI\")\nplt.scatter(points_3[:, 0], points_3[:, 1], marker=\"*\", label=\"EF\", color=\"tab:purple\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n# As suggests the comparable location\n# of their associated learning points,\n# the three level sets provide a good\n# approximation of the target level set.\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  16.644 seconds)</p> <p> Download Python source code: plot_level_set_criterion.py</p> <p> Download Jupyter notebook: plot_level_set_criterion.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_gp/","title":"GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/level_set/plot_level_set_gp/#gp-regressor","title":"GP regressor.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.disciplines.surrogate import SurrogateDiscipline\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to level set estimation is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function with the level set of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and one Gaussian process regressor OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>and the other from scikit-learn:</p> <pre><code>regressor_2 = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of the GP on the active learning procedure, one from the OpenTURNS library and the second from scikit-learn. Those two are notably different, in particular, GPs from scikit-learn do not include trend modeling. All other settings are put to their default value.</p> <pre><code>value_level = 400\nactive_learning_1 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_1, output_value=value_level\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"LevelSet\", input_space, regressor_2, output_value=value_level, criterion_name=\"EI\"\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c7951850&gt;, Optimization problem:\n   minimize -EI\n   with respect to x)\n</code></pre> <p>To study the results, for both active learning algorithms, we plot the training points, the estimated level sets alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nsurrogate_1 = SurrogateDiscipline(active_learning_1.regressor)\nsurrogate_2 = SurrogateDiscipline(active_learning_2.regressor)\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_1 = sample_disciplines(\n    [surrogate_1], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\nobservations_gp_2 = sample_disciplines(\n    [surrogate_2], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\n# and the level sets.\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\n\nlevel_set_exact = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"red\",\n)\nlevel_set_gp_1 = plt.contour(\n    unique(observations_gp_1[:, 0]),\n    unique(observations_gp_1[:, 1]),\n    observations_gp_1[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:blue\",\n    linestyles=\"dotted\",\n)\nlevel_set_gp_2 = plt.contour(\n    unique(observations_gp_2[:, 0]),\n    unique(observations_gp_2[:, 1]),\n    observations_gp_2[:, 2].reshape(n_test, n_test),\n    levels=[value_level],\n    colors=\"tab:orange\",\n    linestyles=\"dotted\",\n)\nplt.clabel(level_set_exact, levels=[value_level], fontsize=10, colors=\"red\")\nplt.annotate(\"Target level set\", (-0.2, 0.75), color=\"red\")\nplt.annotate(\"Level set estimated with OpenTURNS GP\", (-1.5, 0.5), color=\"tab:blue\")\nplt.annotate(\n    \"Level set estimated with scikit-learn GP\",\n    (-1.5, 0.25),\n    color=\"tab:orange\",\n)\n\n\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"OpenTURNS GP\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"scikit-learn GP\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n# Both active learning procedures\n# based on two different Gaussian process regressor\n# provide good approximations\n# of the target level set.\n# Still, the level set estimated\n# with the Gaussian process regressor\n# from OpenTURNS seems slightly better,\n# in particular for $(x_1,x_2) \\in [-1,1]\\times[-2,-0.5]$.\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  10.947 seconds)</p> <p> Download Python source code: plot_level_set_gp.py</p> <p> Download Jupyter notebook: plot_level_set_gp.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/mg_execution_times/","title":"Computation times","text":"<p>00:55.776 total execution time for generated_examples_active_learning_optimization files:</p> <p>+---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_optim_acquisition_algo (docs/examples/active_learning/optimization/plot_optim_acquisition_algo.py) | 00:17.758 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_optim_criterion (docs/examples/active_learning/optimization/plot_optim_criterion.py)                      | 00:16.302 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_optim_gp (docs/examples/active_learning/optimization/plot_optim_gp.py)                                           | 00:15.077 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_optim (docs/examples/active_learning/optimization/plot_optim.py)                                                    | 00:06.639 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | optim_batch (docs/examples/active_learning/optimization/optim_batch.py)                                                 | 00:00.000 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | optim_rbf (docs/examples/active_learning/optimization/optim_rbf.py)                                                       | 00:00.000 | 0.0 MB | +---------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/active_learning/optimization/optim_batch/","title":"Batch acquisition.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/optim_batch/#batch-acquisition","title":"Batch acquisition.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import argmin\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example. More specifically, we aim to test here the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(\n    learning_dataset, trend=\"quadratic\", multi_start_n_samples=20\n)\nregressor_2 = OTGaussianProcessRegressor(\n    learning_dataset, trend=\"quadratic\", multi_start_n_samples=20\n)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. We consider one algorithm with samples added one by one also called sequential acquisition (default), and in the second one, four samples instead of one are added at each iteration (parallel acquisition), using argument <code>batch_size</code> set to 4. Note that here, this feature is only available with the expected improvement criterion. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Minimum\", input_space, regressor_2, batch_size=4\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"Sequential\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"Parallel\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Minimum\")\nplt.legend()\nplt.show()\n</code></pre> <p>We can also compare the estimated optimas from the active learning procedures to their exact counterparts for both algorithms</p> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the exact minimum and the estimated minima\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter([1], [1], marker=\"o\", label=\"Exact minimum\", color=\"red\")\nplt.scatter(\n    points_1[argmin(points_1[:, -1]), 0],\n    points_1[argmin(points_1[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with sequential acquisition\",\n    color=\"tab:blue\",\n)\nplt.scatter(\n    points_2[argmin(points_2[:, -1]), 0],\n    points_2[argmin(points_2[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with parallel acquisition\",\n    color=\"tab:orange\",\n)\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with sequential acquisition\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with parallel acquisition\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: optim_batch.py</p> <p> Download Jupyter notebook: optim_batch.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/optim_rbf/","title":"Non-GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/optim_rbf/#non-gp-regressor","title":"Non-GP regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example, with all default settings. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and a universal regressor, namely a radial basis function network based on SciPy:</p> <pre><code>regressor = RBFRegressor(learning_dataset)\nregressor_distribution = RegressorDistribution(regressor, bootstrap=False)\nregressor_distribution.learn()\n</code></pre> <p>Then, we look for 20 points that will help us to find the minimum. By default, for this purpose, the active learning algorithm looks for the point maximizing the expected improvement with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Minimum\", input_space, regressor_distribution)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Then, we plot the history of the quantity of interest</p> <pre><code>active_learning.plot_qoi_history()\n</code></pre> <p>as well as the training points, the original model, the RBF regressor and the expected improvement after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p>Finally, we compare the estimated minimum from the active learning procedure to its exact theoretical value:</p> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: optim_rbf.py</p> <p> Download Jupyter notebook: optim_rbf.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim/","title":"Default settings.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim/#default-settings","title":"Default settings.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example, with all default settings. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and an initial Gaussian process regressor from OpenTURNS:</p> <pre><code>regressor = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we look for 20 points that will help us to find the minimum. By default, for this purpose, the active learning algorithm looks for the point maximizing the expected improvement with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>active_learning = ActiveLearningAlgo(\"Minimum\", input_space, regressor)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c50b5350&gt;, Optimization problem:\n   minimize -EI\n   with respect to x)\n</code></pre> <p>Then, we plot the history of the quantity of interest</p> <pre><code>active_learning.plot_qoi_history()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;gemseo.post.dataset.lines.Lines object at 0x7fc9cc878980&gt;\n</code></pre> <p>as well as the training points, the original model, the Gaussian process regressor and the expected improvement after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 7 Axes&gt;\n</code></pre> <p>Finally, we compare the estimated minimum from the active learning procedure to its exact theoretical value:</p> <p>Total running time of the script: ( 0 minutes  6.639 seconds)</p> <p> Download Python source code: plot_optim.py</p> <p> Download Jupyter notebook: plot_optim.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_acquisition_algo/","title":"Acquisition algorithm.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_acquisition_algo/#acquisition-algorithm","title":"Acquisition algorithm.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import argmin\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example. More specifically, we aim to test here the impact of the choice of the optimization algorithm used to find the next acquisition point on the active learning procedure. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of acquisition algorithm used to optimize the acquisition criterion on the active learning procedure. One uses the SLSQP gradient-based routine in a multistart fashion (default) for the optimization of the acquisition criterion, and the second the NELDER-MEAD gradient-free algorithm, also in a multistart fashion. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_2)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.set_acquisition_algorithm(\n    algo_name=\"MultiStart\", opt_algo_name=\"NELDER-MEAD\", n_start=20\n)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c41b36d0&gt;, Optimization problem:\n   minimize -EI\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"SLSQP\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"NELDER-MEAD\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Minimum\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated optimas from the active learning procedure to their exact counterparts for both algorithms</p> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the exact minimum and the estimated minima\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter([1], [1], marker=\"o\", label=\"Exact minimum\", color=\"red\")\nplt.scatter(\n    points_1[argmin(points_1[:, -1]), 0],\n    points_1[argmin(points_1[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with multistart SLSQP\",\n    color=\"tab:blue\",\n)\nplt.scatter(\n    points_2[argmin(points_2[:, -1]), 0],\n    points_2[argmin(points_2[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with multistart NELDER-MEAD\",\n    color=\"tab:orange\",\n)\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with multistart SLSQP\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with multistart NELDER-MEAD\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  17.758 seconds)</p> <p> Download Python source code: plot_optim_acquisition_algo.py</p> <p> Download Jupyter notebook: plot_optim_acquisition_algo.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_criterion/","title":"Acquisition criterion.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_criterion/#acquisition-criterion","title":"Acquisition criterion.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import argmin\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and three identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_3 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build three active learning algorithms to test the impact of the choice of the acquisition criterion on the active learning procedure. They respectively refer to the EI (default), LCB and Mean criteria. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Minimum\", input_space, regressor_2, criterion_name=\"LCB\"\n)\nactive_learning_3 = ActiveLearningAlgo(\n    \"Minimum\", input_space, regressor_3, criterion_name=\"Mean\"\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\nactive_learning_3.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c34b46d0&gt;, Optimization problem:\n   minimize Mean\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\nhistory_3 = active_learning_3.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"EI criterion\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"LCB criterion\")\nplt.plot(history_3[0], concatenate(history_2[1]), marker=\"o\", label=\"Mean criterion\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Minimum\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated optimas from the active learning procedures to their exact counterparts for the three algorithms</p> <p>Finally, for the three active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the exact minimum and the estimated minima\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\npoints_3 = active_learning_3.regressor.learning_set.to_numpy()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter([1], [1], marker=\"o\", label=\"Exact minimum\", color=\"red\")\nplt.scatter(\n    points_1[argmin(points_1[:, -1]), 0],\n    points_1[argmin(points_1[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with EI criterion\",\n)\nplt.scatter(\n    points_2[argmin(points_2[:, -1]), 0],\n    points_2[argmin(points_2[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with LCB criterion\",\n)\nplt.scatter(\n    points_3[argmin(points_3[:, -1]), 0],\n    points_3[argmin(points_3[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with Mean criterion\",\n)\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with EI criterion\",\n    color=\"tab:blue\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with LCB criterion\",\n    color=\"tab:orange\",\n)\nplt.scatter(\n    points_3[:, 0],\n    points_3[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with Mean criterion\",\n    color=\"tab:green\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  16.302 seconds)</p> <p> Download Python source code: plot_optim_criterion.py</p> <p> Download Jupyter notebook: plot_optim_criterion.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_gp/","title":"GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/optimization/plot_optim_gp/#gp-regressor","title":"GP regressor.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom numpy import argmin\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to optimization is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function to minimize is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) belonging to \\([-2,2]^2\\):</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and one Gaussian process regressor OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n# and the other from scikit-learn:\nregressor_2 = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of the GP on the active learning procedure, one from the OpenTURNS library and the second from scikit-learn. Those two are notably different, in particular, GPs from scikit-learn do not include trend modeling. All other settings are put to their default values.</p> <pre><code>active_learning_1 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_1)\nactive_learning_2 = ActiveLearningAlgo(\"Minimum\", input_space, regressor_2)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c4a53050&gt;, Optimization problem:\n   minimize -EI\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"OpenTURNS GP\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"Scikit-learn GP\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Minimum\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated optimas from the active learning procedures to their exact counterparts for both algorithms</p> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the exact minimum and the estimated minima\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter([1], [1], marker=\"o\", label=\"Exact minimum\", color=\"red\")\nplt.scatter(\n    points_1[argmin(points_1[:, -1]), 0],\n    points_1[argmin(points_1[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with OpenTURNS GP\",\n)\nplt.scatter(\n    points_2[argmin(points_2[:, -1]), 0],\n    points_2[argmin(points_2[:, -1]), 1],\n    marker=\"o\",\n    label=\"Minimum with scikit-learn GP\",\n)\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with OpenTURNS GP\",\n    color=\"tab:blue\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with scikit-learn GP\",\n    color=\"tab:orange\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  15.077 seconds)</p> <p> Download Python source code: plot_optim_gp.py</p> <p> Download Jupyter notebook: plot_optim_gp.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/mg_execution_times/","title":"Computation times","text":"<p>01:47.491 total execution time for generated_examples_active_learning_quantile files:</p> <p>+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_quantile (docs/examples/active_learning/quantile/plot_quantile.py)                                                    | 00:33.947 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_quantile_criterion (docs/examples/active_learning/quantile/plot_quantile_criterion.py)                      | 00:27.793 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_quantile_acquisition_algo (docs/examples/active_learning/quantile/plot_quantile_acquisition_algo.py) | 00:27.065 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | plot_quantile_gp (docs/examples/active_learning/quantile/plot_quantile_gp.py)                                           | 00:18.685 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | quantile_batch (docs/examples/active_learning/quantile/quantile_batch.py)                                                 | 00:00.000 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+ | quantile_rbf (docs/examples/active_learning/quantile/quantile_rbf.py)                                                       | 00:00.000 | 0.0 MB | +--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile/","title":"Default settings.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile/#default-settings","title":"Default settings.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example, with all default settings. The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and an initial Gaussian process regressor from OpenTURNS:</p> <pre><code>regressor = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we look for 20 points that will help us to approximate the 35% quantile. By default, for this purpose, the active learning algorithm looks for the point minimizing the U-function with the help of the SLSQP algorithm applied in a multistart framework.</p> <pre><code>level = 0.35\nactive_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning.acquire_new_points(discipline, 20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c2ddc550&gt;, Optimization problem:\n   minimize U\n   with respect to x)\n</code></pre> <p>Then, we plot the history of the quantity of interest</p> <pre><code>active_learning.plot_qoi_history()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;gemseo.post.dataset.lines.Lines object at 0x7fc9c46d3b10&gt;\n</code></pre> <p>as well as the training points, the original model, the Gaussian process regressor and the U-function after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 7 Axes&gt;\n</code></pre> <p>Finally, we compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for both algorithms:</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=10000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Total running time of the script: ( 0 minutes  33.947 seconds)</p> <p> Download Python source code: plot_quantile.py</p> <p> Download Jupyter notebook: plot_quantile.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_acquisition_algo/","title":"Acquisition algorithm.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_acquisition_algo/#acquisition-algorithm","title":"Acquisition algorithm.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example. More specifically, we aim to test here the impact of the choice of the optimization algorithm used to find the next acquisition point on the active learning procedure. The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of acquisition algorithm used to optimize the acquisition criterion on the active learning procedure. One uses the SLSQP gradient-based routine in a multistart fashion (default) for the optimization of the acquisition criterion, and the second the NELDER-MEAD gradient-free algorithm, also in a multistart fashion. All other settings are put to their default values.</p> <pre><code>level = 0.35\nactive_learning_1 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_1,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_2,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.set_acquisition_algorithm(\n    algo_name=\"MultiStart\", opt_algo_name=\"NELDER-MEAD\", n_start=20\n)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c26a9fd0&gt;, Optimization problem:\n   minimize U\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"SLSQP\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"NELDER-MEAD\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Quantile\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for both algorithms</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=1000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nlevel_set = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[reference_quantile[\"y\"]],\n    colors=\"red\",\n)\nplt.clabel(level_set, levels=[reference_quantile[\"y\"]], fontsize=10, colors=\"red\")\nplt.annotate(\"True level set\", (-0.2, 0.75), color=\"red\")\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with multistart SLSQP\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with multistart NELDER-MEAD\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/contour.py:931: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  for vs, cs in vertices_and_codes]\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/ticker.py:563: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  else fmt % arg)\n</code></pre> <p>Total running time of the script: ( 0 minutes  27.065 seconds)</p> <p> Download Python source code: plot_quantile_acquisition_algo.py</p> <p> Download Jupyter notebook: plot_quantile_acquisition_algo.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_criterion/","title":"Acquisition criterion.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_criterion/#acquisition-criterion","title":"Acquisition criterion.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example. More specifically, we aim to test here the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and three identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_3 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build three active learning algorithms to test the impact of the choice of the acquisition criterion used to enrich the dataset on the active learning procedure. They respectively refer to the U-function (default), EI and EF criteria. All other settings are put to their default values.</p> <pre><code>level = 0.35\nactive_learning_1 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_1,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_2,\n    level=level,\n    uncertain_space=uncertain_space,\n    criterion_name=\"EI\",\n)\nactive_learning_3 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_3,\n    level=level,\n    uncertain_space=uncertain_space,\n    criterion_name=\"EF\",\n)\nactive_learning_1.acquire_new_points(discipline, n_samples=20)\nactive_learning_2.acquire_new_points(discipline, n_samples=20)\nactive_learning_3.acquire_new_points(discipline, n_samples=20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:98: RuntimeWarning: invalid value encountered in multiply\n  (self._kappa**2 - 1 - t**2) * (norm.cdf(t_p) - norm.cdf(t_m))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:99: RuntimeWarning: invalid value encountered in multiply\n  - 2 * t * ((pdf_t_p := norm.pdf(t_p)) - (pdf_t_m := norm.pdf(t_m)))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:100: RuntimeWarning: invalid value encountered in multiply\n  + t_p * pdf_t_p\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei.py:101: RuntimeWarning: invalid value encountered in multiply\n  - t_m * pdf_t_m\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py:82: RuntimeWarning: divide by zero encountered in divide\n  t = (self._output_value - mean) / standard_deviation\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef.py:100: RuntimeWarning: invalid value encountered in multiply\n  - t * (2 * norm.cdf(t) - cdf_t_p - cdf_t_m)\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c0fb3450&gt;, Optimization problem:\n   minimize -EF\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for the three active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\nhistory_3 = active_learning_3.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"U\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"EI\")\nplt.plot(history_3[0], concatenate(history_3[1]), marker=\"o\", label=\"EF\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Quantile\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for the three algorithms:</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=1000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Finally, for the three active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\npoints_3 = active_learning_3.regressor.learning_set.to_numpy()\nlevel_set = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[reference_quantile[\"y\"]],\n    colors=\"red\",\n)\nplt.clabel(level_set, levels=[reference_quantile[\"y\"]], fontsize=10, colors=\"red\")\nplt.annotate(\"True level set\", (-0.2, 0.75), color=\"red\")\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with U criterion\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with EI criterion\",\n)\nplt.scatter(\n    points_3[:, 0],\n    points_3[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with EF criterion\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/contour.py:931: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  for vs, cs in vertices_and_codes]\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/ticker.py:563: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  else fmt % arg)\n</code></pre> <p>Total running time of the script: ( 0 minutes  27.793 seconds)</p> <p> Download Python source code: plot_quantile_criterion.py</p> <p> Download Jupyter notebook: plot_quantile_criterion.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_gp/","title":"GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/plot_quantile_gp/#gp-regressor","title":"GP regressor.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example. More specifically, we aim to test here the impact of the choice to test the impact of the choice of the GP on the active learning procedure, The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and one Gaussian process regressor OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(\n    learning_dataset,\n    trend=\"quadratic\",\n)\n# and the other from scikit-learn:\nregressor_2 = GaussianProcessRegressor(learning_dataset)\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of the choice of the GP on the active learning procedure, one from the OpenTURNS library and the second from scikit-learn. Those two are notably different, in particular, GPs from scikit-learn do not include trend modeling. All other settings are put to their default value.</p> <pre><code>level = 0.35\nactive_learning_1 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_1,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_2,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_1.acquire_new_points(discipline, 20)\nactive_learning_2.acquire_new_points(discipline, 20)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u.py:71: RuntimeWarning: divide by zero encountered in divide\n  abs(self._output_value - self._compute_mean(input_value))\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/sklearn/gaussian_process/kernels.py:450: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n  warnings.warn(\n\n(&lt;gemseo.algos.database.Database object at 0x7fc9c3259250&gt;, Optimization problem:\n   minimize U\n   with respect to x)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"OpenTURNS GP\")\nplt.plot(history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"Scikit-learn GP\")\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Quantile\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>We can also compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for both algorithms</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=1000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nlevel_set = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[reference_quantile[\"y\"]],\n    colors=\"red\",\n)\nplt.clabel(level_set, levels=[reference_quantile[\"y\"]], fontsize=10, colors=\"red\")\nplt.annotate(\"True level set\", (-0.2, 0.75), color=\"red\")\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(points_1[:, 0], points_1[:, 1], marker=\"*\", label=\"OpenTURNS GP\")\nplt.scatter(points_2[:, 0], points_2[:, 1], marker=\"*\", label=\"scikit-learn GP\")\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/contour.py:931: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  for vs, cs in vertices_and_codes]\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/matplotlib/ticker.py:563: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  else fmt % arg)\n</code></pre> <p>Total running time of the script: ( 0 minutes  18.685 seconds)</p> <p> Download Python source code: plot_quantile_gp.py</p> <p> Download Jupyter notebook: plot_quantile_gp.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/quantile_batch/","title":"Batch acquisition.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/quantile_batch/#batch-acquisition","title":"Batch acquisition.","text":"<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.ot_gpr import OTGaussianProcessRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\nfrom numpy import concatenate\nfrom numpy import unique\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example. More specifically, we aim to test here the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and two identical initial Gaussian process regressors from OpenTURNS:</p> <pre><code>regressor_1 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\nregressor_2 = OTGaussianProcessRegressor(learning_dataset, trend=\"quadratic\")\n</code></pre> <p>Then, we build two active learning algorithms to test the impact of batch acquisition, that is to say when several points are added at each method iteration, on the active learning procedure. We consider one algorithm with samples added one by one also called sequential acquisition (default), and in the second one, four samples instead of one are added at each iteration (parallel acquisition), using argument <code>batch_size</code> set to 4. All other settings are put to their default values.</p> <pre><code>level = 0.35\nactive_learning_1 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_1,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning_2 = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_2,\n    level=level,\n    uncertain_space=uncertain_space,\n    batch_size=4,\n)\nactive_learning_1.acquire_new_points(discipline, 20)\nactive_learning_2.acquire_new_points(discipline, 20)\n</code></pre> <p>To study the results, we extract first the data associated to the history of the quantity of interest for both active learning procedures</p> <pre><code>history_1 = active_learning_1.qoi_history\nhistory_2 = active_learning_2.qoi_history\n# and we compare them in a plot\nplt.plot(\n    history_1[0], concatenate(history_1[1]), marker=\"o\", label=\"Sequential acquisition\"\n)\nplt.plot(\n    history_2[0], concatenate(history_2[1]), marker=\"o\", label=\"Parallel acquisition\"\n)\nplt.xlabel(\"Number of evaluations\")\nplt.ylabel(\"Quantile\")\nplt.legend()\nplt.show()\n</code></pre> <p>We can also compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for both algorithms</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=1000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Finally, for both active learning algorithms, we plot the training points, alongside the original model.</p> <pre><code># Creation of the grid\n# and estimation of the different quantities\nn_test = 10\nobservations = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_FULLFACT\", n_samples=n_test**2\n).values\n\n# Plotting the contours of the Rosenbrock function\n# alongside the learning points\nplt.figure()\npoints_1 = active_learning_1.regressor.learning_set.to_numpy()\npoints_2 = active_learning_2.regressor.learning_set.to_numpy()\nlevel_set = plt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n    levels=[reference_quantile[\"y\"]],\n    colors=\"red\",\n)\nplt.clabel(level_set, levels=[reference_quantile[\"y\"]], fontsize=10, colors=\"red\")\nplt.annotate(\"True level set\", (-0.2, 0.75), color=\"red\")\nplt.contour(\n    unique(observations[:, 0]),\n    unique(observations[:, 1]),\n    observations[:, 2].reshape(n_test, n_test),\n)\nbar = plt.colorbar()\nbar.set_label(r\"$f(x_1,x_2)$\")\nplt.scatter(\n    points_1[:, 0],\n    points_1[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with sequential acquisition\",\n)\nplt.scatter(\n    points_2[:, 0],\n    points_2[:, 1],\n    marker=\"*\",\n    label=\"Learning points from algo with parallel acquisition\",\n)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.legend()\nplt.show()\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: quantile_batch.py</p> <p> Download Jupyter notebook: quantile_batch.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/active_learning/quantile/quantile_rbf/","title":"Non-GP regressor.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/active_learning/quantile/quantile_rbf/#non-gp-regressor","title":"Non-GP regressor.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import configuration\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning.regression.algos.rbf import RBFRegressor\nfrom gemseo.uncertainty.statistics.empirical_statistics import EmpiricalStatistics\n\nfrom gemseo_mlearning.active_learning.active_learning_algo import ActiveLearningAlgo\nfrom gemseo_mlearning.active_learning.distributions.regressor_distribution import (\n    RegressorDistribution,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\n\n# Update the configuration of |g| to speed up the script.\nconfiguration.fast = True\n</code></pre> <p>The use of active learning methods dedicated to quantile estimation is illustrated in this example, with all default settings. The function with the quantile of interest is the Rosenbrock function \\(f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2\\):</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>with \\(x_1\\) and \\(x_2\\) uniformly distributed over \\([-2,2]^2\\):</p> <pre><code>uncertain_space = RosenbrockSpace()\n</code></pre> <p>First, we create an initial training dataset using an optimal LHS including 10 samples:</p> <pre><code>learning_dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>and a universal regressor, namely a radial basis function network based on SciPy:</p> <pre><code>regressor = RBFRegressor(learning_dataset)\nregressor_distribution = RegressorDistribution(regressor, bootstrap=False)\nregressor_distribution.learn()\n</code></pre> <p>Then, we look for 20 points that will help us to approximate the 35% quantile. By default, for this purpose, the active learning algorithm looks for the point minimizing the U-function with the help of the SLSQP gradient-based algorithm applied in a multistart framework.</p> <pre><code>level = 0.35\nactive_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    uncertain_space,\n    regressor_distribution,\n    level=level,\n    uncertain_space=uncertain_space,\n)\nactive_learning.acquire_new_points(discipline, 10)\n</code></pre> <p>Then, we plot the history of the quantity of interest</p> <pre><code>active_learning.plot_qoi_history()\n</code></pre> <p>as well as the training points, the original model, the RBF regressor and the U-function after the last acquisition:</p> <pre><code>active_learning.plot_acquisition_view(discipline=discipline)\n</code></pre> <p>Finally, we compare the estimated quantile from the active learning procedure to the Monte Carlo estimate for both algorithms</p> <pre><code>dataset = sample_disciplines(\n    [discipline], uncertain_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=10000\n)\nreference_quantile = EmpiricalStatistics(dataset, [\"y\"]).compute_quantile(level)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.000 seconds)</p> <p> Download Python source code: quantile_rbf.py</p> <p> Download Jupyter notebook: quantile_rbf.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/","title":"Optimization","text":""},{"location":"generated/examples/optimization/#surrogate-based-optimization","title":"Surrogate-based optimization","text":"<p> Surrogate-based optimization using in-house features. </p> <p> Surrogate-based optimization using SMT. </p> <p> Download all examples in Python source code: optimization_python.zip</p> <p> Download all examples in Jupyter notebooks: optimization_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/mg_execution_times/","title":"Computation times","text":"<p>00:30.838 total execution time for generated_examples_optimization files:</p> <p>+--------------------------------------------------------------------------------+-----------+--------+ | plot_smt_ego (docs/examples/optimization/plot_smt_ego.py) | 00:22.982 | 0.0 MB | +--------------------------------------------------------------------------------+-----------+--------+ | plot_sbo_ego (docs/examples/optimization/plot_sbo_ego.py) | 00:07.856 | 0.0 MB | +--------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/optimization/plot_sbo_ego/","title":"Surrogate-based optimization using in-house features.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/plot_sbo_ego/#surrogate-based-optimization-using-in-house-features","title":"Surrogate-based optimization using in-house features.","text":"<pre><code>from __future__ import annotations\n\nfrom gemseo import execute_algo\nfrom gemseo.post.dataset.zvsxy import ZvsXY\nfrom gemseo.problems.dataset.rosenbrock import create_rosenbrock_dataset\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n</code></pre> <p>In this example, we seek to minimize the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\) over the design space \\([-2,2]^2\\). First, we instantiate the problem with \\((0, 0)\\) as initial guess:</p> <pre><code>problem = Rosenbrock()\n</code></pre> <p>Then, we minimize the Rosenbrock function using:</p> <ul> <li>the <code>\"SBO\"</code> algorithm,</li> <li>a maximum number of 40 evaluations,   including the initial one at the center of the design space   (this first point is common to all optimization algorithms)   and the initial training dataset,</li> <li> <p>its default settings,   namely:</p> </li> <li> <p>the expected improvement as acquisition criterion,</p> </li> <li>1 point acquired at a time,</li> <li>the <code>GaussianProcessRegressor</code> based on scikit-learn,</li> <li>10 initial training points     based on an optimized latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion     from 50 start points with a limit of 20 iterations per local optimization.</li> </ul> <pre><code>execute_algo(problem, algo_name=\"SBO\", max_iter=40)\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n/builds/gemseo/dev/gemseo-mlearning/src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_expected_impovement.py:55: RuntimeWarning: divide by zero encountered in divide\n  value = nan_to_num(improvement / std)\n/builds/gemseo/dev/gemseo-mlearning/.tox/doc/lib64/python3.13/site-packages/scipy/stats/_continuous_distns.py:361: RuntimeWarning: overflow encountered in square\n  return np.exp(-x**2/2.0) / _norm_pdf_C\n</code></pre> Optimization result:<ul><li>Design variables: [1.18135937 1.47278003]</li><li>Objective function: 0.6284131679276446</li><li>Feasible solution: True</li></ul> <p>We can see that the solution is close to the theoretical one \\((x^*,f^*)=((1,1),0)\\).</p> <p>We can also visualize all the evaluations and note that most of the points have been added in the valley as expected:</p> <pre><code>optimization_history = problem.to_dataset()\n\ninitial_point = optimization_history[0:1]\ninitial_point.name = \"Initial point\"\n\ninitial_training_points = optimization_history[1:12]\ninitial_training_points.name = \"Initial training points\"\n\nacquired_points = optimization_history[12:]\nacquired_points.name = \"Acquired points\"\n\nvisualization = ZvsXY(\n    create_rosenbrock_dataset(900),\n    (\"x\", 0),\n    (\"x\", 1),\n    \"rosen\",\n    fill=False,\n    other_datasets=(initial_point, initial_training_points, acquired_points),\n)\nvisualization.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Lastly, we can compare the solution to the one obtained with COBYLA, which is another popular gradient-free optimization algorithm:</p> <pre><code>execute_algo(Rosenbrock(), algo_name=\"NLOPT_COBYLA\", max_iter=40)\n</code></pre> Optimization result:<ul><li>Design variables: [0.31237742 0.09474212]</li><li>Objective function: 0.4736299669847872</li><li>Feasible solution: True</li></ul> <p>and conclude that for this problem and this initial guess, the surrogate-based algorithm is better than COBYLA.</p> <p>Total running time of the script: ( 0 minutes  7.856 seconds)</p> <p> Download Python source code: plot_sbo_ego.py</p> <p> Download Jupyter notebook: plot_sbo_ego.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/optimization/plot_smt_ego/","title":"Surrogate-based optimization using SMT.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/optimization/plot_smt_ego/#surrogate-based-optimization-using-smt","title":"Surrogate-based optimization using SMT.","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling, with a focus on derivatives. Bayesian optimization features are also available through its <code>EGO</code> class, with various acquisition criteria and strategies to acquire points in parallel.</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import execute_algo\nfrom gemseo.post.dataset.zvsxy import ZvsXY\nfrom gemseo.problems.dataset.rosenbrock import create_rosenbrock_dataset\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n</code></pre> <p>In this example, we seek to minimize the Rosenbrock function \\(f(x,y)=(1-x)^2+100(y-x^2)^2\\) over the design space \\([-2,2]^2\\). First, we instantiate the problem with \\((0, 0)\\) as initial guess:</p> <pre><code>problem = Rosenbrock()\n</code></pre> <p>Then, we minimize the Rosenbrock function using:</p> <ul> <li>the <code>\"SMT_EGO\"</code> algorithm,</li> <li>a maximum number of evaluations equal to 40,   including the initial one at the center of the design space   (this first point is common to all optimization algorithms)   and the initial training dataset,</li> <li> <p>its default settings,   namely</p> </li> <li> <p>the expected improvement as acquisition criterion,</p> </li> <li>1 point acquired at a time,</li> <li>the Kriging-based surrogate model <code>\"KRG\"</code>,</li> <li>10 initial training points based on a latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion     from 50 start points with a limit of 20 iterations per local optimization.</li> </ul> <pre><code>execute_algo(problem, algo_name=\"SMT_EGO\", max_iter=40)\n</code></pre> Optimization result:<ul><li>Design variables: [0.98669605 0.99537104]</li><li>Objective function: [0.04770946]</li><li>Feasible solution: True</li></ul> <p>We can see that the solution is close to the theoretical one \\((x^*,f^*)=((1,1),0)\\).</p> <p>We can also visualize all the evaluations and note that most of the points have been added in the valley as expected:</p> <pre><code>optimization_history = problem.to_dataset()\n\ninitial_point = optimization_history[0:1]\ninitial_point.name = \"Initial point\"\n\ninitial_training_points = optimization_history[1:12]\ninitial_training_points.name = \"Initial training points\"\n\nacquired_points = optimization_history[12:]\nacquired_points.name = \"Acquired points\"\n\nvisualization = ZvsXY(\n    create_rosenbrock_dataset(900),\n    (\"x\", 0),\n    (\"x\", 1),\n    \"rosen\",\n    fill=False,\n    other_datasets=(initial_point, initial_training_points, acquired_points),\n)\nvisualization.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Lastly, we can compare the solution to the one obtained with COBYLA, which is another popular gradient-free optimization algorithm:</p> <pre><code>execute_algo(Rosenbrock(), algo_name=\"NLOPT_COBYLA\", max_iter=40)\n</code></pre> Optimization result:<ul><li>Design variables: [0.31237742 0.09474212]</li><li>Objective function: 0.4736299669847872</li><li>Feasible solution: True</li></ul> <p>and conclude that for this problem and this initial guess, the surrogate-based algorithm is better than COBYLA.</p> <p>Total running time of the script: ( 0 minutes  22.982 seconds)</p> <p> Download Python source code: plot_smt_ego.py</p> <p> Download Jupyter notebook: plot_smt_ego.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/","title":"Regression models","text":""},{"location":"generated/examples/regression/#regression-models","title":"Regression models","text":"<p> SMT's surrogate model. </p> <p> Download all examples in Python source code: regression_python.zip</p> <p> Download all examples in Jupyter notebooks: regression_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/regression/mg_execution_times/","title":"Computation times","text":"<p>00:00.957 total execution time for generated_examples_regression files:</p> <p>+------------------------------------------------------------------------------------------------+-----------+--------+ | plot_smt_regressor (docs/examples/regression/plot_smt_regressor.py) | 00:00.957 | 0.0 MB | +------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/regression/plot_smt_regressor/","title":"SMT's surrogate model.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/regression/plot_smt_regressor/#smts-surrogate-model","title":"SMT's surrogate model.","text":"<p>The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling, with a focus on derivatives. The SMTRegressor class allows you to use any SMT's surrogate model in your GEMSEO processes, including the gradient-enhanced surrogate models as long as your training dataset includes both output and gradient samples as explained at the end of this page.</p> <p>In this example, we will approximate the Rosenbrock function<sup>1</sup></p> \\[f(x,y) = (1-x)^2 + 100(y-x^2)^2\\] <p>over the domain \\([-2,2]^2\\).</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import compute_doe\nfrom gemseo import sample_disciplines\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom gemseo.post.dataset.zvsxy import ZvsXY\n\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline import (\n    RosenbrockDiscipline,\n)\nfrom gemseo_mlearning.problems.rosenbrock.rosenbrock_space import RosenbrockSpace\nfrom gemseo_mlearning.regression.smt_regressor import SMTRegressor\n</code></pre> <p>First, we create the Rosenbrock discipline:</p> <pre><code>discipline = RosenbrockDiscipline()\n</code></pre> <p>and the input space:</p> <pre><code>input_space = RosenbrockSpace()\n</code></pre> <p>Then, we use an optimized Latin hypercube sampling (LHS) technique to generate 20 samples:</p> <pre><code>training_data = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_OPT_LHS\", n_samples=20\n)\n</code></pre> <p>From this learning dataset, we train an SMTRegressor based on the SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>surrogate_model = SMTRegressor(\n    training_data, model_class_name=\"RBF\", parameters={\"d0\": 2}\n)\nsurrogate_model.learn()\n</code></pre> <p>Finally, we assess its quality:</p> <pre><code>r2 = R2Measure(surrogate_model)\nr2_l = r2.compute_learning_measure()[0]\nr2_cv = r2.compute_cross_validation_measure()[0]\ntest_data = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"OT_MONTE_CARLO\", n_samples=1000\n)\nr2_t = r2.compute_test_measure(test_data)[0]\nf\"Learning R2: {r2_l}; cross-validation R2: {r2_cv}; test R2: {r2_t}\"\n</code></pre> <p>Out:</p> <pre><code>'Learning R2: 1.0; cross-validation R2: 0.8257126302519093; test R2: 0.9617160347610478'\n</code></pre> <p>see how good it is with its R2 close to 1 on the test dataset, and plot its output over a 20x20 grid:</p> <pre><code>input_data = compute_doe(input_space, algo_name=\"PYDOE_FULLFACT\", n_samples=400)\noutput_data = surrogate_model.predict(input_data)\npredictions = IODataset()\npredictions.add_input_group(input_data, variable_names=[\"x1\", \"x2\"])\npredictions.add_output_group(output_data, variable_names=[\"y\"])\n\nplot = ZvsXY(predictions, \"x1\", \"x2\", \"y\", other_datasets=(training_data,))\nplot.color = \"white\"\nplot.colormap = \"viridis\"\nplot.execute(save=False, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;Figure size 640x480 with 2 Axes&gt;]\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.957 seconds)</p> <p> Download Python source code: plot_smt_regressor.py</p> <p> Download Jupyter notebook: plot_smt_regressor.ipynb</p> <p>Gallery generated by mkdocs-gallery</p> <ol> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_mlearning<ul> <li>active_learning<ul> <li>acquisition_criteria<ul> <li>base_acquisition_criterion</li> <li>base_acquisition_criterion_family</li> <li>base_factory</li> <li>exploration<ul> <li>base_exploration</li> <li>distance</li> <li>exploration</li> <li>factory</li> <li>standard_deviation</li> <li>variance</li> </ul> </li> <li>level_set<ul> <li>base_ei_ef</li> <li>base_level_set</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>level_set</li> <li>u</li> </ul> </li> <li>maximum<ul> <li>base_maximum</li> <li>ei</li> <li>factory</li> <li>maximum</li> <li>mean</li> <li>ucb</li> </ul> </li> <li>minimum<ul> <li>base_minimum</li> <li>ei</li> <li>factory</li> <li>lcb</li> <li>mean</li> <li>minimum</li> </ul> </li> <li>quantile<ul> <li>base_quantile</li> <li>ef</li> <li>ei</li> <li>factory</li> <li>quantile</li> <li>u</li> </ul> </li> </ul> </li> <li>active_learning_algo</li> <li>distributions<ul> <li>base_regressor_distribution</li> <li>kriging_distribution</li> <li>regressor_distribution</li> </ul> </li> <li>visualization<ul> <li>acquisition_view</li> <li>qoi_history_view</li> </ul> </li> </ul> </li> <li>algos<ul> <li>opt<ul> <li>core<ul> <li>surrogate_based_optimizer</li> </ul> </li> <li>sbo_settings</li> <li>smt<ul> <li>ego_settings</li> <li>smt_ego</li> </ul> </li> <li>surrogate_based_optimization</li> </ul> </li> </ul> </li> <li>problems<ul> <li>branin<ul> <li>branin_discipline</li> <li>branin_function</li> <li>branin_problem</li> <li>branin_space</li> <li>functions</li> </ul> </li> <li>rosenbrock<ul> <li>functions</li> <li>rosenbrock_discipline</li> <li>rosenbrock_function</li> <li>rosenbrock_problem</li> <li>rosenbrock_space</li> </ul> </li> </ul> </li> <li>regression<ul> <li>smt_regressor</li> <li>smt_regressor_settings</li> </ul> </li> <li>settings<ul> <li>mlearning</li> <li>opt</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/gemseo_mlearning/","title":"API documentation","text":""},{"location":"reference/gemseo_mlearning/#gemseo_mlearning","title":"gemseo_mlearning","text":"<p>A GEMSEO extension for advanced machine learning.</p> <p>GEMSEO includes the main machine learning capabilities.</p>"},{"location":"reference/gemseo_mlearning/active_learning/","title":"Active learning","text":""},{"location":"reference/gemseo_mlearning/active_learning/#gemseo_mlearning.active_learning","title":"active_learning","text":"<p>Active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/","title":"Active learning algo","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo","title":"active_learning_algo","text":"<p>Active learning algorithm.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo","title":"ActiveLearningAlgo","text":"<pre><code>ActiveLearningAlgo(\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10000,\n    **criterion_arguments: Any\n)\n</code></pre> <p>An active learning algorithm using a regressor and acquisition criteria.</p> PARAMETER DESCRIPTION <code>criterion_family_name</code> <p>The name of a family of acquisition criteria, e.g. <code>\"Minimum\"</code>, <code>\"Maximum\"</code>, <code>\"LevelSet\"</code>, <code>\"Quantile\"</code> or <code>\"Exploration\"</code>.</p> <p> TYPE: <code>str</code> </p> <code>input_space</code> <p>The input space on which to look for the new learning point.</p> <p> TYPE: <code>DesignSpace</code> </p> <code>regressor</code> <p>Either a regressor or a regressor distribution.</p> <p> TYPE: <code>BaseRegressor | BaseRegressorDistribution</code> </p> <code>criterion_name</code> <p>The name of the acquisition criterion. If empty, use the default criterion of the family <code>criterion_family_name</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criteria in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>**criterion_arguments</code> <p>The parameters of the acquisition criterion.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>When the output dimension is greater than 1.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def __init__(\n    self,\n    criterion_family_name: str,\n    input_space: DesignSpace,\n    regressor: BaseRegressor | BaseRegressorDistribution,\n    criterion_name: str = \"\",\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n    **criterion_arguments: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        criterion_family_name: The name of a family of acquisition criteria,\n            *e.g.* `\"Minimum\"`, `\"Maximum\"`, `\"LevelSet\"`, `\"Quantile\"`\n            or `\"Exploration\"`.\n        input_space: The input space on which to look for the new learning point.\n        regressor: Either a regressor or a regressor distribution.\n        criterion_name: The name of the acquisition criterion.\n            If empty,\n            use the default criterion of the family `criterion_family_name`.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criteria in parallel.\n        **criterion_arguments: The parameters of the acquisition criterion.\n\n    Raises:\n        NotImplementedError: When the output dimension is greater than 1.\n    \"\"\"  # noqa: D205 D212 D415\n    # Create the regressor distribution.\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        distribution = KrigingDistribution(regressor)\n        distribution.learn()\n    elif isinstance(regressor, BaseRegressor):\n        distribution = RegressorDistribution(regressor)\n        distribution.learn()\n    else:\n        distribution = regressor\n\n    if distribution.output_dimension &gt; 1:\n        msg = \"ActiveLearningAlgo works only with scalar output.\"\n        raise NotImplementedError(msg)\n\n    # Create the acquisition problem.\n    family_factory = AcquisitionCriterionFamilyFactory()\n    self.__criterion_family_name = criterion_family_name\n    criterion_family = family_factory.get_class(criterion_family_name)\n    criterion_factory = criterion_family.ACQUISITION_CRITERION_FACTORY()\n    self.__acquisition_criterion = criterion_factory.create(\n        criterion_name,\n        distribution,\n        batch_size=batch_size,\n        mc_size=mc_size,\n        **criterion_arguments,\n    )\n    # Create the optimization space\n    # that is different from the input space\n    # when acquiring points in parallel.\n    optimization_space = DesignSpace()\n    lower_bound = tile(input_space.get_lower_bounds(), batch_size)\n    upper_bound = tile(input_space.get_upper_bounds(), batch_size)\n    input_space.initialize_missing_current_values()\n    value = tile(input_space.get_current_value(), batch_size)\n    optimization_space.add_variable(\n        \"x\",\n        size=len(lower_bound),\n        lower_bound=lower_bound,\n        upper_bound=upper_bound,\n        value=value,\n    )\n    problem = self.__acquisition_problem = OptimizationProblem(optimization_space)\n    problem.objective = self.__acquisition_criterion\n    if not problem.objective.has_jac:\n        problem.differentiation_method = (\n            OptimizationProblem.DifferentiationMethod.FINITE_DIFFERENCES\n        )\n    if problem.objective.MAXIMIZE:\n        problem.minimize_objective = False\n\n    # Initialize acquisition algorithm.\n    self.set_acquisition_algorithm(\n        self.__default_algo_name, **self.__default_algo_settings\n    )\n\n    # Miscellaneous.\n    self.__database = Database()\n    self.__n_initial_samples = len(distribution.algo.learning_set)\n    self.__distribution = distribution\n    self.__input_space = input_space\n    self.__batch_size = batch_size\n\n    # Create the acquisition view.\n    if distribution.algo.input_dimension == 2 and batch_size == 1:\n        self.__acquisition_view = AcquisitionView(self)\n    else:\n        self.__acquisition_view = None\n\n    self.__n_evaluations_history = []\n    self.__qoi_history = []\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion","title":"acquisition_criterion  <code>property</code>","text":"<pre><code>acquisition_criterion: BaseAcquisitionCriterion\n</code></pre> <p>The acquisition criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquisition_criterion_family_name","title":"acquisition_criterion_family_name  <code>property</code>","text":"<pre><code>acquisition_criterion_family_name: str\n</code></pre> <p>The name of the acquisition criterion family.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.batch_size","title":"batch_size  <code>property</code>","text":"<pre><code>batch_size: int\n</code></pre> <p>The number of points to be acquired in parallel.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.input_space","title":"input_space  <code>property</code>","text":"<pre><code>input_space: DesignSpace\n</code></pre> <p>The input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.n_initial_samples","title":"n_initial_samples  <code>property</code>","text":"<pre><code>n_initial_samples: int\n</code></pre> <p>The number of initial samples.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: float | None\n</code></pre> <p>The quantity of interest (QOI).</p> <p>When there is no quantity of interest associated with the acquisition criterion, this attribute is <code>None</code>.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.qoi_history","title":"qoi_history  <code>property</code>","text":"<pre><code>qoi_history: tuple[list[int], list[float]]\n</code></pre> <p>The history of the quantity of interest (QOI) when it exists.</p> <p>The second term represents this history while the first one represents the history of the number of model evaluations corresponding to these QOI estimations.</p> <p>When there is no quantity of interest associated with the acquisition criterion, these lists are empty.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor","title":"regressor  <code>property</code>","text":"<pre><code>regressor: BaseRegressor\n</code></pre> <p>The regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.regressor_distribution","title":"regressor_distribution  <code>property</code>","text":"<pre><code>regressor_distribution: BaseRegressorDistribution\n</code></pre> <p>The regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.acquire_new_points","title":"acquire_new_points","text":"<pre><code>acquire_new_points(\n    discipline: Discipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]\n</code></pre> <p>Update the machine learning algorithm by learning new samples.</p> <p>This method acquires new learning input-output samples and trains the machine learning algorithm with the resulting enriched learning set. The effective number of points will be the largest integer multiple of batch_size and less than or equal to n_samples.</p> PARAMETER DESCRIPTION <code>discipline</code> <p>The discipline computing the reference output data from the input data provided by the acquisition process.</p> <p> TYPE: <code>Discipline</code> </p> <code>n_samples</code> <p>The number of samples to update the machine learning algorithm. It should be a multiple of batch_size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>show</code> <p>Whether to display intermediate results Only when the input space dimension is 2.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>file_path</code> <p>The file path to save the plots of the intermediate results. If empty, do not save them. Only when the input space dimension is 2.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>tuple[Database, OptimizationProblem]</code> <p>The concatenation of the optimization histories related to the different points and the last acquisition problem.</p> RAISES DESCRIPTION <code>ValueError</code> <p>When the input space dimension is not 2.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def acquire_new_points(\n    self,\n    discipline: Discipline,\n    n_samples: int = 1,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; tuple[Database, OptimizationProblem]:\n    \"\"\"Update the machine learning algorithm by learning new samples.\n\n    This method acquires new learning input-output samples\n    and trains the machine learning algorithm\n    with the resulting enriched learning set.\n    The effective number of points will be the largest integer multiple\n    of batch_size and less than or equal to n_samples.\n\n    Args:\n        discipline: The discipline computing the reference output data\n            from the input data provided by the acquisition process.\n        n_samples: The number of samples to update the machine learning algorithm.\n            It should be a multiple of batch_size.\n        show: Whether to display intermediate results\n            Only when the input space dimension is 2.\n        file_path: The file path to save the plots of the intermediate results.\n            If empty, do not save them.\n            Only when the input space dimension is 2.\n\n    Returns:\n        The concatenation of the optimization histories\n        related to the different points\n        and the last acquisition problem.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    plot = show or file_path\n    if plot:\n        self.__check_acquisition_view()\n\n    self.__n_evaluations_history.append(self.__n_initial_samples)\n    self.__qoi_history.append(self.__acquisition_criterion.qoi)\n    total_n_samples = self.__n_initial_samples\n    n_batches = int(n_samples / self.__batch_size)\n    LOGGER.info(\"Acquiring %s points in batches of %s\", n_samples, self.batch_size)\n    with OneLineLogging(TQDM_LOGGER):\n        for batch_id in CustomTqdmProgressBar(range(1, n_batches + 1)):\n            array_input_data = self.find_next_point()\n            if plot:\n                self.__acquisition_view.draw(\n                    discipline=discipline,\n                    new_point=array_input_data[0],\n                    show=show,\n                    file_path=file_path,\n                )\n\n            for inputs, outputs in self.__acquisition_problem.database.items():\n                self.__database.store(\n                    array([batch_id, *inputs.unwrap().tolist()]), outputs\n                )\n\n            for points in range(self.__batch_size):\n                input_data = self.__input_space.convert_array_to_dict(\n                    array_input_data[points, :]\n                )\n\n                discipline.execute(input_data)\n\n                extra_learning_set = IODataset()\n                distribution = self.__distribution\n                variable_names_to_n_components = distribution.algo.sizes\n                new_points = hstack(list(input_data.values()))[newaxis]\n                extra_learning_set.add_group(\n                    group_name=IODataset.INPUT_GROUP,\n                    data=new_points,\n                    variable_names=distribution.input_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                output_names = distribution.output_names\n                output_data = discipline.get_output_data()\n                extra_learning_set.add_group(\n                    group_name=IODataset.OUTPUT_GROUP,\n                    data=hstack([output_data[name] for name in output_names])[\n                        newaxis\n                    ],\n                    variable_names=output_names,\n                    variable_names_to_n_components=variable_names_to_n_components,\n                )\n\n                augmented_learning_set = concat(\n                    [distribution.algo.learning_set, extra_learning_set],\n                    ignore_index=True,\n                )\n\n                self.__distribution.change_learning_set(augmented_learning_set)\n\n            self.update_problem()\n            self.__qoi_history.append(self.__acquisition_criterion.qoi)\n            total_n_samples += self.__batch_size\n            self.__n_evaluations_history.append(total_n_samples)\n\n    return self.__database, self.__acquisition_problem\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.find_next_point","title":"find_next_point","text":"<pre><code>find_next_point(as_dict: bool = False) -&gt; DataType\n</code></pre> <p>Find the next <code>batch_size</code> learning point(s).</p> PARAMETER DESCRIPTION <code>as_dict</code> <p>Whether to return the input data split by input names. Otherwise, return a unique array. In both cases, the arrays will be shaped as <code>(batch_size, input_dimension)</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The next <code>batch_size</code> learning point(s).</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def find_next_point(\n    self,\n    as_dict: bool = False,\n) -&gt; DataType:\n    \"\"\"Find the next `batch_size` learning point(s).\n\n    Args:\n        as_dict: Whether to return the input data split by input names.\n            Otherwise, return a unique array.\n            In both cases,\n            the arrays will be shaped as ``(batch_size, input_dimension)``.\n\n    Returns:\n        The next `batch_size` learning point(s).\n    \"\"\"\n    with LoggingContext(logging.getLogger(\"gemseo\")):\n        input_data = self.__acquisition_algo.execute(\n            self.__acquisition_problem, **self.__acquisition_algo_settings\n        ).x_opt\n        input_data = input_data.reshape(self.__batch_size, -1)\n    if as_dict:\n        return self.__acquisition_problem.design_space.convert_array_to_dict(\n            input_data\n        )\n\n    return input_data\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_acquisition_view","title":"plot_acquisition_view","text":"<pre><code>plot_acquisition_view(\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> PARAMETER DESCRIPTION <code>discipline</code> <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> <p> TYPE: <code>Discipline | None</code> DEFAULT: <code>None</code> </p> <code>n_test</code> <p>The number of points per dimension.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>filled</code> <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show</code> <p>Whether to display the view.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>file_path</code> <p>The file path to save the view. If empty, do not save it.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Figure</code> <p>The acquisition view.</p> RAISES DESCRIPTION <code>ValueError</code> <p>When the input space dimension is not 2.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_acquisition_view(\n    self,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n\n    Raises:\n        ValueError: When the input space dimension is not 2.\n    \"\"\"\n    self.__check_acquisition_view()\n    return self.__acquisition_view.draw(\n        discipline=discipline,\n        filled=filled,\n        n_test=n_test,\n        show=show,\n        file_path=file_path,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.plot_qoi_history","title":"plot_qoi_history","text":"<pre><code>plot_qoi_history(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Plot the history of the quantity of interest.</p> PARAMETER DESCRIPTION <code>show</code> <p>Whether to display the view.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>file_path</code> <p>The file path to save the view. If empty, do not save it.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> <code>label</code> <p>The label for the QOI. If empty, the name of the criterion family name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>add_markers</code> <p>Whether to add markers.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>**options</code> <p>The options to create the Lines object.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Lines</code> <p>The history of the quantity of interest.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def plot_qoi_history(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Plot the history of the quantity of interest.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n            If empty, the name of the criterion family name.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        The history of the quantity of interest.\n    \"\"\"\n    return QOIHistoryView(self).draw(\n        show=show,\n        file_path=file_path,\n        label=label,\n        add_markers=add_markers,\n        **options,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.set_acquisition_algorithm","title":"set_acquisition_algorithm","text":"<pre><code>set_acquisition_algorithm(\n    algo_name: str, **settings: Any\n) -&gt; None\n</code></pre> <p>Set sampling or optimization algorithm.</p> PARAMETER DESCRIPTION <code>algo_name</code> <p>The name of a DOE or optimization algorithm to find the learning point(s).</p> <p> TYPE: <code>str</code> </p> <code>**settings</code> <p>The values of some algorithm settings.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def set_acquisition_algorithm(self, algo_name: str, **settings: Any) -&gt; None:\n    \"\"\"Set sampling or optimization algorithm.\n\n    Args:\n        algo_name: The name of a DOE or optimization algorithm\n            to find the learning point(s).\n        **settings: The values of some algorithm settings.\n    \"\"\"\n    factory = DOELibraryFactory()\n    if not factory.is_available(algo_name):\n        factory = OptimizationLibraryFactory()\n\n    self.__acquisition_algo = factory.create(algo_name)\n    self.__acquisition_algo_settings = settings\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/active_learning_algo/#gemseo_mlearning.active_learning.active_learning_algo.ActiveLearningAlgo.update_problem","title":"update_problem","text":"<pre><code>update_problem() -&gt; None\n</code></pre> <p>Update the acquisition problem.</p> Source code in <code>src/gemseo_mlearning/active_learning/active_learning_algo.py</code> <pre><code>def update_problem(self) -&gt; None:\n    \"\"\"Update the acquisition problem.\"\"\"\n    self.__acquisition_problem.reset(preprocessing=False)\n    self.__acquisition_criterion.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/","title":"Acquisition criteria","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/#gemseo_mlearning.active_learning.acquisition_criteria","title":"acquisition_criteria","text":"<p>Acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/","title":"Base acquisition criterion","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion","title":"base_acquisition_criterion","text":"<p>Base class for acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion","title":"BaseAcquisitionCriterion","text":"<pre><code>BaseAcquisitionCriterion(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>Base class for acquisition criteria.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion.BaseAcquisitionCriterion.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/","title":"Base acquisition criterion family","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family","title":"base_acquisition_criterion_family","text":"<p>Base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.AcquisitionCriterionFamilyFactory","title":"AcquisitionCriterionFamilyFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion_family/#gemseo_mlearning.active_learning.acquisition_criteria.base_acquisition_criterion_family.BaseAcquisitionCriterionFamily","title":"BaseAcquisitionCriterionFamily","text":"<p>The base class for families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/","title":"Base factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory","title":"base_factory","text":"<p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory","title":"BaseAcquisitionCriterionFactory","text":"<p>               Bases: <code>BaseFactory</code></p> <p>A factory of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/base_factory/#gemseo_mlearning.active_learning.acquisition_criteria.base_factory.BaseAcquisitionCriterionFamilyFactory","title":"BaseAcquisitionCriterionFamilyFactory","text":"<p>The factory of families of acquisition criteria.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration","title":"exploration","text":"<p>Acquisition criteria for exploration.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/","title":"Base exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration","title":"base_exploration","text":"<p>Base class for the acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration","title":"BaseExploration","text":"<pre><code>BaseExploration(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class the for acquisition criteria to explore the input space.</p> <p>Optimize it to make the error of the surrogate model tends towards zero when the number of acquired points tends to infinity.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/base_exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.base_exploration.BaseExploration.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/","title":"Distance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance","title":"distance","text":"<p>Distance to the learning set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance","title":"Distance","text":"<pre><code>Distance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Distance to the learning set.</p> <p>This acquisition criterion computes the minimum distance between a new point and the point of the learning dataset, scaled by the minimum distance between two distinct learning points.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/distance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.distance.Distance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/","title":"Exploration","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration","title":"exploration","text":"<p>Family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/exploration/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.exploration.Exploration","title":"Exploration","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory","title":"factory","text":"<p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory","title":"ExplorationFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to explore the input space.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/factory/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.factory.ExplorationFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/","title":"Standard deviation","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation","title":"standard_deviation","text":"<p>Output standard deviation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation","title":"StandardDeviation","text":"<pre><code>StandardDeviation(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output standard deviation.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma[x] = \\sqrt{\\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/standard_deviation/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.standard_deviation.StandardDeviation.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/","title":"Variance","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance","title":"variance","text":"<p>Output variance.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance","title":"Variance","text":"<pre><code>Variance(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseExploration</code></p> <p>Output variance.</p> <p>This acquisition criterion is expressed as:</p> \\[\\sigma^2[x] = \\mathbb{E}[(Y(x)-\\mathbb{E}[Y(x)])^2]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/exploration/variance/#gemseo_mlearning.active_learning.acquisition_criteria.exploration.variance.Variance.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def update(self) -&gt; None:\n    \"\"\"Update the acquisition criterion.\"\"\"\n    data = self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = data.max() - data.min()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set","title":"level_set","text":"<p>Acquisition criteria for level set estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/","title":"Base ei ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef","title":"base_ei_ef","text":"<p>Base class for EI and EF criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF","title":"BaseEIEF","text":"<pre><code>BaseEIEF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The base class for EI and EF criteria to approximate a level set.</p> <p>EI and EF stands for expected improvement and expected feasibility respectively. More information in the subclasses EI and EF.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>output_value</code> <p>The model output value characterizing the level set.</p> <p> TYPE: <code>float</code> </p> <code>kappa</code> <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_ei_ef.BaseEIEF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> PARAMETER DESCRIPTION <code>output_value</code> <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/","title":"Base level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set","title":"base_level_set","text":"<p>Base class for acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet","title":"BaseLevelSet","text":"<pre><code>BaseLevelSet(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a level set estimation.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>output_value</code> <p>The model output value characterizing the level set.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.base_level_set.BaseLevelSet.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> PARAMETER DESCRIPTION <code>output_value</code> <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> \\[EF[x]=\\mathbb{E}\\left[\\max\\left(\\kappa\\mathbb{S}[Y(x)]-|y-Y(x)|,0\\right)\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EF[x] = \\mathbb{S}[Y(x)] ( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) ) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left( \\max(\\kappa\\mathbb{S}[Y(x_i)] - |y - Y(x_i)|,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>output_value</code> <p>The model output value characterizing the level set.</p> <p> TYPE: <code>float</code> </p> <code>kappa</code> <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ef/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ef.EF.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> PARAMETER DESCRIPTION <code>output_value</code> <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseEIEF</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] =\\mathbb{E}\\left[\\max\\left((\\kappa\\mathbb{S}[Y(x)])^2-(y - Y(x))^2,0\\right)\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = \\mathbb{V}[Y(x)]\\times ( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) ) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left( \\max((\\kappa\\mathbb{S}[Y(x_i)])^2 - (y - Y(x_i))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>output_value</code> <p>The model output value characterizing the level set.</p> <p> TYPE: <code>float</code> </p> <code>kappa</code> <p>A percentage of the standard deviation describing the area around <code>output_value</code> where to add points.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_ei_ef.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        kappa: A percentage of the standard deviation\n            describing the area around `output_value` where to add points.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(\n        regressor_distribution,\n        output_value,\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n    self._kappa = kappa\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/ei/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.ei.EI.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> PARAMETER DESCRIPTION <code>output_value</code> <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory","title":"LevelSetFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/factory/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.factory.LevelSetFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/","title":"Level set","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set","title":"level_set","text":"<p>Family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/level_set/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.level_set.LevelSet","title":"LevelSet","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a level set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseLevelSet</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\mathbb{E}\\left[\\left(\\frac{y-Y(x)}{\\mathbb{S}[Y(x)]} \\right)^2\\right]\\] <p>where \\(y\\) is the model output value characterizing the level set and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\). It has an analytic expression:</p> \\[U[x] = \\left(\\frac{y-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\right)^2.\\] <p>For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i \\leq q}\\left( \\left(\\frac{y-Y(x_i)}{\\mathbb{S}[Y(x_i)]}\\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>output_value</code> <p>The model output value characterizing the level set.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    output_value: float,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n    \"\"\"  # noqa: D205 D212 D415\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n    self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/level_set/u/#gemseo_mlearning.active_learning.acquisition_criteria.level_set.u.U.update","title":"update","text":"<pre><code>update(output_value: float | None = None) -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> PARAMETER DESCRIPTION <code>output_value</code> <p>The model output value characterizing the level set. If <code>None</code>, do not update the acquisition criterion.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/level_set/base_level_set.py</code> <pre><code>def update(self, output_value: float | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        output_value: The model output value characterizing the level set.\n            If ``None``, do not update the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    super().update()\n    if output_value is not None:\n        self._output_value = output_value\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum","title":"maximum","text":"<p>Acquisition criteria for maximum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/","title":"Base maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum","title":"base_maximum","text":"<p>Base class for acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum","title":"BaseMaximum","text":"<pre><code>BaseMaximum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a maximum.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.base_maximum.BaseMaximum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMaximum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-y_{\\text{max}},0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{max}}\\) is the maximum output value in the learning set.</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = (\\mathbb{E}[Y(x)] - y_{\\text{max}})\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{\\mathbb{E}[Y(x)] - y_{\\text{max}}}{\\mathbb{S}[Y(x)]}\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(Y(x_i)-y_{\\text{max}},0)\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory","title":"MaximumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.factory.MaximumFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/","title":"Maximum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum","title":"maximum","text":"<p>Family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/maximum/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.maximum.Maximum","title":"Maximum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/","title":"Mean","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean","title":"mean","text":"<p>Mean-based criterion.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean","title":"Mean","text":"<pre><code>Mean(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Mean</code>, <code>BaseMaximum</code></p> <p>Mean-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.mean.Mean.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/","title":"Ucb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb","title":"ucb","text":"<p>Upper confidence bound (UCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB","title":"UCB","text":"<pre><code>UCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMaximum</code></p> <p>The upper confidence bound (UCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] + \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>kappa</code> <p>A factor associated with the standard deviation to increase the mean value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/_confidence_bound.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        kappa: A factor associated with the standard deviation\n            to increase the mean value.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self.__kappa = kappa\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/maximum/ucb/#gemseo_mlearning.active_learning.acquisition_criteria.maximum.ucb.UCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/maximum/base_maximum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = max(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum","title":"minimum","text":"<p>Acquisition criteria for minimum estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/","title":"Base minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum","title":"base_minimum","text":"<p>Base class for acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum","title":"BaseMinimum","text":"<pre><code>BaseMinimum(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a minimum.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.base_minimum.BaseMinimum.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ExpectedImprovement</code>, <code>BaseMinimum</code></p> <p>Expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[EI[x] = \\mathbb{E}[\\max(y_{\\text{min}}-Y(x),0)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\text{min}}\\) is the minimum output value in the learning set.</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = (y_{\\text{min}}-\\mathbb{E}[Y(x)])\\Phi(t) + \\mathbb{S}[Y(x)]\\phi(t) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{y_{\\text{min}}-\\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(y_{\\text{min}}-Y(x_i),0)\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/ei/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory","title":"MinimumFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/factory/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.factory.MinimumFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/","title":"Lcb","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb","title":"lcb","text":"<p>Lower confidence bound (LCB).</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB","title":"LCB","text":"<pre><code>LCB(\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>ConfidenceBound</code>, <code>BaseMinimum</code></p> <p>The lower confidence bound (LCB).</p> <p>This acquisition criterion is expressed as</p> \\[M[x;\\kappa] = \\mathbb{E}[Y(x)] - \\kappa \\times \\mathbb{S}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\kappa&gt;0\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>kappa</code> <p>A factor associated with the standard deviation to increase the mean value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    kappa: float = 2.0,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:  # noqa: D102\n    super().__init__(\n        regressor_distribution,\n        kappa=-abs(kappa),\n        batch_size=batch_size,\n        mc_size=mc_size,\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/lcb/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.lcb.LCB.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/","title":"Mean","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean","title":"mean","text":"<p>The mean-based criterion to approximate a maximum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean","title":"Mean","text":"<pre><code>Mean(\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Mean</code>, <code>BaseMinimum</code></p> <p>Mean-based criterion.</p> <p>This acquisition criterion is expressed as</p> \\[E[x] = \\mathbb{E}[Y(x)]\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_acquisition_criterion.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        regressor_distribution: The distribution of the regressor.\n        batch_size: The number of points to be acquired in parallel;\n            if `1`, acquire points sequentially.\n        mc_size: The sample size to estimate the acquisition criterion in parallel.\n    \"\"\"  # noqa: D205 D212 D415\n    self._compute_mean = regressor_distribution.compute_mean\n    self._compute_standard_deviation = (\n        regressor_distribution.compute_standard_deviation\n    )\n    self._compute_variance = regressor_distribution.compute_variance\n    output_data = regressor_distribution.learning_set.output_dataset.to_numpy()\n    self.output_range = output_data.max() - output_data.min()\n    self._regressor_distribution = regressor_distribution\n    try:\n        jac = self._compute_jacobian(\n            ones(regressor_distribution.algo.input_dimension)\n        )\n    except NotImplementedError:\n        jac = None\n\n    self._batch_size = batch_size\n    self._mc_size = mc_size\n    self._compute_samples = regressor_distribution.compute_samples\n\n    if batch_size == 1:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute\n        else:\n            compute_output = self._compute_empirically\n    else:\n        if isinstance(regressor_distribution, KrigingDistribution):\n            compute_output = self._compute_by_batch\n        else:\n            compute_output = self._compute_by_batch_empirically\n\n    self._qoi = None\n    super().__init__(\n        compute_output,\n        self.__class__.__name__,\n        jac=jac,\n    )\n\n    self.update()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/mean/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.mean.Mean.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/minimum/base_minimum.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    self._qoi = min(\n        self._regressor_distribution.learning_set.output_dataset.to_numpy()\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/","title":"Minimum","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum","title":"minimum","text":"<p>Family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/minimum/minimum/#gemseo_mlearning.active_learning.acquisition_criteria.minimum.minimum.Minimum","title":"Minimum","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a minimum.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile","title":"quantile","text":"<p>Acquisition criteria for quantile estimation.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/","title":"Base quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile","title":"base_quantile","text":"<p>Base class for acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile","title":"BaseQuantile","text":"<pre><code>BaseQuantile(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseAcquisitionCriterion</code></p> <p>The base class for acquisition criteria to estimate a quantile.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>level</code> <p>The quantile level.</p> <p> TYPE: <code>float</code> </p> <code>uncertain_space</code> <p>The uncertain variable space.</p> <p> TYPE: <code>ParameterSpace</code> </p> <code>n_samples</code> <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.base_quantile.BaseQuantile.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/","title":"Ef","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef","title":"ef","text":"<p>Expected feasibility.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF","title":"EF","text":"<pre><code>EF(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected feasibility.</p> <p>This acquisition criterion is expressed as</p> \\[ EF[x] = \\mathbb{E}\\left[\\max(\\kappa\\mathbb{S}[Y(x)] - |y_{\\alpha} - Y(x)|,0)\\right] \\] <p>where \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EF[x] = \\mathbb{S}[Y(x)]\\times \\left( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max(\\kappa\\mathbb{S}[Y(x_i)] - |y_{\\alpha} - Y(x_i)|,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>level</code> <p>The quantile level.</p> <p> TYPE: <code>float</code> </p> <code>uncertain_space</code> <p>The uncertain variable space.</p> <p> TYPE: <code>ParameterSpace</code> </p> <code>n_samples</code> <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ef/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ef.EF.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/","title":"Ei","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei","title":"ei","text":"<p>Expected improvement.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI","title":"EI","text":"<pre><code>EI(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The expected improvement.</p> <p>This acquisition criterion is expressed as</p> \\[ EI[x] = \\mathbb{E} \\left[\\max\\left((\\kappa\\mathbb{S}[Y(x)])^2 - (y_{\\alpha} - Y(x))^2,0\\right)\\right] \\] <p>where \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output and \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\).</p> <p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[ EI[x] = \\mathbb{V}[Y(x)] \\left( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y_{\\alpha} - \\mathbb{E}[Y(x)]}{\\mathbb{S}[Y(x)]}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left( \\max((\\kappa\\mathbb{S}[Y(x_i)])^2 - (y_{\\alpha} - Y(x_i))^2,0) \\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>level</code> <p>The quantile level.</p> <p> TYPE: <code>float</code> </p> <code>uncertain_space</code> <p>The uncertain variable space.</p> <p> TYPE: <code>ParameterSpace</code> </p> <code>n_samples</code> <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = True\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/ei/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.ei.EI.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/","title":"Factory","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory","title":"factory","text":"<p>A factory of acquisition criteria to approximate quantiles.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory","title":"QuantileFactory","text":"<p>               Bases: <code>BaseAcquisitionCriterionFactory</code></p> <p>A factory of acquisition criteria to approximate level sets.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/factory/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.factory.QuantileFactory.get_class","title":"get_class","text":"<pre><code>get_class(name: str = '') -&gt; type[BaseAcquisitionCriterion]\n</code></pre> <p>Return a class from its name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the class.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>type[BaseAcquisitionCriterion]</code> <p>The class.</p> RAISES DESCRIPTION <code>ImportError</code> <p>If the class is not available.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/base_factory.py</code> <pre><code>def get_class(self, name: str = \"\") -&gt; type[BaseAcquisitionCriterion]:  # noqa: D102\n    return super().get_class(name or self._DEFAULT_CLASS_NAME)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/","title":"Quantile","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile","title":"quantile","text":"<p>Family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/quantile/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.quantile.Quantile","title":"Quantile","text":"<p>               Bases: <code>BaseAcquisitionCriterionFamily</code></p> <p>The family of acquisition criteria to estimate a quantile.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/","title":"U","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u","title":"u","text":"<p>U-function.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U","title":"U","text":"<pre><code>U(\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 100000,\n    batch_size: int = 1,\n    mc_size: int = 10000,\n)\n</code></pre> <p>               Bases: <code>BaseQuantile</code></p> <p>The U-function.</p> <p>This acquisition criterion is expressed as</p> \\[U[x] = \\frac{|y_{\\alpha}-\\mathbb{E}[Y(x)]|}{\\mathbb{S}[Y(x)])}\\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(y_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the model output. For numerical purposes, the expression effectively minimized corresponds to its square root.</p> <p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i\\leq q}\\left( \\left(\\frac{y_{\\alpha}-Y(x_i)}{\\mathbb{S}[Y(x_i)]}\\right)^2\\right)\\right]\\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition is thus instead evaluated with crude Monte-Carlo.</p> PARAMETER DESCRIPTION <code>regressor_distribution</code> <p>The distribution of the regressor.</p> <p> TYPE: <code>BaseRegressorDistribution</code> </p> <code>level</code> <p>The quantile level.</p> <p> TYPE: <code>float</code> </p> <code>uncertain_space</code> <p>The uncertain variable space.</p> <p> TYPE: <code>ParameterSpace</code> </p> <code>n_samples</code> <p>The number of samples to estimate the quantile of the regressor by Monte Carlo.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criterion in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def __init__(\n    self,\n    regressor_distribution: BaseRegressorDistribution,\n    level: float,\n    uncertain_space: ParameterSpace,\n    n_samples: int = 10_0000,\n    batch_size: int = 1,\n    mc_size: int = 10_000,\n) -&gt; None:\n    \"\"\"\n    Args:\n        level: The quantile level.\n        uncertain_space: The uncertain variable space.\n        n_samples: The number of samples\n            to estimate the quantile of the regressor by Monte Carlo.\n    \"\"\"  # noqa: D205 D212 D415\n    input_names = regressor_distribution.input_names\n    missing_names = set(input_names) - set(uncertain_space.variable_names)\n    if missing_names:\n        msg = (\n            \"The probability distributions of the input variables \"\n            f\"{pretty_str(missing_names, use_and=True)} are missing.\"\n        )\n        raise ValueError(msg)\n\n    # Create a new uncertain space sorted by model inputs.\n    new_uncertain_space = ParameterSpace()\n    new_uncertain_space.add_variables_from(uncertain_space, *input_names)\n    self.__input_data = new_uncertain_space.compute_samples(n_samples)\n    self.__level = level\n    # The value 0. will be replaced by the quantile estimation at each update,\n    # including the first one due to super().__init__.\n    self.__level_set_criterion = self._LEVEL_SET_CLASS(\n        regressor_distribution, 0.0, batch_size=batch_size, mc_size=mc_size\n    )\n\n    super().__init__(regressor_distribution, batch_size=batch_size, mc_size=mc_size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.MAXIMIZE","title":"MAXIMIZE  <code>class-attribute</code>","text":"<pre><code>MAXIMIZE: bool = False\n</code></pre> <p>Whether this acquisition criterion must be maximized.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.output_range","title":"output_range  <code>instance-attribute</code>","text":"<pre><code>output_range: float = max() - min()\n</code></pre> <p>The output range.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.qoi","title":"qoi  <code>property</code>","text":"<pre><code>qoi: Any\n</code></pre> <p>The quantity of interest.</p>"},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/acquisition_criteria/quantile/u/#gemseo_mlearning.active_learning.acquisition_criteria.quantile.u.U.update","title":"update","text":"<pre><code>update() -&gt; None\n</code></pre> <p>Update the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/acquisition_criteria/quantile/base_quantile.py</code> <pre><code>def update(self) -&gt; None:  # noqa: D102\n    super().update()\n    qoi = self._qoi = atleast_1d(self.__compute_quantile())\n    self.__level_set_criterion.update(output_value=qoi[0])\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/","title":"Distributions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions","title":"distributions","text":"<p>Regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/#gemseo_mlearning.active_learning.distributions.get_regressor_distribution","title":"get_regressor_distribution","text":"<pre><code>get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution\n</code></pre> <p>Return the distribution of a regressor.</p> PARAMETER DESCRIPTION <code>regressor</code> <p>The regression algorithm.</p> <p> TYPE: <code>BaseRegressor</code> </p> <code>use_bootstrap</code> <p>Whether to use bootstrap for resampling. If <code>False</code>, use cross-validation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_loo</code> <p>Whether to use leave-one-out resampling when <code>use_bootstrap</code> is <code>False</code>. If <code>False</code>, use parameterized cross-validation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>size</code> <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BaseRegressorDistribution</code> <p>The distribution of the regression algorithm.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/__init__.py</code> <pre><code>def get_regressor_distribution(\n    regressor: BaseRegressor,\n    use_bootstrap: bool = True,\n    use_loo: bool = False,\n    size: int | None = None,\n) -&gt; BaseRegressorDistribution:\n    \"\"\"Return the distribution of a regressor.\n\n    Args:\n        regressor: The regression algorithm.\n        use_bootstrap: Whether to use bootstrap for resampling.\n            If `False`, use cross-validation.\n        use_loo: Whether to use leave-one-out resampling when\n            `use_bootstrap` is `False`.\n            If `False`, use parameterized cross-validation.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n\n    Returns:\n        The distribution of the regression algorithm.\n    \"\"\"\n    if isinstance(regressor, BaseRandomProcessRegressor):\n        return KrigingDistribution(regressor)\n\n    return RegressorDistribution(regressor, use_bootstrap, use_loo, size)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/","title":"Base regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution","title":"base_regressor_distribution","text":"<p>Base class for regressor distributions.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution","title":"BaseRegressorDistribution","text":"<pre><code>BaseRegressorDistribution(regressor: BaseRegressor)\n</code></pre> <p>The distribution of a regressor.</p> PARAMETER DESCRIPTION <code>regressor</code> <p>A regression model.</p> <p> TYPE: <code>BaseRegressor</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def __init__(self, regressor: BaseRegressor) -&gt; None:\n    \"\"\"\n    Args:\n        regressor: A regression model.\n    \"\"\"  # noqa: D205 D212 D415\n    self.algo = regressor\n    self._samples = []\n    self._transform_input_group = self.algo._transform_input_group\n    self._transform_output_group = self.algo._transform_output_group\n    self._input_variables_to_transform = self.algo._input_variables_to_transform\n    self._output_variables_to_transform = self.algo._output_variables_to_transform\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> PARAMETER DESCRIPTION <code>learning_set</code> <p>The new learning set.</p> <p> TYPE: <code>Dataset</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval  <code>abstractmethod</code>","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> <code>level</code> <p>A quantile level.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> RETURNS DESCRIPTION <code>tuple[dict[str, NumberArray], dict[str, NumberArray], tuple[NumberArray, NumberArray]] | None</code> <p>The lower and upper bound values of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_confidence_interval(\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[\n        dict[str, NumberArray],\n        dict[str, NumberArray],\n        tuple[NumberArray, NumberArray],\n    ]\n    | None\n):\n    \"\"\"Compute the lower bounds and upper bounds of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n        level: A quantile level.\n\n    Returns:\n        The lower and upper bound values of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_covariance","title":"compute_covariance  <code>abstractmethod</code>","text":"<pre><code>compute_covariance(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Compute the output covariance of the regressor.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to compute the covariance; shaped as \\((N, d)\\).</p> <p> TYPE: <code>NumberArray</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The posterior covariance matrix at the input points of shape \\((Np, Np)\\) with \\(p\\) the output dimension. The covariance between the \\(k\\)-th output at the \\(i\\)-th input point and the \\(l\\)-th output at the \\(j\\)-th input point is located at the \\(m\\)-th line and \\(n\\)-th column with \\(m=ip+k\\), \\(n=jp+l\\), \\(i,j\\in\\{0,\\ldots,N-1\\}\\) and \\(k,l\\in\\{0,\\ldots,p-1\\}\\).</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_covariance(\n    self,\n    input_data: NumberArray,\n) -&gt; NumberArray:\n    r\"\"\"Compute the output covariance of the regressor.\n\n    Args:\n        input_data: The $N$ input points of dimension $d$\n            at which to compute the covariance;\n            shaped as $(N, d)$.\n\n    Returns:\n        The posterior covariance matrix at the input points\n        of shape $(Np, Np)$\n        with $p$ the output dimension.\n        The covariance between\n        the $k$-th output\n        at the $i$-th input point\n        and the $l$-th output\n        at the $j$-th input point\n        is located at\n        the $m$-th line and $n$-th column\n        with $m=ip+k$, $n=jp+l$,\n        $i,j\\in\\{0,\\ldots,N-1\\}$\n        and $k,l\\in\\{0,\\ldots,p-1\\}$.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_mean","title":"compute_mean  <code>abstractmethod</code>","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output mean of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_mean(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output mean of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output mean of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_samples","title":"compute_samples  <code>abstractmethod</code>","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> <p> TYPE: <code>NumberArray</code> </p> <code>n_samples</code> <p>The number of samples <code>M</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_samples(\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    \"\"\"Generate samples from the random process.\n\n    Args:\n        input_data: The $N$ input points of dimension $d$\n            at which to observe the random process;\n            shaped as `(N, d)`.\n        n_samples: The number of samples `M`.\n\n    Returns:\n        The output samples per output dimension shaped as `(N, M, p)`\n        where `p` is the output dimension.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output standard deviation  of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.compute_variance","title":"compute_variance  <code>abstractmethod</code>","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output variance of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@abstractmethod\ndef compute_variance(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output variance of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output variance of the regressor.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> PARAMETER DESCRIPTION <code>samples</code> <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> <p> TYPE: <code>list[int] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/base_regressor_distribution/#gemseo_mlearning.active_learning.distributions.base_regressor_distribution.BaseRegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output value(s) of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/","title":"Kriging distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution","title":"kriging_distribution","text":"<p>Kriging-like regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution","title":"KrigingDistribution","text":"<pre><code>KrigingDistribution(algo: BaseRandomProcessRegressor)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Kriging-like regressor distribution.</p> PARAMETER DESCRIPTION <code>algo</code> <p>The description is missing.</p> <p> TYPE: <code>BaseRandomProcessRegressor</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def __init__(  # noqa: D107\n    self, algo: BaseRandomProcessRegressor\n) -&gt; None:\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRandomProcessRegressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> PARAMETER DESCRIPTION <code>learning_set</code> <p>The new learning set.</p> <p> TYPE: <code>Dataset</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:\n    \"\"\"Re-train the regressor from a new learning set.\n\n    Args:\n        learning_set: The new learning set.\n    \"\"\"\n    self.algo.learning_set = learning_set\n    self.learn()\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> <code>level</code> <p>A quantile level.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> RETURNS DESCRIPTION <code>tuple[dict[str, NumberArray], dict[str, NumberArray]] | tuple[NumberArray, NumberArray]</code> <p>The lower and upper bound values of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    mean = self.compute_mean(input_data)\n    std = self.compute_standard_deviation(input_data)\n    quantile = norm.ppf(level)\n    if isinstance(mean, Mapping):\n        lower = {name: mean[name] - quantile * std[name] for name in mean}\n        upper = {name: mean[name] + quantile * std[name] for name in mean}\n    else:\n        lower = mean - quantile * std\n        upper = mean + quantile * std\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_covariance","title":"compute_covariance","text":"<pre><code>compute_covariance(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Compute the output covariance of the regressor.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to compute the covariance; shaped as \\((N, d)\\).</p> <p> TYPE: <code>NumberArray</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The posterior covariance matrix at the input points of shape \\((Np, Np)\\) with \\(p\\) the output dimension. The covariance between the \\(k\\)-th output at the \\(i\\)-th input point and the \\(l\\)-th output at the \\(j\\)-th input point is located at the \\(m\\)-th line and \\(n\\)-th column with \\(m=ip+k\\), \\(n=jp+l\\), \\(i,j\\in\\{0,\\ldots,N-1\\}\\) and \\(k,l\\in\\{0,\\ldots,p-1\\}\\).</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_covariance(  # noqa: D102\n    self,\n    input_data: NumberArray,\n) -&gt; NumberArray:\n    return self.algo.predict_covariance(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output mean of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> <p> TYPE: <code>NumberArray</code> </p> <code>n_samples</code> <p>The number of samples <code>M</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.algo.compute_samples(input_data, n_samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output standard deviation  of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.algo.predict_std(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output variance of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/kriging_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(  # noqa: D102\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    return self.compute_standard_deviation(input_data) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> PARAMETER DESCRIPTION <code>samples</code> <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> <p> TYPE: <code>list[int] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def learn(self, samples: list[int] | None = None) -&gt; None:\n    \"\"\"Train the regressor from the learning dataset.\n\n    Args:\n        samples: The indices of the learning samples.\n            If `None`, use the whole learning dataset\n    \"\"\"\n    self._samples = samples or range(len(self.learning_set))\n    self.algo.learn(self._samples)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/kriging_distribution/#gemseo_mlearning.active_learning.distributions.kriging_distribution.KrigingDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output value(s) of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/","title":"Regressor distribution","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution","title":"regressor_distribution","text":"<p>Universal regressor distribution.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution","title":"RegressorDistribution","text":"<pre><code>RegressorDistribution(\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseRegressorDistribution</code></p> <p>Universal regressor distribution.</p> PARAMETER DESCRIPTION <code>algo</code> <p>The description is missing.</p> <p> TYPE: <code>BaseRegressor</code> </p> <code>bootstrap</code> <p>The resampling method. If <code>True</code>, use bootstrap resampling. Otherwise, use cross-validation resampling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>loo</code> <p>The leave-One-Out sub-method, when bootstrap is <code>False</code>. If <code>False</code>, use parameterized cross-validation, Otherwise use leave-one-out.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>size</code> <p>The size of the resampling set, i.e. the number of times the regression algorithm is rebuilt. If <code>None</code>, N_BOOTSTRAP in the case of bootstrap and N_FOLDS in the case of cross-validation. This argument is ignored in the case of leave-one-out.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def __init__(\n    self,\n    algo: BaseRegressor,\n    bootstrap: bool = True,\n    loo: bool = False,\n    size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        bootstrap: The resampling method.\n            If `True`, use bootstrap resampling.\n            Otherwise, use cross-validation resampling.\n        loo: The leave-One-Out sub-method, when bootstrap is `False`.\n            If `False`, use parameterized cross-validation,\n            Otherwise use leave-one-out.\n        size: The size of the resampling set,\n            i.e. the number of times the regression algorithm is rebuilt.\n            If `None`,\n            [N_BOOTSTRAP][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP]\n            in the case of bootstrap\n            and\n            [N_FOLDS][gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS]\n            in the case of cross-validation.\n            This argument is ignored in the case of leave-one-out.\n    \"\"\"  # noqa: D205 D212 D415\n    if bootstrap:\n        self.method = self.BOOTSTRAP\n        self.size = size or self.N_BOOTSTRAP\n    else:\n        if loo:\n            self.method = self.LOO\n            self.size = len(algo.learning_set)\n        else:\n            self.method = self.CROSS_VALIDATION\n            self.size = size or self.N_FOLDS\n    self.algos = [\n        algo.__class__(algo.learning_set, settings_model=algo._settings)\n        for _ in range(self.size)\n    ]\n    self.weights = []\n    super().__init__(algo)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_BOOTSTRAP","title":"N_BOOTSTRAP  <code>class-attribute</code>","text":"<pre><code>N_BOOTSTRAP: int = 100\n</code></pre> <p>The default number of replicates for the bootstrap method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.N_FOLDS","title":"N_FOLDS  <code>class-attribute</code>","text":"<pre><code>N_FOLDS: int = 5\n</code></pre> <p>The default number of folds for the cross-validation method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.algo","title":"algo  <code>instance-attribute</code>","text":"<pre><code>algo: BaseRegressor = regressor\n</code></pre> <p>The regression model.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.input_names","title":"input_names  <code>property</code>","text":"<pre><code>input_names: list[str]\n</code></pre> <p>The input names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learning_set","title":"learning_set  <code>property</code>","text":"<pre><code>learning_set: IODataset\n</code></pre> <p>The learning dataset used by the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>The resampling method.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_dimension","title":"output_dimension  <code>property</code>","text":"<pre><code>output_dimension: int\n</code></pre> <p>The output dimension of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.output_names","title":"output_names  <code>property</code>","text":"<pre><code>output_names: list[str]\n</code></pre> <p>The output names of the regressor.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre> <p>The size of the resampling set.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights: list[Callable[[NumberArray], float]] = []\n</code></pre> <p>The weight functions related to the sub-algorithms.</p> <p>A weight function computes a weight from an input data array.</p>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.change_learning_set","title":"change_learning_set","text":"<pre><code>change_learning_set(learning_set: Dataset) -&gt; None\n</code></pre> <p>Re-train the regressor from a new learning set.</p> PARAMETER DESCRIPTION <code>learning_set</code> <p>The new learning set.</p> <p> TYPE: <code>Dataset</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def change_learning_set(self, learning_set: Dataset) -&gt; None:  # noqa: D102\n    for algo in self.algos:\n        algo.learning_set = learning_set\n    super().change_learning_set(learning_set)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_confidence_interval","title":"compute_confidence_interval","text":"<pre><code>compute_confidence_interval(\n    input_data: DataType, level: float = 0.95\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n)\n</code></pre> <p>Compute the lower bounds and upper bounds of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> <code>level</code> <p>A quantile level.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> RETURNS DESCRIPTION <code>tuple[dict[str, NumberArray], dict[str, NumberArray]] | tuple[NumberArray, NumberArray]</code> <p>The lower and upper bound values of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_confidence_interval(  # noqa: D102\n    self,\n    input_data: DataType,\n    level: float = 0.95,\n) -&gt; (\n    tuple[dict[str, NumberArray], dict[str, NumberArray]]\n    | tuple[NumberArray, NumberArray]\n):\n    level = (1.0 - level) / 2.0\n    predictions = self.predict_members(input_data)\n    if isinstance(predictions, Mapping):\n        lower = {\n            name: quantile(value, level, axis=0)\n            for name, value in predictions.items()\n        }\n        upper = {\n            name: quantile(value, 1 - level, axis=0)\n            for name, value in predictions.items()\n        }\n    else:\n        lower = quantile(predictions, level, axis=0)\n        upper = quantile(predictions, 1 - level, axis=0)\n    return lower, upper\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_covariance","title":"compute_covariance","text":"<pre><code>compute_covariance(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Compute the output covariance of the regressor.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to compute the covariance; shaped as \\((N, d)\\).</p> <p> TYPE: <code>NumberArray</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The posterior covariance matrix at the input points of shape \\((Np, Np)\\) with \\(p\\) the output dimension. The covariance between the \\(k\\)-th output at the \\(i\\)-th input point and the \\(l\\)-th output at the \\(j\\)-th input point is located at the \\(m\\)-th line and \\(n\\)-th column with \\(m=ip+k\\), \\(n=jp+l\\), \\(i,j\\in\\{0,\\ldots,N-1\\}\\) and \\(k,l\\in\\{0,\\ldots,p-1\\}\\).</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_covariance(  # noqa: D102\n    self,\n    input_data: NumberArray,\n) -&gt; NumberArray:\n    msg = (\n        \"The estimation of the covariance matrix for regressors \"\n        \"that are not based on a random process \"\n        \"is not implemented.\"\n    )\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_mean","title":"compute_mean","text":"<pre><code>compute_mean(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output mean of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output mean of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_mean(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    return self.__average(weights, predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_samples","title":"compute_samples","text":"<pre><code>compute_samples(\n    input_data: NumberArray, n_samples: int\n) -&gt; NumberArray\n</code></pre> <p>Generate samples from the random process.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The \\(N\\) input points of dimension \\(d\\) at which to observe the random process; shaped as <code>(N, d)</code>.</p> <p> TYPE: <code>NumberArray</code> </p> <code>n_samples</code> <p>The number of samples <code>M</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The output samples per output dimension shaped as <code>(N, M, p)</code> where <code>p</code> is the output dimension.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def compute_samples(  # noqa: D102\n    self,\n    input_data: NumberArray,\n    n_samples: int,\n) -&gt; NumberArray:\n    return self.predict_members(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_standard_deviation","title":"compute_standard_deviation","text":"<pre><code>compute_standard_deviation(\n    input_data: DataType,\n) -&gt; DataType\n</code></pre> <p>Compute the output standard deviation of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output standard deviation  of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_standard_deviation(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output standard deviation of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g.  `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output standard deviation  of the regressor.\n    \"\"\"\n    return self.compute_variance(input_data) ** 0.5\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.compute_variance","title":"compute_variance","text":"<pre><code>compute_variance(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output variance of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g.  <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output variance of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>@RegressionDataFormatters.format_dict\n@RegressionDataFormatters.format_samples()\ndef compute_variance(self, input_data: DataType) -&gt; DataType:  # noqa: D102\n    predictions = self.predict_members(input_data)\n    weights = self.evaluate_weights(input_data)\n    term1 = self.__average(weights, predictions**2)\n    term2 = self.__average(weights, predictions) ** 2\n    return term1 - term2\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.evaluate_weights","title":"evaluate_weights","text":"<pre><code>evaluate_weights(input_data: NumberArray) -&gt; NumberArray\n</code></pre> <p>Evaluate weights.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data with shape (size, n_input_data)</p> <p> TYPE: <code>NumberArray</code> </p> RETURNS DESCRIPTION <code>NumberArray</code> <p>The weights.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def evaluate_weights(self, input_data: NumberArray) -&gt; NumberArray:\n    \"\"\"Evaluate weights.\n\n    Args:\n        input_data: The input data with shape (size, n_input_data)\n\n    Returns:\n        The weights.\n    \"\"\"\n    weights = array([func(input_data) for func in self.weights])\n    return weights / npsum(weights, 0)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.learn","title":"learn","text":"<pre><code>learn(samples: list[int] | None = None) -&gt; None\n</code></pre> <p>Train the regressor from the learning dataset.</p> PARAMETER DESCRIPTION <code>samples</code> <p>The indices of the learning samples. If <code>None</code>, use the whole learning dataset</p> <p> TYPE: <code>list[int] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def learn(  # noqa: D102\n    self,\n    samples: list[int] | None = None,\n) -&gt; None:\n    self.weights = []\n    super().learn(samples)\n    if self.method in [self.CROSS_VALIDATION, self.LOO]:\n        n_folds = self.size\n        folds = array_split(self._samples, n_folds)\n    for index, algo in enumerate(self.algos):\n        if self.method == self.BOOTSTRAP:\n            new_samples = unique(\n                default_rng(1).choice(self._samples, len(self._samples))\n            )\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n        else:\n            fold = folds[index]\n            new_samples = npdelete(self._samples, fold)\n            other_samples = list(set(self._samples) - set(new_samples))\n            self.weights.append(self.__weight_function(other_samples))\n\n        algo.learn(new_samples.tolist())\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict","title":"predict","text":"<pre><code>predict(input_data: DataType) -&gt; DataType\n</code></pre> <p>Compute the output value(s) of the regressor.</p> <p>The user can specify the input data either as a NumPy array, e.g. <code>array([1., 2., 3.])</code> or as a dictionary, e.g. <code>{\"a\": array([1.]), \"b\": array([2., 3.])}</code>.</p> <p>The output data type will be consistent with the input data type.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output value(s) of the regressor.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/base_regressor_distribution.py</code> <pre><code>def predict(\n    self,\n    input_data: DataType,\n) -&gt; DataType:\n    \"\"\"Compute the output value(s) of the regressor.\n\n    The user can specify the input data either as a NumPy array,\n    e.g. `array([1., 2., 3.])`\n    or as a dictionary,\n    e.g. `{\"a\": array([1.]), \"b\": array([2., 3.])}`.\n\n    The output data type will be consistent with the input data type.\n\n    Args:\n        input_data: The input data.\n\n    Returns:\n        The output value(s) of the regressor.\n    \"\"\"\n    return self.algo.predict(input_data)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/distributions/regressor_distribution/#gemseo_mlearning.active_learning.distributions.regressor_distribution.RegressorDistribution.predict_members","title":"predict_members","text":"<pre><code>predict_members(input_data: DataType) -&gt; DataType\n</code></pre> <p>Predict the output value with the different machine learning algorithms.</p> <p>After prediction, the method stacks the results.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The input data, specified as either as a NumPy array or as dictionary of NumPy arrays indexed by inputs names. The NumPy array can be either a <code>(d,)</code> array representing a sample in dimension <code>d</code>, or a <code>(M, d)</code> array representing <code>M</code> samples in dimension <code>d</code>.</p> <p> TYPE: <code>DataType</code> </p> RETURNS DESCRIPTION <code>DataType</code> <p>The output data (dimension <code>p</code>) of <code>N</code> machine learning algorithms.     If <code>input_data.shape == (d,)</code>, then <code>output_data.shape == (N, p)</code>.     If <code>input_data.shape == (M,d)</code>, then <code>output_data.shape == (N,M,p)</code>.</p> Source code in <code>src/gemseo_mlearning/active_learning/distributions/regressor_distribution.py</code> <pre><code>def predict_members(self, input_data: DataType) -&gt; DataType:\n    \"\"\"Predict the output value with the different machine learning algorithms.\n\n    After prediction, the method stacks the results.\n\n    Args:\n        input_data: The input data,\n            specified as either as a NumPy array or as dictionary of NumPy arrays\n            indexed by inputs names.\n            The NumPy array can be either a `(d,)` array\n            representing a sample in dimension `d`,\n            or a `(M, d)` array representing `M` samples in dimension `d`.\n\n    Returns:\n        The output data (dimension `p`) of `N` machine learning algorithms.\n            If `input_data.shape == (d,)`, then `output_data.shape == (N, p)`.\n            If `input_data.shape == (M,d)`, then `output_data.shape == (N,M,p)`.\n    \"\"\"\n    predictions = [algo.predict(input_data) for algo in self.algos]\n    if isinstance(input_data, Mapping):\n        return {\n            name: stack([prediction[name] for prediction in predictions])\n            for name in predictions[0]\n        }\n    return stack(predictions)\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/","title":"Visualization","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/#gemseo_mlearning.active_learning.visualization","title":"visualization","text":"<p>Visualization tools for active learning.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/","title":"Acquisition view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view","title":"acquisition_view","text":"<p>An acquisition plot.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView","title":"AcquisitionView","text":"<pre><code>AcquisitionView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of points acquired during active learning.</p> <p>This visualization tool only works with a 2-dimensional input space.</p> PARAMETER DESCRIPTION <code>active_learning_algo</code> <p>The active learning algorithm using sequential acquisition.</p> <p> TYPE: <code>ActiveLearningAlgo</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>When the active learning algorithm uses parallel acquisition.</p> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm\n            using sequential acquisition.\n\n    Raises:\n        NotImplementedError: When the active learning algorithm uses parallel\n            acquisition.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.batch_size &gt; 1:\n        msg = (\n            \"AcquisitionView does not support active learning algorithm using \"\n            \"parallel acquisition.\"\n        )\n        raise NotImplementedError(msg)\n\n    self.__algo = active_learning_algo\n    self.__input_dimension = (\n        active_learning_algo.regressor_distribution.algo.input_dimension\n    )\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/acquisition_view/#gemseo_mlearning.active_learning.visualization.acquisition_view.AcquisitionView.draw","title":"draw","text":"<pre><code>draw(\n    new_point: RealArray | None = None,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Draw the points acquired through active learning.</p> <p>This visualization includes four surface plots representing variables of interest in function of the two inputs:</p> <ul> <li>(top left) the surface plot of the discipline,</li> <li>(top right) the surface plot of the acquisition criterion,</li> <li>(bottom left) the surface plot of the regressor,</li> <li>(bottom right) the standard deviation of the regressor.</li> </ul> <p>The \\(N\\) data used to initialize the regressor are represented by small dots, the point optimizing the acquisition criterion is represented by a big dot (this will be the next point to learn) and the points added by the active learning procedure are represented by plus signs and labeled by their position in the learning dataset.</p> PARAMETER DESCRIPTION <code>new_point</code> <p>The new point to be acquired.</p> <p> TYPE: <code>RealArray | None</code> DEFAULT: <code>None</code> </p> <code>discipline</code> <p>The discipline for which <code>n_test**2</code> evaluations will be done. If <code>None</code>, do not plot the discipline, i.e. the first subplot. In particular, if the discipline is costly, it is better to leave this argument to <code>None</code>.</p> <p> TYPE: <code>Discipline | None</code> DEFAULT: <code>None</code> </p> <code>n_test</code> <p>The number of points per dimension.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>filled</code> <p>Whether to plot filled contours. Otherwise, plot contour lines.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show</code> <p>Whether to display the view.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>file_path</code> <p>The file path to save the view. If empty, do not save it.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Figure</code> <p>The acquisition view.</p> Source code in <code>src/gemseo_mlearning/active_learning/visualization/acquisition_view.py</code> <pre><code>def draw(\n    self,\n    new_point: RealArray | None = None,\n    discipline: Discipline | None = None,\n    filled: bool = True,\n    n_test: int = 30,\n    show: bool = True,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Draw the points acquired through active learning.\n\n    This visualization includes four surface plots\n    representing variables of interest in function of the two inputs:\n\n    - (top left) the surface plot of the discipline,\n    - (top right) the surface plot of the acquisition criterion,\n    - (bottom left) the surface plot of the regressor,\n    - (bottom right) the standard deviation of the regressor.\n\n    The $N$ data used to initialize the regressor are represented by small dots,\n    the point optimizing the acquisition criterion is represented by a big dot\n    (this will be the next point to learn)\n    and the points added by the active learning procedure are represented\n    by plus signs and labeled by their position in the learning dataset.\n\n    Args:\n        new_point: The new point to be acquired.\n        discipline: The discipline for which `n_test**2` evaluations will be done.\n            If `None`,\n            do not plot the discipline, i.e. the first subplot.\n            In particular,\n            if the discipline is costly,\n            it is better to leave this argument to `None`.\n        n_test: The number of points per dimension.\n        filled: Whether to plot filled contours.\n            Otherwise, plot contour lines.\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n\n    Returns:\n        The acquisition view.\n    \"\"\"\n    # Create grid.\n    input_space = self.__algo.input_space\n    lower_bounds = input_space.get_lower_bounds()\n    upper_bounds = input_space.get_upper_bounds()\n    test_x1 = linspace(lower_bounds[0], upper_bounds[0], n_test)\n    test_x2 = linspace(lower_bounds[1], upper_bounds[1], n_test)\n    grid = array(meshgrid(test_x1, test_x2)).T.reshape(-1, 2)\n\n    # Generate data.\n    distribution = self.__algo.regressor_distribution\n    final_dataset = distribution.algo.learning_set\n    #    The learning input samples.\n    points = final_dataset.input_dataset.to_numpy()\n    points_x = points[:, 0]\n    points_y = points[:, 1]\n    #    The predictions, the standard deviations and the criterion values.\n    predictions = distribution.predict(grid).reshape((n_test, n_test)).T\n    std = distribution.compute_standard_deviation(grid).reshape((n_test, n_test)).T\n    acquisition_criterion = self.__algo.acquisition_criterion.original.func\n    criterion_values = acquisition_criterion(grid).reshape((n_test, n_test)).T\n    x_name, y_name = final_dataset.input_names\n    output_name = final_dataset.output_names[0]\n    #    The observations if the discipline is available.\n    observations = None\n    if discipline is not None:\n        observations = zeros((n_test, n_test))\n        for i in range(n_test):\n            for j in range(n_test):\n                xij = array([test_x1[j], test_x2[i]])\n                input_data = {x_name: array([xij[0]]), y_name: array([xij[1]])}\n                observations[i, j] = discipline.execute(input_data)[output_name][0]\n\n    # Create figure and sub-figures.\n    fig, axes = plt.subplots(2, 2)\n    titles = [\n        [\"Discipline\", self.__algo.acquisition_criterion.__class__.__name__],\n        [\"Surrogate\", \"Standard deviation\"],\n    ]\n    data = [[observations, criterion_values], [predictions, std]]\n    cf = []\n    color = \"white\" if filled else \"black\"\n    n_initial_samples = self.__algo.n_initial_samples\n    contour_method = \"contourf\" if filled else \"contour\"\n    for i in range(2):\n        for j in range(2):\n            if (i, j) == (0, 0) and discipline is None:\n                continue\n\n            ax = axes[i, j]\n            cf.append(getattr(ax, contour_method)(test_x1, test_x2, data[i][j]))\n            for x, y in zip(\n                points_x[:n_initial_samples],\n                points_y[:n_initial_samples],\n                strict=False,\n            ):\n                ax.plot(x, y, \".\", ms=2, color=color)\n\n            for index, (x, y) in enumerate(\n                zip(\n                    points_x[n_initial_samples:],\n                    points_y[n_initial_samples:],\n                    strict=False,\n                )\n            ):\n                ax.plot(x, y, \"+\", color=color)\n                ax.annotate(\n                    str(n_initial_samples + 1 + index),\n                    (0.05 + x, 0.05 + y),\n                    color=color,\n                )\n\n            if new_point is not None:\n                ax.plot(*new_point, \"o\", color=color)\n\n            ax.set_title(titles[i][j])\n\n    if discipline is None:\n        fig.delaxes(axes.flatten()[0])\n        fig.colorbar(cf[0], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[1, 0])\n        fig.colorbar(cf[2], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n    else:\n        fig.colorbar(cf[0], ax=axes[:, 0])\n        axes[0, 0].set_xticks([])\n        fig.colorbar(cf[1], ax=axes[0, 1])\n        axes[0, 1].set_xticks([])\n        axes[0, 1].set_yticks([])\n        fig.colorbar(cf[3], ax=axes[1, 1])\n        axes[1, 1].set_yticks([])\n\n    save_show_figure(fig, show, file_path)\n    return fig\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/","title":"Qoi history view","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view","title":"qoi_history_view","text":"<p>History view.</p>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView","title":"QOIHistoryView","text":"<pre><code>QOIHistoryView(active_learning_algo: ActiveLearningAlgo)\n</code></pre> <p>View of the history of the quantity of interest (QOI).</p> PARAMETER DESCRIPTION <code>active_learning_algo</code> <p>The active learning algorithm.</p> <p> TYPE: <code>ActiveLearningAlgo</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>When there is no quantity of interest associated with the acquisition criterion.</p> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def __init__(self, active_learning_algo: ActiveLearningAlgo) -&gt; None:\n    \"\"\"\n    Args:\n        active_learning_algo: The active learning algorithm.\n\n    Raises:\n        ValueError: When there is no quantity of interest\n            associated with the acquisition criterion.\n    \"\"\"  # noqa: D205, D212\n    if active_learning_algo.acquisition_criterion.qoi is None:\n        msg = (\n            \"There is no quantity of interest \"\n            \"associated with the acquisition criterion \"\n            f\"{active_learning_algo.acquisition_criterion.__class__.__name__}.\"\n        )\n        raise ValueError(msg)\n\n    self.__algo = active_learning_algo\n</code></pre>"},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/active_learning/visualization/qoi_history_view/#gemseo_mlearning.active_learning.visualization.qoi_history_view.QOIHistoryView.draw","title":"draw","text":"<pre><code>draw(\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any\n) -&gt; Lines\n</code></pre> <p>Draw the QOI history.</p> PARAMETER DESCRIPTION <code>show</code> <p>Whether to display the view.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>file_path</code> <p>The file path to save the view. If empty, do not save it.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> <code>label</code> <p>The label for the QOI. If empty, use the name of the acquisition criterion family.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>add_markers</code> <p>Whether to add markers.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>**options</code> <p>The options to create the Lines object.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Lines</code> <p>A view of the QOI history.</p> Source code in <code>src/gemseo_mlearning/active_learning/visualization/qoi_history_view.py</code> <pre><code>def draw(\n    self,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    label: str = \"\",\n    add_markers: bool = True,\n    **options: Any,\n) -&gt; Lines:\n    \"\"\"Draw the QOI history.\n\n    Args:\n        show: Whether to display the view.\n        file_path: The file path to save the view.\n            If empty, do not save it.\n        label: The label for the QOI.\n            If empty, use the name of the acquisition criterion family.\n        add_markers: Whether to add markers.\n        **options: The options to create the\n            [Lines][gemseo.post.dataset.lines.Lines] object.\n\n    Returns:\n        A view of the QOI history.\n    \"\"\"\n    if not label:\n        label = self.__algo.acquisition_criterion_family_name\n\n    x_label = \"Number of evaluations\"\n    n_evaluations_history, qoi_history = self.__algo.qoi_history\n    qoi_history = [qoi[0] for qoi in qoi_history]\n    dataset = IODataset()\n    dataset.add_variable(x_label, array(n_evaluations_history)[:, newaxis])\n    dataset.add_variable(label, array(qoi_history)[:, newaxis])\n    lines = Lines(\n        dataset,\n        variables=[label],\n        abscissa_variable=x_label,\n        add_markers=add_markers,\n        **options,\n    )\n    lines.marker = \".\"\n    lines.execute(show=show, save=file_path != \"\", file_path=file_path)\n    return lines\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/","title":"Algos","text":""},{"location":"reference/gemseo_mlearning/algos/#gemseo_mlearning.algos","title":"algos","text":"<p>Wrappers for algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/#gemseo_mlearning.algos.opt","title":"opt","text":"<p>Wrappers for optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/","title":"Sbo settings","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings","title":"sbo_settings","text":"<p>Settings for the surrogate-based optimization algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion","title":"AcquisitionCriterion","text":"<p>               Bases: <code>StrEnum</code></p> <p>An acquisition criterion.</p> <p>In the following, the training output values already used and the random output of the surrogate model at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\) and \\(Y(x)\\). The expectation and the standard deviation of \\(Y(x)\\) are respectively denoted \\(\\mathbb{E}[Y(x)]\\) and \\(\\mathbb{S}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.CB","title":"CB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CB = 'CB'\n</code></pre> <p>The confidence bound.</p> <p>The acquisition criterion is \\(\\mathbb{E}[Y(x)]-3\\mathbb{S}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.EI","title":"EI  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EI = 'EI'\n</code></pre> <p>The expected improvement.</p> <p>The acquisition criterion is \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.AcquisitionCriterion.Output","title":"Output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Output = 'Output'\n</code></pre> <p>The mean output.</p> <p>The acquisition criterion is \\(\\mathbb{E}[Y(x)]\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/sbo_settings/#gemseo_mlearning.algos.opt.sbo_settings.SBO_Settings","title":"SBO_Settings","text":"<p>               Bases: <code>BaseOptimizerSettings</code></p> <p>The settings for the surrogate-based optimization algorithm.</p> PARAMETER DESCRIPTION <code>enable_progress_bar</code> <p>Whether to enable the progress bar in the optimization log.</p> <p>If <code>None</code>, use the global value of <code>enable_progress_bar</code> (see the <code>configure</code> function to change it globally).</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>eq_tolerance</code> <p>The tolerance on the equality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>ineq_tolerance</code> <p>The tolerance on the inequality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>log_problem</code> <p>Whether to log the definition and result of the problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_time</code> <p>The maximum runtime in seconds, disabled if 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>normalize_design_space</code> <p>Whether to normalize the design space variables between 0 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_bar_data_name</code> <p>The name of a :class:<code>.BaseProgressBarData</code> class to define the data of an evaluation problem to be displayed in the progress bar.</p> <p> TYPE: <code>ProgressBarDataName</code> DEFAULT: <code>'ProgressBarData'</code> </p> <code>reset_iteration_counters</code> <p>Whether to reset the iteration counters before each execution.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>round_ints</code> <p>Whether to round the integer variables.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>store_jacobian</code> <p>Whether to store the Jacobian matrices in the database.</p> <pre><code>This argument is ignored when the ``use_database`` option is ``False``.\nIf a gradient-based algorithm is used,\nthis option cannot be set along with kkt options.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_database</code> <p>Whether to wrap the functions in the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_one_line_progress_bar</code> <p>Whether to log the progress bar on a single line.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ftol_rel</code> <p>The relative tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>ftol_abs</code> <p>The absolute tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_iter</code> <p>The maximum number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>scaling_threshold</code> <p>The threshold on the reference function value that triggers scaling.</p> <p>If <code>None</code>, do not scale the functions.</p> <p> TYPE: <code>Annotated[float, Ge] | None</code> DEFAULT: <code>None</code> </p> <code>stop_crit_n_x</code> <p>The minimum number of design vectors to consider in the stopping criteria.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>xtol_rel</code> <p>The relative tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>xtol_abs</code> <p>The absolute tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>acquisition_algorithm</code> <p>The name of the algorithm to optimize the data acquisition criterion.             If empty, use the default algorithm with its default settings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>acquisition_settings</code> <p>The settings of the algorithm             to optimize the data acquisition criterion.             Ignored when <code>acquisition_algorithm</code> is empty.</p> <p> TYPE: <code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>criterion</code> <p>The acquisition criterion.</p> <p> TYPE: <code>AcquisitionCriterion</code> DEFAULT: <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> </p> <code>doe_algorithm</code> <p>The name of the DOE algorithm for the initial sampling.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'OT_OPT_LHS'</code> </p> <code>doe_settings</code> <p>The settings of the DOE algorithm for the initial sampling.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>doe_size</code> <p>Either the initial DOE size or 0 if it is inferred from <code>doe_settings</code>.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criteria in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>regression_algorithm</code> <p>The regression algorithm.             Either the name of the regression algorithm             approximating the objective function over the design space             or the regression algorithm itself.</p> <p> TYPE: <code>str | BaseRegressor</code> DEFAULT: <code>'OTGaussianProcessRegressor'</code> </p> <code>regression_file_path</code> <p>The path to the file to save the regression model.             If empty, do not save the regression model.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> <code>regression_settings</code> <p>The settings of the regression algorithm.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>Mapping[str, Optional[Any]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/","title":"Surrogate based optimization","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization","title":"surrogate_based_optimization","text":"<p>A library for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedAlgorithmDescription","title":"SurrogateBasedAlgorithmDescription  <code>dataclass</code>","text":"<pre><code>SurrogateBasedAlgorithmDescription(\n    library_name: str = \"gemseo-mlearning\",\n)\n</code></pre> <p>               Bases: <code>OptimizationAlgorithmDescription</code></p> <p>The description of a surrogate-based optimization algorithm.</p> PARAMETER DESCRIPTION <code>algorithm_name</code> <p> TYPE: <code>str</code> </p> <code>internal_algorithm_name</code> <p> TYPE: <code>str</code> </p> <code>library_name</code> <p> TYPE: <code>str</code> DEFAULT: <code>'gemseo-mlearning'</code> </p> <code>description</code> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>website</code> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>Settings</code> <p> TYPE: <code>str</code> DEFAULT: <code>&lt;class 'gemseo.algos.opt.base_optimizer_settings.BaseOptimizerSettings'&gt;</code> </p> <code>handle_integer_variables</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>handle_equality_constraints</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>handle_inequality_constraints</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>handle_multiobjective</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>positive_constraints</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>for_linear_problems</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p> <code>require_gradient</code> <p> TYPE: <code>str</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/gemseo_mlearning/algos/opt/surrogate_based_optimization/#gemseo_mlearning.algos.opt.surrogate_based_optimization.SurrogateBasedOptimization","title":"SurrogateBasedOptimization","text":"<p>               Bases: <code>BaseOptimizationLibrary[SBO_Settings]</code></p> <p>A wrapper for surrogate-based optimization.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/","title":"Core","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/#gemseo_mlearning.algos.opt.core","title":"core","text":"<p>Core functionalities for the optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/","title":"Surrogate based optimizer","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer","title":"surrogate_based_optimizer","text":"<p>A class for surrogate-based optimization.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer","title":"SurrogateBasedOptimizer","text":"<pre><code>SurrogateBasedOptimizer(\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_settings: Mapping[\n        str, DriverLibrarySettingType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: str | BaseRegressor = __name__,\n    regression_settings: Mapping[\n        str, MLAlgoParameterType\n    ] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    **acquisition_settings: DriverLibrarySettingType\n)\n</code></pre> <p>An optimizer based on surrogate models.</p> PARAMETER DESCRIPTION <code>acquisition_algorithm</code> <p>The name of the algorithm to optimize the data acquisition criterion. N.B. this algorithm must handle integers if some of the optimization variables are integers.</p> <p> TYPE: <code>str</code> </p> <code>problem</code> <p>The optimization problem.</p> <p> TYPE: <code>OptimizationProblem</code> </p> <code>doe_size</code> <p>Either the size of the initial DOE or 0 if the size is inferred from doe_settings. This argument is ignored when regression_algorithm is a BaseRegressor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>doe_algorithm</code> <p>The name of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'OT_OPT_LHS'</code> </p> <code>doe_settings</code> <p>The settings of the algorithm for the initial sampling. This argument is ignored when regression_algorithm is a BaseRegressor.</p> <p> TYPE: <code>Mapping[str, DriverLibrarySettingType]</code> DEFAULT: <code>READ_ONLY_EMPTY_DICT</code> </p> <code>regression_algorithm</code> <p>Either the name of the regression algorithm approximating the objective function over the design space or the regression algorithm itself.</p> <p> TYPE: <code>str | BaseRegressor</code> DEFAULT: <code>__name__</code> </p> <code>regression_settings</code> <p>The settings of the regression algorithm. If transformer is missing, use :attr:<code>.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER</code>. This argument is ignored when regression_algorithm is a BaseRegressor.</p> <p> TYPE: <code>Mapping[str, MLAlgoParameterType]</code> DEFAULT: <code>READ_ONLY_EMPTY_DICT</code> </p> <code>regression_file_path</code> <p>The path to the file to save the regression model. If empty, do not save the regression model.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> <code>**acquisition_settings</code> <p>The settings of the algorithm to optimize the data acquisition criterion.</p> <p> TYPE: <code>DriverLibrarySettingType</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def __init__(\n    self,\n    problem: OptimizationProblem,\n    acquisition_algorithm: str,\n    doe_size: int = 0,\n    doe_algorithm: str = \"OT_OPT_LHS\",\n    doe_settings: Mapping[str, DriverLibrarySettingType] = READ_ONLY_EMPTY_DICT,\n    regression_algorithm: str | BaseRegressor = OTGaussianProcessRegressor.__name__,\n    regression_settings: Mapping[str, MLAlgoParameterType] = READ_ONLY_EMPTY_DICT,\n    regression_file_path: str | Path = \"\",\n    **acquisition_settings: DriverLibrarySettingType,\n) -&gt; None:\n    \"\"\"\n    Args:\n        acquisition_algorithm: The name of the algorithm to optimize the data\n            acquisition criterion.\n            N.B. this algorithm must handle integers if some of the optimization\n            variables are integers.\n        problem: The optimization problem.\n        doe_size: Either the size of the initial DOE\n            or 0 if the size is inferred from doe_settings.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_algorithm: The name of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        doe_settings: The settings of the algorithm for the initial sampling.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_algorithm: Either the name of the regression algorithm\n            approximating the objective function over the design space\n            or the regression algorithm itself.\n        regression_settings: The settings of the regression algorithm.\n            If transformer is missing,\n            use :attr:`.BaseMLRegressionAlgo.DEFAULT_TRANSFORMER`.\n            This argument is ignored\n            when regression_algorithm is a\n            [BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\n        regression_file_path: The path to the file to save the regression model.\n            If empty, do not save the regression model.\n        **acquisition_settings: The settings of the algorithm to optimize\n            the data acquisition criterion.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__problem = problem\n    database = problem.database\n    self.__initial_input_samples = tuple(database.keys())\n    if isinstance(regression_algorithm, BaseRegressor):\n        self.__dataset = regression_algorithm.learning_set\n    else:\n        # Store max_iter as it will be overwritten by DOELibrary\n        max_iter = problem.evaluation_counter.maximum\n        settings = dict(doe_settings)\n        if doe_size &gt; 0 and \"n_samples\" not in settings:\n            settings[\"n_samples\"] = doe_size\n\n        # Store the listeners as they will be cleared by DOELibrary.\n        new_iter_listeners, store_listeners = database.clear_listeners()\n        with LoggingContext(logging.getLogger(\"gemseo\")):\n            DOELibraryFactory().execute(\n                problem, algo_name=doe_algorithm, **settings\n            )\n\n        for listener in new_iter_listeners:\n            database.add_new_iter_listener(listener)\n\n        for listener in store_listeners:\n            database.add_store_listener(listener)\n\n        self.__dataset = problem.to_dataset(opt_naming=False)\n        if self.__initial_input_samples:\n            self.__dataset = self.__dataset[len(self.__initial_input_samples) :]\n\n        regression_settings_ = {\"transformer\": {\"inputs\": \"MinMaxScaler\"}}\n        regression_settings_.update(dict(regression_settings))\n        regression_algorithm = RegressorFactory().create(\n            regression_algorithm,\n            self.__dataset,\n            **regression_settings_,\n        )\n        # Add the first iteration to the current_iter reset by DOELibrary.\n        problem.evaluation_counter.current += 1\n        # And restore max_iter.\n        problem.evaluation_counter.maximum = max_iter\n\n    self.__active_learning_algo = ActiveLearningAlgo(\n        Minimum.__name__, problem.design_space, regression_algorithm\n    )\n    if acquisition_algorithm:\n        self.__active_learning_algo.set_acquisition_algorithm(\n            acquisition_algorithm, **acquisition_settings\n        )\n    self.__regression_file_path = regression_file_path\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer/#gemseo_mlearning.algos.opt.core.surrogate_based_optimizer.SurrogateBasedOptimizer.execute","title":"execute","text":"<pre><code>execute(number_of_acquisitions: int) -&gt; str\n</code></pre> <p>Execute the surrogate-based optimization.</p> PARAMETER DESCRIPTION <code>number_of_acquisitions</code> <p>The number of learning points to be acquired.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The termination message.</p> Source code in <code>src/gemseo_mlearning/algos/opt/core/surrogate_based_optimizer.py</code> <pre><code>def execute(self, number_of_acquisitions: int) -&gt; str:\n    \"\"\"Execute the surrogate-based optimization.\n\n    Args:\n        number_of_acquisitions: The number of learning points to be acquired.\n\n    Returns:\n        The termination message.\n    \"\"\"\n    regressor_distribution = self.__active_learning_algo.regressor_distribution\n    regressor_distribution.learn()\n    message = self.__STOP_BECAUSE_MAX_ACQUISITIONS\n    for _ in range(number_of_acquisitions):\n        input_data = self.__active_learning_algo.find_next_point()[0]\n        hashed_input_data = HashableNdarray(input_data)\n        if hashed_input_data in self.__problem.database and (\n            hashed_input_data not in self.__initial_input_samples\n        ):\n            message = self.__STOP_BECAUSE_ALREADY_KNOWN\n            break\n\n        output_data = self.__problem.evaluate_functions(\n            design_vector=input_data, design_vector_is_normalized=False\n        )[0]\n        extra_learning_set = IODataset()\n        variable_names_to_n_components = regressor_distribution.algo.sizes\n        extra_learning_set.add_group(\n            group_name=IODataset.INPUT_GROUP,\n            data=input_data[newaxis],\n            variable_names=regressor_distribution.input_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        output_names = regressor_distribution.output_names\n        extra_learning_set.add_group(\n            group_name=IODataset.OUTPUT_GROUP,\n            data=hstack([output_data[output_name] for output_name in output_names])[\n                newaxis\n            ],\n            variable_names=output_names,\n            variable_names_to_n_components=variable_names_to_n_components,\n        )\n        self.__dataset = concat(\n            [regressor_distribution.algo.learning_set, extra_learning_set],\n            ignore_index=True,\n        )\n        self.__dataset = self.__dataset.map(lambda x: x.real)\n        regressor_distribution.change_learning_set(self.__dataset)\n        self.__active_learning_algo.update_problem()\n\n        if self.__regression_file_path:\n            with Path(self.__regression_file_path).open(\"wb\") as file:\n                pickle.dump(regressor_distribution.algo, file)\n\n    return message\n</code></pre>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/","title":"Smt","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/#gemseo_mlearning.algos.opt.smt","title":"smt","text":"<p>Surrogate-based optimization using SMT.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/","title":"Ego settings","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings","title":"ego_settings","text":"<p>Settings for the multi-start algorithm.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion","title":"AcquisitionCriterion","text":"<p>               Bases: <code>StrEnum</code></p> <p>An acquisition criterion.</p> <p>In the following, the training output values already used and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.EI","title":"EI  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EI = 'EI'\n</code></pre> <p>The expected improvement.</p> <p>The acquisition criterion is \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y,0]\\) where \\(Y\\) is a Gaussian random variable with mean \\(\\mu(x)\\) and standard deviation \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.LCB","title":"LCB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LCB = 'LCB'\n</code></pre> <p>The lower confidence bound.</p> <p>The acquisition criterion is \\(\\mu(x)-3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.AcquisitionCriterion.SBO","title":"SBO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SBO = 'SBO'\n</code></pre> <p>The surrogate-based optimization.</p> <p>The acquisition criterion is \\(\\mu(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy","title":"ParallelStrategy","text":"<p>               Bases: <code>StrEnum</code></p> <p>The strategy to set the outputs of the virtual points for parallel acquisition.</p> <p>In the following, the output variable, the training output values already used, and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(y\\), \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.CLmin","title":"CLmin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLmin = 'CLmin'\n</code></pre> <p>The minimum constant liar.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\min \\{y_1,\\ldots,y_n\\}\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KB","title":"KB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KB = 'KB'\n</code></pre> <p>The Kriging believer.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBLB","title":"KBLB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBLB = 'KBLB'\n</code></pre> <p>The Kriging believer lower bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)-3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBRand","title":"KBRand  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBRand = 'KBRand'\n</code></pre> <p>The Kriging believer random bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)+\\kappa(x)\\sigma(x)\\) where \\(\\kappa(x)\\) is the realization of a random variable distributed according to the standard normal distribution.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.ParallelStrategy.KBUB","title":"KBUB  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KBUB = 'KBUB'\n</code></pre> <p>The Kriging believer upper bound.</p> <p>The output of the virtual point at \\(x\\) is defined by \\(\\mu(x)+3\\sigma(x)\\).</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.SMT_EGO_Settings","title":"SMT_EGO_Settings","text":"<p>               Bases: <code>BaseOptimizerSettings</code></p> <p>The settings of the SMT's SBO algorithm.</p> PARAMETER DESCRIPTION <code>enable_progress_bar</code> <p>Whether to enable the progress bar in the optimization log.</p> <p>If <code>None</code>, use the global value of <code>enable_progress_bar</code> (see the <code>configure</code> function to change it globally).</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>eq_tolerance</code> <p>The tolerance on the equality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>ineq_tolerance</code> <p>The tolerance on the inequality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>log_problem</code> <p>Whether to log the definition and result of the problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_time</code> <p>The maximum runtime in seconds, disabled if 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>normalize_design_space</code> <p>Whether to normalize the design space variables between 0 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress_bar_data_name</code> <p>The name of a :class:<code>.BaseProgressBarData</code> class to define the data of an evaluation problem to be displayed in the progress bar.</p> <p> TYPE: <code>ProgressBarDataName</code> DEFAULT: <code>'ProgressBarData'</code> </p> <code>reset_iteration_counters</code> <p>Whether to reset the iteration counters before each execution.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>round_ints</code> <p>Whether to round the integer variables.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>store_jacobian</code> <p>Whether to store the Jacobian matrices in the database.</p> <pre><code>This argument is ignored when the ``use_database`` option is ``False``.\nIf a gradient-based algorithm is used,\nthis option cannot be set along with kkt options.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_database</code> <p>Whether to wrap the functions in the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_one_line_progress_bar</code> <p>Whether to log the progress bar on a single line.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ftol_rel</code> <p>The relative tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>ftol_abs</code> <p>The absolute tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_iter</code> <p>The maximum number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>scaling_threshold</code> <p>The threshold on the reference function value that triggers scaling.</p> <p>If <code>None</code>, do not scale the functions.</p> <p> TYPE: <code>Annotated[float, Ge] | None</code> DEFAULT: <code>None</code> </p> <code>stop_crit_n_x</code> <p>The minimum number of design vectors to consider in the stopping criteria.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>xtol_rel</code> <p>The relative tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>xtol_abs</code> <p>The absolute tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>criterion</code> <p>The acquisition criterion.</p> <p> TYPE: <code>AcquisitionCriterion</code> DEFAULT: <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> </p> <code>enable_tunneling</code> <p>Whether to enable the penalization of points that have been already evaluated in EI criterion.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_doe</code> <p>The number of points of the initial LHS DOE to train the surrogate.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>n_max_optim</code> <p>The maximum number of iterations for each sub-optimization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>n_parallel</code> <p>The number of points to be acquired in parallel; if <code>1</code>, acquire points sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_start</code> <p>The number of sub-optimizations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>qEI</code> <p>The strategy to acquire points in parallel.</p> <p> TYPE: <code>ParallelStrategy</code> DEFAULT: <code>&lt;ParallelStrategy.KBUB: 'KBUB'&gt;</code> </p> <code>random_state</code> <p>The Numpy RandomState object or seed number which controls random draws.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>surrogate</code> <p>The SMT Kriging-based surrogate model used internally; either an instance of the surrogate before training or its class name.</p> <p> TYPE: <code>Surrogate | SurrogateModel</code> DEFAULT: <code>&lt;Surrogate.KRG: 'KRG'&gt;</code> </p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate","title":"Surrogate","text":"<p>               Bases: <code>StrEnum</code></p> <p>A surrogate model.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.GPX","title":"GPX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GPX = 'GPX'\n</code></pre> <p>Kriging based on the <code>egobox</code> library.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KPLS","title":"KPLS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KPLS = 'KPLS'\n</code></pre> <p>Kriging using partial least squares (PLS) to reduce the input dimension.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KPLSK","title":"KPLSK  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KPLSK = 'KPLSK'\n</code></pre> <p>A variant of KPLS.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.KRG","title":"KRG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KRG = 'KRG'\n</code></pre> <p>Kriging.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/ego_settings/#gemseo_mlearning.algos.opt.smt.ego_settings.Surrogate.MGP","title":"MGP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MGP = 'MGP'\n</code></pre> <p>A marginal Gaussian process (MGP) regressor.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/","title":"Smt ego","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego","title":"smt_ego","text":"<p>The efficient global optimization (EGO) algorithm of SMT.</p>"},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/algos/opt/smt/smt_ego/#gemseo_mlearning.algos.opt.smt.smt_ego.SMTEGO","title":"SMTEGO","text":"<pre><code>SMTEGO(algo_name: str = 'SMT_EGO')\n</code></pre> <p>               Bases: <code>BaseOptimizationLibrary[SMT_EGO_Settings]</code></p> <p>Surrogate-based optimizers from SMT.</p> Notes <p>The missing current values of the :class:<code>.DesignSpace</code> attached to the :class:<code>.OptimizationProblem</code> are automatically initialized with the method :meth:<code>.DesignSpace.initialize_missing_current_values</code>.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> PARAMETER DESCRIPTION <code>algo_name</code> <p>The algorithm name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'SMT_EGO'</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>When the algorithm is not in the library.</p> Source code in <code>src/gemseo_mlearning/algos/opt/smt/smt_ego.py</code> <pre><code>def __init__(self, algo_name: str = \"SMT_EGO\") -&gt; None:  # noqa: D107\n    super().__init__(algo_name)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/","title":"Problems","text":""},{"location":"reference/gemseo_mlearning/problems/#gemseo_mlearning.problems","title":"problems","text":"<p>Use cases to benchmark and illustrate active learning algorithms.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/","title":"Branin","text":""},{"location":"reference/gemseo_mlearning/problems/branin/#gemseo_mlearning.problems.branin","title":"branin","text":"<p>The Branin use case to benchmark and illustrate active learning algorithms.</p> <p>The Branin function \\(\\(f(x_1,x_2) = \\left(15x_2 - \\frac{5.1}{4\\pi^2}(15x_1-5)^2 + \\frac{5}{\\pi}(15x_1-5)-6\\right)^2 + \\left(10- \\frac{10}{8\\pi}\\right)\\cos(15x_1-5) +10\\)\\) is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([0,1]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/","title":"Branin discipline","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline","title":"branin_discipline","text":"<p>The Branin function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline.BraninDiscipline","title":"BraninDiscipline","text":"<pre><code>BraninDiscipline()\n</code></pre> <p>               Bases: <code>Discipline</code></p> <p>The Branin function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_input_data.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_discipline/#gemseo_mlearning.problems.branin.branin_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/","title":"Branin function","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function","title":"branin_function","text":"<p>The Branin function.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function.BraninFunction","title":"BraninFunction","text":"<pre><code>BraninFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Branin function.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Branin\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_function/#gemseo_mlearning.problems.branin.branin_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/","title":"Branin problem","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem","title":"branin_problem","text":"<p>A problem connecting the Branin function with its input space.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_problem/#gemseo_mlearning.problems.branin.branin_problem.BraninProblem","title":"BraninProblem","text":"<pre><code>BraninProblem(use_uncertain_space: bool = True)\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Branin function with its input space.</p> PARAMETER DESCRIPTION <code>use_uncertain_space</code> <p>Whether to consider the input space as an uncertain space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_problem.py</code> <pre><code>def __init__(self, use_uncertain_space: bool = True) -&gt; None:\n    \"\"\"\n    Args:\n        use_uncertain_space: Whether to consider the input space\n            as an uncertain space.\n    \"\"\"  # noqa: D205 D212\n    input_space = BraninSpace()\n    if not use_uncertain_space:\n        input_space = input_space.to_design_space()\n\n    super().__init__(input_space)\n    self.objective = BraninFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/","title":"Branin space","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space","title":"branin_space","text":"<p>The uncertain space used in the Branin use case.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/branin/branin_space/#gemseo_mlearning.problems.branin.branin_space.BraninSpace","title":"BraninSpace","text":"<pre><code>BraninSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Branin use case.</p> Source code in <code>src/gemseo_mlearning/problems/branin/branin_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(f\"x{index + 1}\", \"OTUniformDistribution\")\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions","title":"functions","text":"<p>The Branin function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Branin function.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input values.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The value of the gradient of the Branin function.</p> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The value of the gradient of the Branin function.\n    \"\"\"\n    x0 = 15 * x[0] - 5\n    tmp = 15 * x[1] - __A * x0**2 + __B * x0 - 6\n    return array([-15 * (__C * sin(x0) + 2 * tmp * (2 * __A * x0 - __B)), 30 * tmp])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/branin/functions/#gemseo_mlearning.problems.branin.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Branin function.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input values.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The output value.</p> Source code in <code>src/gemseo_mlearning/problems/branin/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Branin function.\n\n    Args:\n        x: The input values.\n\n    Returns:\n        The output value.\n    \"\"\"\n    x0 = 15 * x[0] - 5\n    return (15 * x[1] - __A * x0**2 + __B * x0 - 6) ** 2 + __C * cos(x0) + 10\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/","title":"Rosenbrock","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/#gemseo_mlearning.problems.rosenbrock","title":"rosenbrock","text":"<p>The Rosenbrock use case to benchmark and illustrate active learning algorithms.</p> <p>The Rosenbrock function $$f(x_1,x_2) = 100(x_2-x_1^2)^2 + (1-x_1)^2 $$ is commonly studied through the random variable \\(Y=f(X_1,X_2)\\) where \\(X_1\\) and \\(X_2\\) are independent random variables uniformly distributed over \\([-2,2]\\).</p> <p>See [@molga2005test].</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions","title":"functions","text":"<p>The Rosenbrock function and its gradient.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_gradient","title":"compute_gradient","text":"<pre><code>compute_gradient(x: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the gradient of the Rosenbrock function.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input value.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The value of the gradient of the Rosenbrock function.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_gradient(x: ndarray) -&gt; ndarray:\n    \"\"\"Compute the gradient of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The value of the gradient of the Rosenbrock function.\n    \"\"\"\n    return array([\n        4 * __A * (x[0] ** 3 - x[0] * x[1]) + 2 * (x[0] - 1),\n        2 * __A * (x[1] - x[0] ** 2),\n    ])\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/functions/#gemseo_mlearning.problems.rosenbrock.functions.compute_output","title":"compute_output","text":"<pre><code>compute_output(x: ndarray) -&gt; float\n</code></pre> <p>Compute the output of the Rosenbrock function.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input value.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The output value.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/functions.py</code> <pre><code>def compute_output(x: ndarray) -&gt; float:\n    \"\"\"Compute the output of the Rosenbrock function.\n\n    Args:\n        x: The input value.\n\n    Returns:\n        The output value.\n    \"\"\"\n    return (1 - x[0]) ** 2 + __A * (x[1] - x[0] ** 2) ** 2\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/","title":"Rosenbrock discipline","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline","title":"rosenbrock_discipline","text":"<p>The Rosenbrock function as a discipline.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline.RosenbrockDiscipline","title":"RosenbrockDiscipline","text":"<pre><code>RosenbrockDiscipline()\n</code></pre> <p>               Bases: <code>Discipline</code></p> <p>The Rosenbrock function as a discipline.</p> <p>Initialize self.  See help(type(self)) for accurate signature.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__()\n    self.input_grammar.update_from_names([\"x1\", \"x2\"])\n    self.output_grammar.update_from_names([\"y\"])\n    self.default_input_data.update({\n        name: array([0.0]) for name in self.input_grammar.names\n    })\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_discipline/#gemseo_mlearning.problems.rosenbrock.rosenbrock_discipline-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/","title":"Rosenbrock function","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function","title":"rosenbrock_function","text":"<p>The Rosenbrock function.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function.RosenbrockFunction","title":"RosenbrockFunction","text":"<pre><code>RosenbrockFunction()\n</code></pre> <p>               Bases: <code>MDOFunction</code></p> <p>The Rosenbrock function.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_function.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    super().__init__(compute_output, \"Rosenbrock\", jac=compute_gradient)\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_function/#gemseo_mlearning.problems.rosenbrock.rosenbrock_function-functions","title":"Functions","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/","title":"Rosenbrock problem","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem","title":"rosenbrock_problem","text":"<p>A problem connecting the Rosenbrock function with its uncertain space.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem/#gemseo_mlearning.problems.rosenbrock.rosenbrock_problem.RosenbrockProblem","title":"RosenbrockProblem","text":"<pre><code>RosenbrockProblem(use_uncertain_space: bool = True)\n</code></pre> <p>               Bases: <code>OptimizationProblem</code></p> <p>A problem connecting the Rosenbrock function with its uncertain space.</p> PARAMETER DESCRIPTION <code>use_uncertain_space</code> <p>Whether to consider the input space as an uncertain space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_problem.py</code> <pre><code>def __init__(self, use_uncertain_space: bool = True) -&gt; None:\n    \"\"\"\n    Args:\n        use_uncertain_space: Whether to consider the input space\n            as an uncertain space.\n    \"\"\"  # noqa: D205 D212\n    input_space = RosenbrockSpace()\n    if not use_uncertain_space:\n        input_space = input_space.to_design_space()\n\n    super().__init__(input_space)\n    self.objective = RosenbrockFunction()\n</code></pre>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/","title":"Rosenbrock space","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space","title":"rosenbrock_space","text":"<p>The uncertain space used in the Rosenbrock use case.</p>"},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/problems/rosenbrock/rosenbrock_space/#gemseo_mlearning.problems.rosenbrock.rosenbrock_space.RosenbrockSpace","title":"RosenbrockSpace","text":"<pre><code>RosenbrockSpace()\n</code></pre> <p>               Bases: <code>ParameterSpace</code></p> <p>The uncertain space used in the Rosenbrock use case.</p> Source code in <code>src/gemseo_mlearning/problems/rosenbrock/rosenbrock_space.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa:D107\n    super().__init__()\n    for index in range(2):\n        self.add_random_variable(\n            f\"x{index + 1}\", \"OTUniformDistribution\", minimum=-2, maximum=2\n        )\n</code></pre>"},{"location":"reference/gemseo_mlearning/regression/","title":"Regression","text":""},{"location":"reference/gemseo_mlearning/regression/#gemseo_mlearning.regression","title":"regression","text":"<p>The regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/","title":"Smt regressor","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor","title":"smt_regressor","text":"<p>A regression model from SMT.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor/#gemseo_mlearning.regression.smt_regressor.SMTRegressor","title":"SMTRegressor","text":"<p>               Bases: <code>BaseRegressor</code></p> <p>A regression model from SMT.</p> <p>Note</p> <p>SMT is an open-source Python package consisting of libraries of surrogate modeling methods, sampling methods, and benchmarking problems. Read this page for the list of surrogate models and options.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/","title":"Smt regressor settings","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings","title":"smt_regressor_settings","text":"<p>The settings for the SMT's regression models.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings.SMTSurrogateModel","title":"SMTSurrogateModel  <code>module-attribute</code>","text":"<pre><code>SMTSurrogateModel = StrEnum('SurrogateModel', list(keys()))\n</code></pre> <p>The class name of an SMT surrogate model.</p>"},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/regression/smt_regressor_settings/#gemseo_mlearning.regression.smt_regressor_settings.SMT_Regressor_Settings","title":"SMT_Regressor_Settings","text":"<p>               Bases: <code>BaseRegressorSettings</code></p> <p>The settings for the SMT's regression models.</p> PARAMETER DESCRIPTION <code>transformer</code> <p>The strategies to transform the variables.</p> <p>The values are instances of :class:<code>.BaseTransformer</code> while the keys are the names of either the variables or the groups of variables, e.g. <code>\"inputs\"</code> or <code>\"outputs\"</code> in the case of the regression algorithms. If a group is specified, the :class:<code>.BaseTransformer</code> will be applied to all the variables of this group. If :attr:<code>.IDENTITY</code>, do not transform the variables.</p> <p> TYPE: <code>Mapping[str, Any]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>parameters</code> <p>Other parameters.</p> <p> TYPE: <code>Mapping[str, Any]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>input_names</code> <p>The names of the input variables</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>()</code> </p> <code>output_names</code> <p>The names of the output variables</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>()</code> </p> <code>model_class_name</code> <p>The class name of a surrogate model available in SMT, i.e. a subclass of <code>smt.surrogate_models.surrogate_model.SurrogateModel</code>.</p> <p> TYPE: <code>SurrogateModel</code> </p>"},{"location":"reference/gemseo_mlearning/settings/","title":"Settings","text":""},{"location":"reference/gemseo_mlearning/settings/#gemseo_mlearning.settings","title":"settings","text":"<p>Settings of the different algorithms.</p>"},{"location":"reference/gemseo_mlearning/settings/mlearning/","title":"Mlearning","text":""},{"location":"reference/gemseo_mlearning/settings/mlearning/#gemseo_mlearning.settings.mlearning","title":"mlearning","text":"<p>Settings of the machine learning algorithms.</p>"},{"location":"reference/gemseo_mlearning/settings/mlearning/#gemseo_mlearning.settings.mlearning-classes","title":"Classes","text":""},{"location":"reference/gemseo_mlearning/settings/opt/","title":"Opt","text":""},{"location":"reference/gemseo_mlearning/settings/opt/#gemseo_mlearning.settings.opt","title":"opt","text":"<p>Settings of the optimization algorithms.</p>"},{"location":"reference/gemseo_mlearning/settings/opt/#gemseo_mlearning.settings.opt-classes","title":"Classes","text":""},{"location":"user_guide/","title":"Introduction","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":""},{"location":"user_guide/active_learning/active_learning_algo/","title":"AL algorithm","text":""},{"location":"user_guide/active_learning/active_learning_algo/#active-learning-algorithm","title":"Active learning algorithm","text":"<p>The ActiveLearningAlgo class defines an active learning algorithm.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#in-a-nutshell","title":"In a nutshell","text":"<p>Given a coarse regressor \\(\\hat{f}\\) (a.k.a. surrogate model) of a model \\(f\\) (a.k.a. substituted model), this algorithm updates this regressor sequentially from input-output points maximizing (or minimizing) an acquisition criterion (a.k.a. infill criterion) chosen for a specific purpose.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-an-acquisition-criterion","title":"Choosing an acquisition criterion","text":"<p><code>gemseo-mlearning</code> includes five families of acquisition criteria corresponding to as many purposes:</p> <ul> <li>the Minimum family   aims to make the minimum of the surrogate model   tends towards the minimum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Maximum family   aims to make the maximum of the surrogate model   tends towards the maximum of the substituted model   when the number of acquired points tends to infinity,</li> <li>the LevelSet family   aims to make a level set of the surrogate model   tends towards the corresponding level set of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Quantile family   aims to make a quantile of the surrogate model   tends towards the corresponding quantile of the substituted model   when the number of acquired points tends to infinity,</li> <li>the Exploration family   aims to make the error of the surrogate model tends towards zero   when the number of acquired points tends to infinity.</li> </ul> <p>Minimum, Maximum and Quantile provides an estimation of the quantity of interest, namely the minimum, the maximum and a quantile respectively. This value is updated each time a training point is acquired and can be accessed via the attribute qoi.</p> <p>Whatever the family, the surrogate model can be used for prediction as any surrogate model. However, outside the Exploration family, it is important to bear in mind that its learning dataset has been designed to estimate a particular quantity of interest, and should therefore be used with caution. In other words, a surrogate model built to find the minimum can be bad at predicting the high output values. In such cases, the predicted standard-deviation that characterizes the uncertainty of the prediction can help to judge the accuracy of the surrogate prediction.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#choosing-a-surrogate-model","title":"Choosing a surrogate model","text":"<p>Many of these acquisition criteria consider that the surrogate model is a Gaussian process (GP) regressor \\(\\hat{f}\\) and are expressed from realizations or statistics of this GP \\(\\hat{F}\\), e.g. its mean \\(m_n(x)\\) and its variance \\(c_n(x,x)\\) at \\(x\\) where \\(c_n(\\cdot,\\cdot)\\) is its covariance function. These statistics and realizations can be obtained using the KrigingDistribution class which can be built from any regressor deriving from BaseRandomProcessRegressor, such as GaussianProcessRegressor based on scikit-learn and OTGaussianProcessRegressor based on OpenTURNS. By distribution we mean the probability distribution of a random function of which \\(f\\) is an instance. For non-GP regressors, this distribution is not a random process from the literature but an empirical distribution based on resampling techniques and is qualified as universal by its authors: RegressorDistribution.</p>"},{"location":"user_guide/active_learning/active_learning_algo/#how-to-use-this-algorithm","title":"How to use this algorithm","text":"<p>A basic use of this class is</p> <ol> <li>instantiate    the ActiveLearningAlgo    from an input space of type DesignSpace,    a BaseRegressor    and the name of a family of acquisition criteria    (a default acquisition criterion will be set accordingly),</li> <li>update the regressor with the method <code>acquire_new_points</code>,</li> <li>get the updated regressor with the attribute <code>regressor</code>.</li> </ol> <p>For more advanced use, it is possible to change the acquisition algorithm, i.e. the optimization algorithm to minimize or maximize the acquisition criterion, as well as the acquisition criterion among the selected family.</p> <p>Lastly, the visualization subpackage offers plotting capabilities to draw the evolution of both the surrogate model and the acquisition criterion. They can be easily accessed from the ActiveLearningAlgo using its methods:</p> <ul> <li>plot_qoi_history   to visualize the estimation of the quantity of interest in function of the acquisition step,</li> <li>plot_acquisition_view   to visualize the discipline, the regressor, the acquisition criterion and the standard deviation   in the case of two scalar input variables.</li> </ul>"},{"location":"user_guide/active_learning/exploration/","title":"AL for exploration","text":""},{"location":"user_guide/active_learning/exploration/#active-learning-for-exploration","title":"Active learning for exploration","text":"<p>Active learning techniques are very intuitive for refining a surrogate model. Instead of spending the entire computational budget to create a training dataset in one-go, the idea is to start from a rough surrogate model and acquire new training points sequentially to increase its accuracy.</p> <p>The ActiveLearningAlgo presented in the previous section can be used to explore the input space in search of unlearned points, using the family of acquisition criteria Exploration.</p> <p>For now, these acquisition criteria do not have quantities of interest and do not support parallel acquisition.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Whatever surrogate model is chosen, given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p>"},{"location":"user_guide/active_learning/exploration/#distance","title":"Distance","text":"<p>The most naive approach is to learn the point furthest away from those in the training dataset. So, this criterion to maximize is the minimum distance between a new point and the point of the training dataset, scaled by the minimum distance between two distinct training points:</p> \\[D[x]=\\frac{\\min_{1\\leq i \\leq n} \\|x-x_i\\|_2}{\\min_{1\\leq i,j \\leq n, i\\neq j} \\|x_i-x_j\\|_2}\\] <p>where \\(\\|.\\|_2\\) is the Euclidean norm.</p>"},{"location":"user_guide/active_learning/exploration/#api","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Distance\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"Distance\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/exploration/#variance","title":"Variance","text":"<p>The previous criterion considers neither the output values nor the quality of the surrogate model. However, taking account of model uncertainty can be relevant and this is the reason why the variance of the prediction is the default criterion:</p> \\[V[x]=\\sigma^2(x).\\]"},{"location":"user_guide/active_learning/exploration/#api_1","title":"API","text":"<p>This criterion, named <code>\"Variance\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n)\n</code></pre>"},{"location":"user_guide/active_learning/exploration/#standard-deviation","title":"Standard deviation","text":"<p>This criterion is simply the square root of the previous one:</p> \\[S[x]=\\sigma(x).\\]"},{"location":"user_guide/active_learning/exploration/#api_2","title":"API","text":"<p>This criterion can be accessed via the name <code>\"StandardDeviation\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Exploration\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"StandardDeviation\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/","title":"AL for level sets","text":""},{"location":"user_guide/active_learning/level_set/#active-learning-for-level-set-estimation","title":"Active learning for level set estimation","text":"<p>Active learning techniques are very useful to estimate a level set, i.e. the input values for which the model output is equal to a specific value \\(y\\).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to approximate a particular level set, using the family of acquisition criteria LevelSet. As new points are acquired, the level set of the surrogate model will be increasingly similar to that of the substituted model.</p> <p>For now, these acquisition criteria do not have quantities of interest.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p> <p>The value characterizing the level set can be passed using the argument <code>output_value</code>,  e.g. <code>output_value=12</code>.</p> <p>All these acquisition criteria support parallel acquisition. The number of points to be acquired at a time and the sample size to estimate the statistics defined below can be set with the arguments <code>batch_size</code> (default: <code>1</code>) and <code>mc_size</code> (default: <code>10000</code>) respectively.</p>"},{"location":"user_guide/active_learning/level_set/#u-function","title":"U-function","text":"<p>The simplest criterion to approximate a level set \\(y\\) is the U-function<sup>1</sup>:</p> \\[U[x] = \\mathbb{E}\\left[\\left(\\frac{y-Y(x)}{\\sigma(x)}\\right)^2\\right]\\] <p>which can be simplified to</p> \\[U[x] = \\left(\\frac{y-\\mu(x)}{\\sigma(x)}\\right)^2.\\]"},{"location":"user_guide/active_learning/level_set/#api","title":"API","text":"<p>This criterion, named <code>\"U\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ U[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\min_{1\\leq i \\leq q}\\left(\\left(\\frac{y-Y(x_i)}{\\sigma(x_i)}\\right)^2\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/level_set/#expected-feasibility","title":"Expected feasibility","text":"<p>Another criteria to approximate a level set \\(y\\) is the expected feasibility<sup>2</sup>:</p> \\[EF[x] =  \\mathbb{E}\\left[\\max(\\kappa\\sigma(x) - |y - Y(x)|,0)\\right].\\]"},{"location":"user_guide/active_learning/level_set/#api_1","title":"API","text":"<p>This criterion can be accessed via the name <code>\"EF\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EF\",\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#gaussian-case","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[ EF[x] = \\sigma(x) \\left( \\kappa (\\Phi(t^+)-\\Phi(t^-)) -t(2\\Phi(t)-\\Phi(t^+)-\\Phi(t^-)) -(2\\phi(t)-\\phi(t^+)-\\phi(t^-)) \\right) \\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution, \\(t=\\frac{y - \\mu(x)}{\\sigma(x)}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition_1","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EF[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i \\leq q}\\left(\\max(\\kappa\\sigma(x_i) - |y - Y(x_i)|,0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/level_set/#expected-improvement","title":"Expected improvement","text":"<p>Another criteria to approximate a level set \\(y\\) is the expected improvement<sup>2</sup>:</p> \\[EI[x] = \\mathbb{E}\\left[\\max((\\kappa\\sigma(x))^2 - (y - Y(x))^2,0)\\right].\\]"},{"location":"user_guide/active_learning/level_set/#api_2","title":"API","text":"<p>This criterion can be accessed via the name <code>\"EI\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"LevelSet\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EI\",\n    output_value=12.\n)\n</code></pre>"},{"location":"user_guide/active_learning/level_set/#gaussian-case_1","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[ EI[x] = \\sigma(x) \\left( (\\kappa^2-1-t^2)(\\Phi(t^+)-\\Phi(t^-)) -2t(\\phi(t^+)-\\phi(t^-)) +t^+\\phi(t^+) -t^-\\phi(t^-) \\right) \\] <p>where \\(Y\\) is the random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\), \\(t=\\frac{y - \\mu(x)}{\\sigma(x)}\\), \\(t^+=t+\\kappa\\), \\(t^-=t-\\kappa\\) and \\(\\kappa&gt;0\\).</p>"},{"location":"user_guide/active_learning/level_set/#parallel-acquisition_2","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max((\\kappa\\sigma(x_i))^2 - (y - Y(x_i))^2,0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p> <ol> <li> <p>S. Roy and W. Notz. Estimating percentiles in computer experiments: a comparison of sequential-adaptive designs and fixed designs. Journal of Statistical Theory and Practice, 8:12\u201329, 2014.\u00a0\u21a9</p> </li> <li> <p>J. Bect, D. Ginsbourger, L. Li, V. Picheny, and E. Vazquez. Sequential design of computer experiments for the estimation of a probability of failure. Statistics and Computing, 22:773\u2013793, 2012.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/active_learning/optimization/","title":"AL for optimization","text":""},{"location":"user_guide/active_learning/optimization/#active-learning-for-optimization","title":"Active learning for optimization","text":"<p>Active learning techniques have become popular in global optimization, with the famous Bayesian optimization algorithms, including the efficient global optimization (EGO) one<sup>1</sup>. These approaches can be relevant when the functions of the optimization problem:</p> <ul> <li>are costly,</li> <li>are not accompanied by their gradient functions,</li> <li>depend on an important number of optimization variables,   making the estimation of their gradient   by finite differences impossible.</li> </ul> <p>In this case, multi-start gradient-based optimization cannot be used, and neither can classical global optimization algorithms (e.g. evolutionary algorithms).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to minimize (resp. maximize) a cost function (resp. performance function) using the family of acquisition criteria Minimum (resp. Maximum ). The quantity of interest is the minimum (resp. maximum) of the function.</p> <p>Given a random process \\(Y\\) modelling the uncertainty of the surrogate model \\(\\hat{f}\\), this page lists several acquisition criteria, first for the minimum, then for the maximum. Most often, \\(Y\\) is a Gaussian process (GP) conditioned by the training dataset and \\(\\hat{f}\\) is the associated GP regressor, which corresponds to its expectation. Given an input point \\(x\\), \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\).</p> <p>For acquisition criteria supporting parallel acquisition, the number of points to be acquired at a time and the sample size to estimate the statistics defined below can be set with the arguments <code>batch_size</code> (default: <code>1</code>) and <code>mc_size</code> (default: <code>10000</code>) respectively.</p>"},{"location":"user_guide/active_learning/optimization/#minimum","title":"Minimum","text":""},{"location":"user_guide/active_learning/optimization/#mean","title":"Mean","text":"<p>The simplest criterion to minimize is the expectation:</p> \\[E[x] = \\mu(x),\\] <p>also called Kriging believer in the case of a Gaussian process regressor.</p>"},{"location":"user_guide/active_learning/optimization/#api","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Mean\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"Mean\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#lower-confidence-bound","title":"Lower confidence bound","text":"<p>A more advanced criterion to minimize is the lower confidence bound</p> \\[M[x;\\kappa] = \\mu(x) - \\kappa \\times \\sigma(x)\\] <p>where \\(\\kappa &gt; 0\\) (default: 2).</p>"},{"location":"user_guide/active_learning/optimization/#api_1","title":"API","text":"<p>This criterion can be accessed via the name <code>\"LCB\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"LCB\"\n)\n</code></pre> <p>and the \\(\\kappa\\) constant can be set using the argument <code>kappa</code>.</p>"},{"location":"user_guide/active_learning/optimization/#expected-improvement","title":"Expected improvement","text":"<p>The most popular criterion to maximize is the expected improvement<sup>1</sup></p> \\[EI[x] = \\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0)].\\] <p>where \\(y_1,\\dots,y_n\\) are the learning output values.</p>"},{"location":"user_guide/active_learning/optimization/#api_2","title":"API","text":"<p>This criterion, named <code>\"EI\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Minimum\",\n    input_space,\n    initial_regressor\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#gaussian-case","title":"Gaussian case","text":"<p>In the case of a Gaussian process regressor, it has an analytic expression:</p> \\[EI[x] = (\\min(y_1,\\dots,y_n)-\\mu(x))\\Phi(t) + \\sigma(x)\\phi(t)\\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{y_{\\text{min}}-\\mu(x)}{\\sigma(x)}\\).</p>"},{"location":"user_guide/active_learning/optimization/#parallel-acquisition","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max(\\min(y_1,\\dots,y_n)-Y(x_i),0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p>"},{"location":"user_guide/active_learning/optimization/#maximum","title":"Maximum","text":""},{"location":"user_guide/active_learning/optimization/#mean_1","title":"Mean","text":"<p>The simplest criterion to maximize is the expectation</p> \\[E[x] = \\mu(x),\\] <p>also called Kriging believer in the case of a Gaussian process regressor.</p>"},{"location":"user_guide/active_learning/optimization/#api_3","title":"API","text":"<p>This criterion can be accessed via the name <code>\"Mean\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n  \"Maximum\",\n  input_space,\n  initial_regressor,\n  criterion_name=\"Mean\"\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#upper-confidence-bound","title":"Upper confidence bound","text":"<p>A more advanced criterion to maximize is the upper confidence bound</p> \\[M[x;\\kappa] = \\mu(x) + \\kappa \\times \\sigma(x)\\] <p>where \\(\\kappa &gt; 0\\) (default: 2).</p>"},{"location":"user_guide/active_learning/optimization/#api_4","title":"API","text":"<p>This criterion can be accessed via the name <code>\"UCB\"</code>:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Maximum\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"UCB\"\n)\n</code></pre> <p>and the \\(\\kappa\\) constant can be set using the argument <code>kappa</code>.</p>"},{"location":"user_guide/active_learning/optimization/#expected-improvement_1","title":"Expected improvement","text":"<p>The most popular criterion to maximize is the expected improvement<sup>1</sup></p> \\[EI[x] = \\mathbb{E}[\\max(Y(x)-\\max(y_1,\\dots,y_n),0)]\\] <p>where \\(y_1,\\dots,y_n\\) are the learning output values.</p>"},{"location":"user_guide/active_learning/optimization/#api_5","title":"API","text":"<p>This criterion, named <code>\"EI\"</code>, is the default one; so there's no need to provide its name:</p> <pre><code>active_learning = ActiveLearningAlgo(\n    \"Maximum\",\n    input_space,\n    initial_regressor\n)\n</code></pre>"},{"location":"user_guide/active_learning/optimization/#gaussian-case_1","title":"Gaussian case","text":"<p>In the case of a GP, the criterion has an analytic expression:</p> \\[EI[x] = (\\mu(x) - \\max(y_1,\\dots,y_n))\\Phi(t) + \\sigma(x)\\phi(t)\\] <p>where \\(\\Phi\\) and \\(\\phi\\) are respectively the cumulative and probability density functions of the standard normal distribution and \\(t=\\frac{\\mu(x) - \\max(y_1,\\dots,y_n)}{\\sigma(x)}\\).</p>"},{"location":"user_guide/active_learning/optimization/#parallel-acquisition_1","title":"Parallel acquisition","text":"<p>For the acquisition of \\(q&gt;1\\) points at a time, the acquisition criterion changes to</p> \\[ EI[x_1,\\dots,x_q] = \\mathbb{E}\\left[\\max_{1\\leq i\\leq q}\\left(\\max(Y(x_i)-\\max(y_1,\\dots,y_n),0)\\right)\\right] \\] <p>where the expectation is taken with respect to the distribution of the random vector \\((Y(x_1),\\dots,Y(x_q))\\). There is no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte Carlo.</p> <ol> <li> <p>D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/active_learning/quantile/","title":"AL for quantiles","text":""},{"location":"user_guide/active_learning/quantile/#quantile","title":"Quantile","text":"<p>Active learning techniques are very useful to estimate a level set,  i.e. the input values for which the model output is equal to a specific value \\(y\\).</p> <p>In particular, this specific value can be the \\(\\alpha\\)-quantile for a specific \\(\\alpha\\in]0,1[\\).</p> <p>The ActiveLearningAlgo presented in the previous section can be used to approximate a particular quantile, using the family of acquisition criteria Quantile. These criteria are the same as for the estimation of a level set. For this reason, we refer you to this page listing these acquisition criteria.</p> <p>In this page, we simply indicate that the quantile level and the uncertain space are set using the arguments <code>level</code> and <code>uncertain_space</code> respectively, and the user code looks like below.</p>"},{"location":"user_guide/active_learning/quantile/#u-function","title":"U-function","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/quantile/#expected-feasibility","title":"Expected feasibility","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EF\",\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/quantile/#expected-improvement","title":"Expected improvement","text":"<pre><code>active_learning = ActiveLearningAlgo(\n    \"Quantile\",\n    input_space,\n    initial_regressor,\n    criterion_name=\"EI\",\n    level=0.1,\n    uncertain_space=probability_space\n)\n</code></pre>"},{"location":"user_guide/active_learning/what_active_learning_is/","title":"What AL is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#active-learning","title":"Active learning","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#what-active-learning-is","title":"What active learning is","text":""},{"location":"user_guide/active_learning/what_active_learning_is/#introduction","title":"Introduction","text":"<p>Active learning techniques are iterative methods that aim to sequentially estimate various quantities of interest: optimas, contours, failure probabilities, quantiles, expected values, etc. These methods generally start by constructing a rough surrogate model (an approximation of a costly exact physical model much faster to run). Then, they iteratively look for new simulations to run, and update the surrogate model and the estimated target quantities until a budget or accuracy criterion is reached. This search is guided by an acquisition criterion, also called infill criterion, to be minimized or maximized. This approach is inspired by the efficient global optimization (EGO) algorithm <sup>1</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#surrogate-modeling","title":"Surrogate modeling","text":"<p>Surrogate models<sup>2</sup> are approximations of a costly exact model \\(f:\\mathbf{x} \\in \\mathbb{X}\\subset\\mathbb{R}^d\\mapsto y \\in \\mathbb{R}\\) much faster to run. There are many types of surrogates, but one of the most popular, in particular in the active learning literature, is the Kriging model, a.k.a. Gaussian processes (GP) regressor <sup>3</sup>. One reason is that GP models provide a characterization of the prediction uncertainty. The exact simulator \\(f\\) is considered as a realization of a GP, \\(F\\), with trend \\(m : \\mathbb{X} \\rightarrow \\mathbb{R}\\) and covariance kernel \\(c : \\mathbb{X}\\times \\mathbb{X} \\rightarrow \\mathbb{R}\\). Conditioned by \\(n\\) observations \\(\\mathcal{A}_n = \\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n)\\}\\), \\(F_n  = F|\\mathcal{A}_n\\) is also a GP, with trend \\(m_n(\\cdot)\\) and covariance function \\(c_n(\\cdot,\\cdot)\\) that express differently depending on whether the trend function is known or not. The variance of the output prediction is denoted by \\(s_n^2(\\cdot) = c_n(\\cdot,\\cdot)\\).</p> <p>Note that the implementation of active learning strategies is not limited to the use of GP surrogates. In this case, a critical step of the method is the characterization of the uncertainty of the surrogate prediction, which can be achieved using bootstrap-based techniques<sup>4</sup>.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#the-rosenbrock-function","title":"The Rosenbrock function","text":"<p>The Rosenbrock function<sup>5</sup>, which will be used for illustrative purposes, expresses as :</p> \\[ f(\\mathbf{x}) = \\sum_{i = 1}^{d-1} [100(x_{i+1} - x_i^2)^2 +(1-x_i)^2], \\] <p>with \\(\\mathbf{x}\\) a \\(d\\)-dimensional input, \\(d \\geqslant 2\\). An illustration of the function with a two-dimensional input is depicted below:</p>  Bidimensional Rosenbrock function  <p>In the case of quantile estimation, \\(x_1\\) and \\(x_2\\) are replaced by the independent random variables \\(X_1\\) and \\(X_2\\) uniformly distributed over \\([-2,2]\\).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#acquisition-criteria","title":"Acquisition criteria","text":"<p>One of the key step of the active learning methodology is the choice of the acquisition criterion that guides the enrichment of the surrogate model. It is noted \\(J_n(\\cdot)\\) in the following. Several criteria have been developed for different purposes <sup>6</sup>. Typical applications of this methodology include the improvement of the surrogate model, the minimization of black-box functions and contour estimation, among others. Some popular criteria are detailed next for the objectives listed above.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#improving-the-quality-of-the-surrogate","title":"Improving the quality of the surrogate","text":"<p>A straightforward method to improve a surrogate model consists in choosing as enrichment points those with the highest predicted uncertainty, giving for the infill criterion \\(J_n(\\cdot)\\) :</p> \\[ J_n(\\mathbf{x}) = s_n^2(\\mathbf{x}),\\]  Level plot of the predicted uncertainty. The learning points are denoted with a red star.  <p>Note in this figure that, the predicted uncertainty (in fact the infill criterion) is the highest in the areas where the amount of learning points for the training of the surrogate is the smallest.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#finding-optimas","title":"Finding optimas","text":"<p>One of the most popular acquisition criterion to minimize (alternatively to maximize) black-box functions is the expected improvement <sup>1</sup> defined as:</p> \\[ J_n(\\mathbf{x}) =  \\text{EI}_n(\\mathbf{x})  = \\mathbb{E}[\\max(\\mathbf{y}_n - F_n(\\mathbf{x}),0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). In the previous equation, \\(\\mathbf{y}_n\\) corresponds to the output values of the database \\(\\mathcal{A}_n\\) and stands for \\(\\mathbf{y}_n = (y_i)_{i \\in [1, n]}\\).</p>  Level plot of the expected improvement. The learning points are denoted with a red star.  The global minimum is set in green.  <p>In this figure, some learning points are already close to the global minimum, so the infill criterion rather seeks here to improve the GP overall prediction.</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#estimating-contour-sets","title":"Estimating contour sets","text":"<p>A usual criterion to estimate the level set<sup>7</sup> associated to value \\(a\\) is :</p> \\[ J_n(\\mathbf{x}) =  \\mathbb{E}[\\max((2 s_n^2(\\mathbf{x})) -\\vert{a - F_n(\\mathbf{x})}\\vert^2,0)],\\] <p>where the expectation is taken with respect to the distribution of the conditioned GP \\(F_n(\\cdot)\\). With an appropriate choice of \\(a\\), this criterion can be used to estimate quantiles or failure probabilities. In the following, it is implemented to estimate the \\(35\\%\\) quantile of the Rosenbrock function.</p>  Level plot of the acquisition criterion for level set estimation. The learning points are denoted with a red star. The level set associated to the quantile to estimate is drawn in black.  <p>In this figure, the areas with the highest acquisition criterion values refer to those that are located around the level set corresponding to the target quantile (which is here the objective study) and far from the learning points (where knowledge is already available).</p>"},{"location":"user_guide/active_learning/what_active_learning_is/#parallelization","title":"Parallelization","text":"<p>Most of the time, a single observation is added at the different steps of the active learning algorithm. However, to reduce the number of iterations of the active learning strategy, a batch of several points can be added <sup>8</sup> instead of considering a single one. This implies several changes, among which the expression of the infill criterion and its maximization. Let emphasize that parallelizing the active learning methodology significantly increases the computational burden which can still be alleviated by considering approximated heuristics <sup>8</sup>.</p> <p>A key challenge of this method is defining the size of the batch of points added at each iteration. In theory, it should be as high as possible, because for a fixed number of black-box evaluations, the higher it is, the lower the number of iterations is. However, this is synonymous with an increase of the numerical complexity of the method, and a balance must be therefore struck between the computational costs and the efficiency of the active learning strategy.</p> <p>In the next section, we will present the active learning algorithm available in gemseo-mlearning.</p> <ol> <li> <p>D. Jones, M. Schonlau, and W. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Forrester, A. Sobester, and A. Keane. Engineering design via surrogate modelling: a practical guide. John Wiley &amp; Sons, 2008.\u00a0\u21a9</p> </li> <li> <p>C. Williams and C. Rasmussen. Gaussian processes for machine learning. Volume 2. MIT press Cambridge, MA, 2006.\u00a0\u21a9</p> </li> <li> <p>M. Ben Salem, O. Roustant, F. Gamboa, and L. Tomaso. Universal prediction distribution for surrogate models. SIAM/ASA Journal on Uncertainty Quantification, 5:, 12 2015. doi:10.1137/15M1053529.\u00a0\u21a9</p> </li> <li> <p>M. Molga and C. Smutnicki. Test functions for optimization needs. Test functions for optimization needs, 101:48, 2005.\u00a0\u21a9</p> </li> <li> <p>F. Viana, C. Gogu, and T. Goel. Surrogate modeling: tricks that endured the test of time and some recent developments. Structural and Multidisciplinary Optimization, pages 1\u201328, 2021.\u00a0\u21a9</p> </li> <li> <p>P. Ranjan, D. Bingham, and G. Michailidis. Sequential experiment design for contour estimation from complex computer codes. Technometrics, 50(4):527\u2013541, 2008.\u00a0\u21a9</p> </li> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/optimization/al/","title":"AL package","text":""},{"location":"user_guide/optimization/al/#surrogate-based-optimizers-using-the-al-algorithm","title":"Surrogate-based optimizers using the AL algorithm","text":"<p>The ActiveLearningAlgo presented in this page can be used to minimize a cost function using the family of acquisition criteria Minimum</p> <p>SurrogateBasedOptimization is a collection of surrogate-based optimization algorithms built on top of ActiveLearningAlgo and Minimum.</p>"},{"location":"user_guide/optimization/al/#basic-usage","title":"Basic usage","text":"<p>SurrogateBasedOptimization includes a single optimization algorithm, called <code>\"SBO\"</code>.</p> <p>Given a maximum number of iterations, it can be used as is by any OptimizationProblem: <pre><code>execute_algo(optimization_problem, algo_name=\"SBO\", max_iter=50)\n</code></pre> and any MDOScenario: <pre><code>scenario.execute(algo_name=\"SBO\", max_iter=50)\n</code></pre></p> <p>In this case, the settings are</p> <ul> <li>the expected improvement as acquisition criterion,</li> <li>1 point acquired at a time,</li> <li>the OTGaussianProcessRegressor   wrapping the Kriging model from OpenTURNS,</li> <li>10 initial training points based on an optimized latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion   with a gradient-based algorithm   from 20 start points with a limit of 200 iterations per local optimization.</li> </ul>"},{"location":"user_guide/optimization/al/#settings","title":"Settings","text":"<p>In the following, the training output values already acquired and the output and uncertainty predictions at a given input point \\(x\\) are respectively denoted \\(\\{y_1,\\ldots,y_n\\}\\), \\(\\mu(x)\\) and \\(\\sigma(x)\\).</p>"},{"location":"user_guide/optimization/al/#general","title":"General","text":"<p>The settings for the surrogate-based optimization algorithm.</p> PARAMETER DESCRIPTION <code>enable_progress_bar</code> <p>Whether to enable the progress bar in the optimization log.</p> <p>If <code>None</code>, use the global value of <code>enable_progress_bar</code> (see the <code>configure</code> function to change it globally).</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>eq_tolerance</code> <p>The tolerance on the equality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>ineq_tolerance</code> <p>The tolerance on the inequality constraints.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>log_problem</code> <p>Whether to log the definition and result of the problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_time</code> <p>The maximum runtime in seconds, disabled if 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>normalize_design_space</code> <p>Whether to normalize the design space variables between 0 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_bar_data_name</code> <p>The name of a :class:<code>.BaseProgressBarData</code> class to define the data of an evaluation problem to be displayed in the progress bar.</p> <p> TYPE: <code>ProgressBarDataName</code> DEFAULT: <code>'ProgressBarData'</code> </p> <code>reset_iteration_counters</code> <p>Whether to reset the iteration counters before each execution.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>round_ints</code> <p>Whether to round the integer variables.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>store_jacobian</code> <p>Whether to store the Jacobian matrices in the database.</p> <pre><code>This argument is ignored when the ``use_database`` option is ``False``.\nIf a gradient-based algorithm is used,\nthis option cannot be set along with kkt options.\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_database</code> <p>Whether to wrap the functions in the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_one_line_progress_bar</code> <p>Whether to log the progress bar on a single line.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ftol_rel</code> <p>The relative tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>ftol_abs</code> <p>The absolute tolerance on the objective function.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_iter</code> <p>The maximum number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>scaling_threshold</code> <p>The threshold on the reference function value that triggers scaling.</p> <p>If <code>None</code>, do not scale the functions.</p> <p> TYPE: <code>Annotated[float, Ge] | None</code> DEFAULT: <code>None</code> </p> <code>stop_crit_n_x</code> <p>The minimum number of design vectors to consider in the stopping criteria.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>xtol_rel</code> <p>The relative tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>xtol_abs</code> <p>The absolute tolerance on the design parameters.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>acquisition_algorithm</code> <p>The name of the algorithm to optimize the data acquisition criterion.             If empty, use the default algorithm with its default settings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>acquisition_settings</code> <p>The settings of the algorithm             to optimize the data acquisition criterion.             Ignored when <code>acquisition_algorithm</code> is empty.</p> <p> TYPE: <code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>batch_size</code> <p>The number of points to be acquired in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>criterion</code> <p>The acquisition criterion.</p> <p> TYPE: <code>AcquisitionCriterion</code> DEFAULT: <code>&lt;AcquisitionCriterion.EI: 'EI'&gt;</code> </p> <code>doe_algorithm</code> <p>The name of the DOE algorithm for the initial sampling.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'OT_OPT_LHS'</code> </p> <code>doe_settings</code> <p>The settings of the DOE algorithm for the initial sampling.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>Mapping[str, Union[str, float, int, bool, list[str], ndarray, Iterable[Callable[list, None]], Mapping[str, Any]]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p> <code>doe_size</code> <p>Either the initial DOE size or 0 if it is inferred from <code>doe_settings</code>.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>mc_size</code> <p>The sample size to estimate the acquisition criteria in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>regression_algorithm</code> <p>The regression algorithm.             Either the name of the regression algorithm             approximating the objective function over the design space             or the regression algorithm itself.</p> <p> TYPE: <code>str | BaseRegressor</code> DEFAULT: <code>'OTGaussianProcessRegressor'</code> </p> <code>regression_file_path</code> <p>The path to the file to save the regression model.             If empty, do not save the regression model.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>''</code> </p> <code>regression_settings</code> <p>The settings of the regression algorithm.             This argument is ignored             when regression_algorithm is a             BaseRegressor.</p> <p> TYPE: <code>Mapping[str, Optional[Any]]</code> DEFAULT: <code>&lt;class 'dict'&gt;</code> </p>"},{"location":"user_guide/optimization/al/#acquisition-criteria","title":"Acquisition criteria","text":"<p>The three acquisition criteria are</p> Value Name Expression <code>\"EI\"</code> Expected improvement \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y(x),0]\\) <code>\"LCB\"</code> Lower confidence bound \\(\\mu(x)-\\kappa\\times\\sigma(x)\\) with \\(\\kappa&gt;0\\) <code>\"Mean\"</code> Mean output \\(\\mu(x)\\) <p>where \\(Y\\) is a random process modelling the uncertainty of the surrogate model \\(\\hat{f}\\) and \\(\\mu(x)\\equiv\\mathbb{E}[Y(x)]\\) and \\(\\sigma(x)\\equiv\\mathbb{S}[Y(x)]\\) denote the expectation and the standard deviation of \\(Y(x)\\) at input point \\(x\\). Most of the time, \\(Y\\) is a Gaussian process and \\(\\hat{f}\\) is a Kriging model.</p>"},{"location":"user_guide/optimization/al/#initial-training-dataset","title":"Initial training dataset","text":"<p>Any DOE algorithm known by GEMSEO can be used to generate the initial training dataset.</p>"},{"location":"user_guide/optimization/al/#parallel-acquisition","title":"Parallel acquisition","text":"<p>Points can be acquired by batch of \\(q&gt;1\\) points, as Kriging is well-suited to parallelize optimization<sup>1</sup>. To this aim, when <code>batch_size</code> is greater than 1, ActiveLearningAlgo uses a parallel version of the expected improvement criterion. Unfortunately, the latter has no analytic expression and the acquisition criterion is thus instead evaluated with crude Monte-Carlo.</p>"},{"location":"user_guide/optimization/al/#surrogate-models","title":"Surrogate models","text":"<p>SurrogateBasedOptimization is compatible with all regressors, whose classes derive from BaseRegressor.</p> <p>It can also be used from an existing surrogate models. In this case, it will skip the construction and learning of the initial training dataset.</p> <ol> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/optimization/smt/","title":"SMT library","text":""},{"location":"user_guide/optimization/smt/#smts-surrogate-based-optimizers","title":"SMT's surrogate-based optimizers","text":"<p> The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling with a focus on derivatives <sup>1</sup><sup>2</sup>.</p> <p><code>gemseo-mlearning</code> proposes the SMTEGO optimization library to easily use the surrogate-based optimizers available in SMT, through its <code>EGO</code> class.</p>"},{"location":"user_guide/optimization/smt/#basic-usage","title":"Basic usage","text":"<p>SMTEGO includes a single optimization algorithm, called <code>\"SMT_EGO\"</code>.</p> <p>Given a maximum number of iterations, it can be used as is by any OptimizationProblem: <pre><code>execute_algo(optimization_problem, algo_name=\"SMT_EGO\", max_iter=50)\n</code></pre> and any MDOScenario: <pre><code>scenario.execute(algo_name=\"SMT_EGO\", max_iter=50)\n</code></pre></p> <p>In this case, the settings are</p> <ul> <li>the expected improvement as acquisition criterion,</li> <li>1 point acquired at a time,</li> <li>the Kriging-based surrogate model <code>\"KRG\"</code>,</li> <li>10 initial training points based on a latin hypercube sampling (LHS) technique,</li> <li>a multi-start local optimization of the acquisition criterion   from 50 start points with a limit of 20 iterations per local optimization.</li> </ul>"},{"location":"user_guide/optimization/smt/#settings","title":"Settings","text":"<p>This section presents the options of the optimization algorithm <code>\"SMT_EGO\"</code>.</p> <p>Their default values are defined in SMT_EGO_Settings.</p> <p>Info</p> <p>You will find more information in the SMT's user guide.</p>"},{"location":"user_guide/optimization/smt/#acquisition-criteria","title":"Acquisition criteria","text":"<p>You can use the option <code>criterion</code> to change the acquisition criterion:</p> Value Name Expression <code>\"EI\"</code> Expected improvement \\(\\mathbb{E}[\\max(\\min(y_1,\\dots,y_n)-Y,0]\\) <code>\"LCB\"</code> Lower confidence bound \\(\\mu(x)-3\\times\\sigma(x)\\) <code>\"SBO\"</code> Kriging believer \\(\\mu(x)\\) <p>where \\(Y\\) is a Gaussian random variable with mean function \\(\\mu\\) and standard deviation function \\(\\sigma\\), and where \\(\\{y_1,\\ldots,y_n\\}\\) denote the training output values already used.</p>"},{"location":"user_guide/optimization/smt/#optimization-algorithm","title":"Optimization algorithm","text":"<p>The optimization algorithm <code>\"SMT_EGO\"</code> uses sub-optimizations to optimize the acquisition criterion. The number of sub-optimizations is parametrized by <code>n_start</code> while the maximum number of iterations for each sub-optimization is parametrized by <code>n_max_optim</code>.</p>"},{"location":"user_guide/optimization/smt/#parallel-acquisition","title":"Parallel acquisition","text":"<p>Points can be acquired by batch of \\(q&gt;1\\) points, as Kriging is well-suited to parallelize optimization<sup>3</sup>. To this aim, when <code>n_parallel</code> is greater than 1, SMT uses a technique of virtual points to update the training dataset with \\(k\\leq q\\) training points whose output value mimics the substituted model using a strategy.</p> <p>You can use the options <code>n_parallel</code> to acquire <code>n_parallel</code> points in parallel using the acquisition strategy <code>qEI</code>.</p> <p>The strategies are:</p> Value Name Expression <code>\"CLmin\"</code> Minimum constant liar \\(\\min \\{y_1,\\ldots,y_n\\}\\) <code>\"KB\"</code> Kriging believer \\(\\mu(x)\\) <code>\"KBLB\"</code> Kriging believer lower bound \\(\\mu(x)-3\\sigma(x)\\) <code>\"KBRand\"</code> Kriging believer random bound \\(\\mu(x)+\\kappa(x)\\sigma(x)\\) <code>\"KBUB\"</code> Kriging believer upper bound \\(\\mu(x)+3\\sigma(x)\\) <p>where \\(\\kappa(x)\\) is the realization of a random variable distributed according to the standard normal distribution.</p> <p>You can also enable the penalization of points that have been already evaluated in EI criterion, by using the option <code>enable_tunneling</code>.</p>"},{"location":"user_guide/optimization/smt/#surrogate-models","title":"Surrogate models","text":"<p>You can use the option <code>surrogate</code> to change the surrogate model:</p> Value Name <code>\"GPX\"</code> Kriging based on the egobox library written in Rust <code>\"KRG\"</code> Kriging <code>\"KPLS\"</code> Kriging using partial least squares (PLS) to reduce the input dimension <code>\"KPLSK\"</code> A variant of KPLS <code>\"MGP\"</code> A marginal Gaussian process (MGP) regressor <p>You can also change the size of the training dataset using the option <code>n_doe</code>.</p> <ol> <li> <p>B. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, M. Morlier, and J. R. R. A. Martins. A python surrogate modeling framework with derivatives. Advances in Engineering Software, pages 102662, 2019. doi:https://doi.org/10.1016/j.advengsoft.2019.03.005.\u00a0\u21a9</p> </li> <li> <p>P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, and J. R. R. A. Martins. SMT 2.0: A surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes. Advances in Engineering Sofware, 188:103571, 2024. doi:https://doi.org/10.1016/j.advengsoft.2023.103571.\u00a0\u21a9</p> </li> <li> <p>D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimization. In Computational intelligence in expensive optimization problems, pages 131\u2013162. Springer, 2010.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/regression/smt/","title":"SMT library","text":""},{"location":"user_guide/regression/smt/#smts-surrogate-models","title":"SMT's surrogate models","text":"<p> The surrogate modeling toolbox (SMT) is an open-source Python package for surrogate modeling with a focus on derivatives <sup>1</sup><sup>2</sup>.</p> <p><code>gemseo-mlearning</code> proposes the SMTRegressor to easily use any SMT's surrogate model in your GEMSEO processes.</p>"},{"location":"user_guide/regression/smt/#basic-usage","title":"Basic usage","text":"<p>You only have to instantiate this SMTRegressor class from:</p> <ul> <li>the IODataset including your input and output samples,</li> <li>the name of an SMT's surrogate model (e.g. <code>\"KRG\"</code> for Kriging or <code>\"RBF\"</code> for radial basis function),</li> <li>the options of this surrogate model,</li> <li>and the usual options of a BaseRegressor,   namely   <code>transformer</code> to transform the input and output data,   <code>input_names</code> to use a subset of input variables and   <code>output_names</code> to use a subset of output variables.</li> </ul> <p>Here's how to build an SMT's RBF surrogate model with the basis function scaling parameter <code>d_0</code> set to 2.0 (instead of 1.0):</p> <pre><code>model = SMTRegressor(training_dataset, model_class_name=\"RBF\", parameters={\"d0\": 2})\nmodel.learn()\n</code></pre>"},{"location":"user_guide/regression/smt/#options","title":"Options","text":"<p>Regarding the options of the SMT's surrogate models, you will find more information in the SMT's user guide by looking at the tables at the bottom of the pages.</p>"},{"location":"user_guide/regression/smt/#derivatives","title":"Derivatives","text":""},{"location":"user_guide/regression/smt/#differentiation","title":"Differentiation","text":"<p>You can use the <code>predict_jacobian</code> method of any SMTRegressor as long as the corresponding SMT's surrogate model can be derived with respect to its inputs.</p> <p>This feature is particularly useful in the case of gradient-based optimization.</p>"},{"location":"user_guide/regression/smt/#gradient-enhanced","title":"Gradient-enhanced","text":"<p>You can wrap the gradient-enhanced surrogate models available in SMT, such as <code>\"GEKPLS\"</code> and <code>\"GENN\"</code>, as long as your training dataset includes both output and Jacobian samples (the Jacobian samples have to be stored in the <code>\"gradients\"</code> group of the IODataset).</p> <p>This feature can improve the quality of the surrogate model without increasing the size of the training dataset.</p> <ol> <li> <p>B. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, M. Morlier, and J. R. R. A. Martins. A python surrogate modeling framework with derivatives. Advances in Engineering Software, pages 102662, 2019. doi:https://doi.org/10.1016/j.advengsoft.2019.03.005.\u00a0\u21a9</p> </li> <li> <p>P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, and J. R. R. A. Martins. SMT 2.0: A surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes. Advances in Engineering Sofware, 188:103571, 2024. doi:https://doi.org/10.1016/j.advengsoft.2023.103571.\u00a0\u21a9</p> </li> </ol>"}]}